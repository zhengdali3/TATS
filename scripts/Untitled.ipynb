{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n",
    "import sys\n",
    "sys.path.insert(0, '/userhome/42/msd21003/TATS')\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from tats import Net2NetTransformer, VideoData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_path:<class 'str'>,sequence_len:<class 'int'>,dataset:<class 'str'>,train:<class 'bool'>,dataset:<class 'type'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'istrain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m VideoData(args)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# pre-make relevant cached files if necessary\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m data\u001b[38;5;241m.\u001b[39mtest_dataloader()\n\u001b[1;32m     19\u001b[0m args\u001b[38;5;241m.\u001b[39mclass_cond_dim \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mn_classes \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39munconditional \u001b[38;5;129;01mand\u001b[39;00m args\u001b[38;5;241m.\u001b[39mcond_stage_key\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/TATS/tats/data.py:318\u001b[0m, in \u001b[0;36mVideoData.train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TATS/tats/data.py:297\u001b[0m, in \u001b[0;36mVideoData._dataloader\u001b[0;34m(self, train)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m, train):\n\u001b[0;32m--> 297\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dist\u001b[38;5;241m.\u001b[39mis_initialized():\n\u001b[1;32m    299\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mDistributedSampler(\n\u001b[1;32m    300\u001b[0m             dataset, num_replicas\u001b[38;5;241m=\u001b[39mdist\u001b[38;5;241m.\u001b[39mget_world_size(), rank\u001b[38;5;241m=\u001b[39mdist\u001b[38;5;241m.\u001b[39mget_rank()\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/TATS/tats/data.py:292\u001b[0m, in \u001b[0;36mVideoData._dataset\u001b[0;34m(self, train)\u001b[0m\n\u001b[1;32m    288\u001b[0m         Dataset \u001b[38;5;241m=\u001b[39m VideoDataset \u001b[38;5;28;01mif\u001b[39;00m osp\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata_path) \u001b[38;5;28;01melse\u001b[39;00m HDF5Dataset\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_path:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,sequence_len:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msequence_length)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,dataset:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,train:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,dataset:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(Dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 292\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mistrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'istrain'"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(1234)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser = pl.Trainer.add_argparse_args(parser)\n",
    "parser = Net2NetTransformer.add_model_specific_args(parser)\n",
    "parser = VideoData.add_data_specific_args(parser)\n",
    "\n",
    "args = parser.parse_args(args=[\"--num_workers\", \"32\", \"--val_check_interval\", \" 0.5\", \"--progress_bar_refresh_rate\", \" 500\",\n",
    "                    \"--gpus\", \" 8\" ,\"--sync_batchnorm\" ,\"--batch_size\", \" 3\",  \"--unconditional\",\n",
    "                    \"--vqvae\", \" ../../ckpt/vqgan_ucf.ckpt\", \"--data_path\", \" ../../ucf101\", \"--dataset\", \"ucf101\", \"--default_root_dir\", \" ../../trainGPT_ckpt\",\n",
    "                    \"--vocab_size\", \" 16384\", \"--block_size\", \" 1024\", \"--n_layer\", \" 24\", \"--n_head\", \" 16\", \"--n_embd\", \" 1024\",\n",
    "                    \"--resolution\", \" 128\", \"--sequence_length\", \" 16\", \"--max_steps\", \" 2000000\"])\n",
    "\n",
    "data = VideoData(args)\n",
    "# pre-make relevant cached files if necessary\n",
    "data.train_dataloader()\n",
    "data.test_dataloader()\n",
    "\n",
    "args.class_cond_dim = data.n_classes if not args.unconditional and args.cond_stage_key=='label' else None\n",
    "model = Net2NetTransformer(args, first_stage_key=args.first_stage_key, cond_stage_key=args.cond_stage_key)\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(ModelCheckpoint(every_n_train_steps=10000, save_top_k=-1, filename='{epoch}-{step}-{train/loss:.2f}'))\n",
    "callbacks.append(ModelCheckpoint(every_n_train_steps=50000, save_top_k=-1, filename='{epoch}-{step}-{train/loss:.2f}'))\n",
    "callbacks.append(ModelCheckpoint(monitor='val/loss', mode='min', save_top_k=3, filename='best_checkpoint'))\n",
    "\n",
    "kwargs = dict()\n",
    "if args.gpus > 1:\n",
    "    # find_unused_parameters = False to support gradient checkpointing\n",
    "    kwargs = dict(gpus=args.gpus,\n",
    "                  # plugins=[\"deepspeed_stage_2\"])\n",
    "                  plugins=[pl.plugins.DDPPlugin(find_unused_parameters=False)])\n",
    "\n",
    "# configure learning rate\n",
    "bs, base_lr = args.batch_size, args.base_lr\n",
    "ngpu = args.gpus\n",
    "accumulate_grad_batches = args.accumulate_grad_batches or 1\n",
    "print(f\"accumulate_grad_batches = {accumulate_grad_batches}\")\n",
    "model.learning_rate = accumulate_grad_batches * ngpu * bs * base_lr\n",
    "print(\"Setting learning rate to {:.2e} = {} (accumulate_grad_batches) * {} (num_gpus) * {} (batchsize) * {:.2e} (base_lr)\".format(\n",
    "    model.learning_rate, accumulate_grad_batches, ngpu, bs, base_lr))\n",
    "\n",
    "# load the most recent checkpoint file\n",
    "base_dir = os.path.join(args.default_root_dir, 'lightning_logs')\n",
    "if os.path.exists(base_dir):\n",
    "    log_folder = ckpt_file = ''\n",
    "    version_id_used = step_used = 0\n",
    "    for folder in os.listdir(base_dir):\n",
    "        version_id = int(folder.split('_')[1])\n",
    "        if version_id > version_id_used:\n",
    "            version_id_used = version_id\n",
    "            log_folder = folder\n",
    "    if len(log_folder) > 0:\n",
    "        ckpt_folder = os.path.join(base_dir, log_folder, 'checkpoints')\n",
    "        for fn in os.listdir(ckpt_folder):\n",
    "            if fn == 'latest_checkpoint.ckpt':\n",
    "                ckpt_file = 'latest_checkpoint_prev.ckpt'\n",
    "                os.rename(os.path.join(ckpt_folder, fn), os.path.join(ckpt_folder, ckpt_file))\n",
    "        if len(ckpt_file) > 0:\n",
    "            args.resume_from_checkpoint = os.path.join(ckpt_folder, ckpt_file)\n",
    "            print('will start from the recent ckpt %s'%args.resume_from_checkpoint)\n",
    "\n",
    "trainer = pl.Trainer.from_argparse_args(args, callbacks=callbacks,\n",
    "                                        max_steps=args.max_steps, **kwargs)\n",
    "\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import cupy\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from datetime import datetime\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import softmax\n",
    "from torch.autograd import Function\n",
    "\n",
    "\n",
    "def getGaussian(T, H, W, beta, d):\n",
    "\n",
    "    diag = np.diag([beta[0], beta[1], beta[1]])\n",
    "    rv = multivariate_normal([T - 1, H - 1, W - 1], diag)\n",
    "    tensor = torch.tensor((), dtype=torch.float32)\n",
    "\n",
    "    NT = 2 * T - 1\n",
    "    NH = 2 * H - 1\n",
    "    NW = 2 * W - 1\n",
    "\n",
    "    weight = tensor.new_ones((NT, NW, NH), device=d)\n",
    "\n",
    "    for pos in np.arange(0, NT * NH * NW):\n",
    "        i = math.floor(pos / (NH * NW))\n",
    "        j = math.floor((pos - i * NH * NW) / NH)\n",
    "        k = pos - i * NH * NW - j * NW\n",
    "        weight[i, j, k] = rv.pdf([i, j, k])\n",
    "\n",
    "        weight = weight / torch.max(weight)\n",
    "\n",
    "    return weight\n",
    "\n",
    "class focusAttention(Function):\n",
    "\n",
    "    T, H, W = 4, 4, 4\n",
    "    T_flatten = T * H * W\n",
    "    center_T, center_H, center_W = T - 1, H - 1, W - 1\n",
    "    beta = [100, 100]\n",
    "    \n",
    "    diag = np.diag([beta[0], beta[1], beta[1]])\n",
    "    rv = multivariate_normal([T - 1, H - 1, W - 1], diag)\n",
    "    tensor = torch.tensor((), dtype=torch.float32)\n",
    "\n",
    "    NT = 2 * T - 1\n",
    "    NH = 2 * H - 1\n",
    "    NW = 2 * W - 1\n",
    "\n",
    "    weight_cuda0 = tensor.new_ones((NT, NW, NH), device=torch.device(\"cuda:0\"))\n",
    "\n",
    "    for pos in np.arange(0, NT * NH * NW):\n",
    "        i = math.floor(pos / (NH * NW))\n",
    "        j = math.floor((pos - i * NH * NW) / NH)\n",
    "        k = pos - i * NH * NW - j * NW\n",
    "        weight_cuda0[i, j, k] = rv.pdf([i, j, k])\n",
    "\n",
    "        weight_cuda0 = weight_cuda0 / torch.max(weight_cuda0)\n",
    "    \n",
    "    weight_cuda1 = weight_cuda0.detach().to(\"cuda:1\")\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, score, V):\n",
    "\n",
    "        att=[]\n",
    "        \n",
    "        if V.get_device() == 0:\n",
    "            weight = focusAttention.weight_cuda0\n",
    "        else:\n",
    "            weight = focusAttention.weight_cuda1\n",
    "        \n",
    "        st = torch.cuda.memory_allocated()\n",
    "        \n",
    "        st_loop = datetime.now()\n",
    "        \n",
    "        for pos in np.arange(0, focusAttention.T_flatten):\n",
    "            \n",
    "            st = datetime.now()\n",
    "            print(f\"start of loop {st}\")\n",
    "            \n",
    "            i = math.floor(pos / (focusAttention.H * focusAttention.W))\n",
    "            j = math.floor((pos - i * focusAttention.H * focusAttention.W) / focusAttention.H)\n",
    "            k = pos - i * focusAttention.H * focusAttention.W - j * focusAttention.W\n",
    "            \n",
    "            t1 = datetime.now()\n",
    "            \n",
    "            print(f\"pos calculate {t1 - st}\")\n",
    "            \n",
    "\n",
    "            weight_xyz = weight[focusAttention.center_T - i:2 * focusAttention.center_T - i + 1, focusAttention.center_W - j:2 * focusAttention.center_W - j + 1,\n",
    "                         focusAttention.center_H - k:2 * focusAttention.center_H - k + 1].reshape(-1)\n",
    "            \n",
    "            t2 = datetime.now()\n",
    "            print(f\"weight idx {t2 - t1}\")\n",
    "            \n",
    "            weight_xyz = weight_xyz[None, None, :, None]\n",
    "            t3 = datetime.now()\n",
    "            \n",
    "            print(f\"weight boardcast {t3 - t2}\")\n",
    "            \n",
    "            # qk shape (B, NH, 1, T)\n",
    "            qk = score[:, :, pos, :]\n",
    "            qk = qk[:, :, None, :]\n",
    "            \n",
    "            t4 = datetime.now()\n",
    "            \n",
    "            print(f\"qk index and boardcast {t4 - t3}\")\n",
    "\n",
    "            att_pos = torch.matmul(qk, (V * weight_xyz)).detach()\n",
    "            \n",
    "            t5 = datetime.now()\n",
    "            \n",
    "            print(f\"att cal {t5 - t4}\")\n",
    "\n",
    "            att.append(att_pos)\n",
    "            \n",
    "            t6 = datetime.now()\n",
    "            \n",
    "            print(f\"append {t6 -t5}\")\n",
    "            # V = torch.clone(V_ori)\n",
    "\n",
    "        \n",
    "        end = datetime.now()\n",
    "        \n",
    "        print(f\"complete loop {end - st_loop}\")\n",
    "        \n",
    "        # print(f\"Before cat {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "        result = torch.cat(att, dim=2)\n",
    "        \n",
    "        # print(f\"result shape {result.shape}\")\n",
    "\n",
    "        end = torch.cuda.memory_allocated()\n",
    "\n",
    "        # print(f\"result memory usage is {result.element_size() * result.nelement()}, memory used {end - st}, memory for v is {V.element_size() * V.nelement()}\")\n",
    "\n",
    "        # print(f\"After focused attention, memory usage is {end}, memory used {end - st}\")\n",
    "\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "        # print(f\"After empty cache, memory usage is {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "        # ctx.save_for_backward(score, V, result)\n",
    "\n",
    "        # print(f\"After save for backwards, memory usage is {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        score, V, result = ctx.saved_tensors\n",
    "        \n",
    "        if V.get_device() == 0:\n",
    "            weight = focusAttention.weight_cuda0\n",
    "        else:\n",
    "            weight = focusAttention.weight_cuda1\n",
    "\n",
    "        grad_score = []\n",
    "        grad_V = []\n",
    "\n",
    "        for pos in np.arange(0, focusAttention.T_flatten):\n",
    "            grad_att_pos = grad_output[:, :, pos, :]\n",
    "\n",
    "            grad_att_pos = grad_att_pos[:, :, None, :]\n",
    "\n",
    "            i = math.floor(pos / (focusAttention.H * focusAttention.W))\n",
    "            j = math.floor((pos - i * focusAttention.H * focusAttention.W) / focusAttention.H)\n",
    "            k = pos - i * focusAttention.H * focusAttention.W - j * focusAttention.W\n",
    "\n",
    "            weight_xyz = weight[focusAttention.center_T - i:2 * focusAttention.center_T - i + 1, focusAttention.center_W - j:2 * focusAttention.center_W - j + 1,\n",
    "                         focusAttention.center_H - k:2 * focusAttention.center_H - k + 1].reshape(-1)\n",
    "\n",
    "            qk = score[:, :, pos, :]\n",
    "            \n",
    "            qk = qk[:, :, None, :]\n",
    "            \n",
    "            print(f\"shape qk {qk.shape}\")\n",
    "            \n",
    "            qk = torch.swapaxes(qk, 2, 3)\n",
    "            \n",
    "            weight_xyz = weight_xyz[None, None, :, None]\n",
    "            \n",
    "            print(f\"shape qk {qk.shape}, weight_xyz {weight_xyz.shape}\")\n",
    "            \n",
    "            print(f\"shape qk*weight {(qk * weight_xyz).shape}, grad_att_pos {grad_att_pos.shape}\")\n",
    "            \n",
    "            grad_V_pos = torch.matmul((qk * weight_xyz), grad_att_pos)[:, :, pos, :]\n",
    "            \n",
    "            grad_V.append(grad_V_pos[:, :, None, :])\n",
    "            \n",
    "            print(f\"V_pos shape {grad_V_pos.shape}\")\n",
    "            \n",
    "            grad_score_pos = (torch.matmul(weight_xyz, grad_att_pos) * V)[:, :, :, 0]\n",
    "            \n",
    "            print(f\"grad_score_pos shape {grad_score_pos.shape}\")\n",
    "            \n",
    "            grad_score.append(grad_score_pos[:, :, None, :])\n",
    "\n",
    "            # grad_qk = grad_att_pos @ torch.linalg.inv(V_focus)\n",
    "            # grad_V_focus = torch.linalg.inv(qk) @ grad_att_pos\n",
    "            #\n",
    "            # grad_score.append(grad_qk)\n",
    "            # grad_V_focus = grad_V_focus * weight_xyz\n",
    "            # grad_V.append(grad_V_focus)\n",
    "\n",
    "        # Shape should be B, NH, T, T\n",
    "        grad_score = torch.cat(grad_score, dim=2)\n",
    "\n",
    "        # Shape should be B, NH, T, HS\n",
    "        grad_V = torch.cat(grad_V, dim=2)\n",
    "        \n",
    "        # print(f\"grad_score {grad_score.shape}, grad_V {grad_V.shape}\")\n",
    "\n",
    "        return grad_score, grad_V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import cupy\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from datetime import datetime\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import softmax\n",
    "from torch.autograd import Function\n",
    "\n",
    "\n",
    "def getGaussian(T, H, W, beta, d):\n",
    "\n",
    "    diag = np.diag([beta[0], beta[1], beta[1]])\n",
    "    rv = multivariate_normal([T - 1, H - 1, W - 1], diag)\n",
    "    tensor = torch.tensor((), dtype=torch.float32)\n",
    "\n",
    "    NT = 2 * T - 1\n",
    "    NH = 2 * H - 1\n",
    "    NW = 2 * W - 1\n",
    "\n",
    "    weight = tensor.new_ones((NT, NW, NH), device=d)\n",
    "\n",
    "    for pos in np.arange(0, NT * NH * NW):\n",
    "        i = math.floor(pos / (NH * NW))\n",
    "        j = math.floor((pos - i * NH * NW) / NH)\n",
    "        k = pos - i * NH * NW - j * NW\n",
    "        weight[i, j, k] = rv.pdf([i, j, k])\n",
    "\n",
    "        weight = weight / torch.max(weight)\n",
    "\n",
    "    return weight\n",
    "\n",
    "class focusAttention(Function):\n",
    "\n",
    "    T, H, W = 4, 16, 16\n",
    "    T_flatten = T * H * W\n",
    "    center_T, center_H, center_W = T - 1, H - 1, W - 1\n",
    "    beta = [10000, 10000]\n",
    "    \n",
    "    diag = np.diag([beta[0], beta[1], beta[1]])\n",
    "    rv = multivariate_normal([T - 1, H - 1, W - 1], diag)\n",
    "    tensor = torch.tensor((), dtype=torch.float32)\n",
    "\n",
    "    NT = 2 * T - 1\n",
    "    NH = 2 * H - 1\n",
    "    NW = 2 * W - 1\n",
    "\n",
    "    weight_cuda0 = tensor.new_ones((NT, NW, NH), device=torch.device(\"cuda:0\"))\n",
    "\n",
    "    for pos in np.arange(0, NT * NH * NW):\n",
    "        i = math.floor(pos / (NH * NW))\n",
    "        j = math.floor((pos - i * NH * NW) / NH)\n",
    "        k = pos - i * NH * NW - j * NW\n",
    "        weight_cuda0[i, j, k] = rv.pdf([i, j, k])\n",
    "\n",
    "    weight_cuda0 = weight_cuda0 / torch.max(weight_cuda0)\n",
    "    \n",
    "    # print(weight_cuda0, weight_cuda0[T-1, H-1, W-1])\n",
    "    \n",
    "    # Shape T, 1, 1, T, 1\n",
    "    V_weight_cuda0 = torch.empty((T_flatten,1,1,T_flatten,1), dtype=torch.float32, device =\"cuda:0\")\n",
    "\n",
    "    for pos in np.arange(0, T_flatten):\n",
    "\n",
    "        i = math.floor(pos / (H * W))\n",
    "        j = math.floor((pos - i * H * W) / H)\n",
    "        k = pos - i * H * W - j * W\n",
    "\n",
    "        weight_xyz = weight_cuda0[center_T - i:2 * center_T - i + 1, center_W - j:2 * center_W - j + 1,\n",
    "                     center_H - k:2 * center_H - k + 1].reshape(-1)\n",
    "\n",
    "        V_weight_cuda0[pos, 0, 0, :, 0] = weight_xyz\n",
    "\n",
    "    V_weight_cuda1 = V_weight_cuda0.detach().to(\"cuda:1\")\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, score, V):\n",
    "        \n",
    "        B, NH, T_flatten, HS = V.shape\n",
    "        d = V.get_device()\n",
    "        \n",
    "        if d == 0:\n",
    "            V_weight = focusAttention.V_weight_cuda0\n",
    "        else:\n",
    "            V_weight = focusAttention.V_weight_cuda1\n",
    "        \n",
    "\n",
    "        st_loop = datetime.now()\n",
    "\n",
    "        # V full shape is T, B, NH, T, HS = T, 1, 1, T, 1 * B, NH, T, HS\n",
    "        V_full = V_weight * V\n",
    "        \n",
    "        # print(\"V full size\", V_full.nelement() * V_full.element_size(), V_full.shape, V_weight.shape)\n",
    "        # print(\"before score \", score.shape)\n",
    "        \n",
    "        qk = score[:, :, :, None, :]\n",
    "        \n",
    "        # qk should be T, B, NH, 1 , T \n",
    "        qk = qk.permute(2, 0, 1, 3, 4)\n",
    "        \n",
    "        # print(\"qk \", qk.shape)\n",
    "        \n",
    "        # result should be T, B, NH, 1, HS\n",
    "        result = torch.empty((T_flatten, B, NH, 1, HS), dtype = torch.float32, device = f\"cuda:{d}\")\n",
    "        \n",
    "        # mem = torch.cuda.torch.cuda.memory_allocated()\n",
    "        # result1 = torch.einsum('ijklm, ijkmn -> ijkln', qk, V_full)\n",
    "        # mem1 = torch.cuda.torch.cuda.memory_allocated()\n",
    "        # print(\"einsum used memory\", mem1 - mem)\n",
    "        \n",
    "        div = 16\n",
    "        mem2 = torch.cuda.torch.cuda.memory_allocated()\n",
    "        for sub in np.arange(div):\n",
    "            base = int(focusAttention.T_flatten / div)\n",
    "            result[base*sub:base*(sub+1), :, :, :, :] = torch.matmul(qk[base*sub:base*(sub+1), :, :, :, :], V_full[base*sub:base*(sub+1), :, :, :, :])\n",
    "            \n",
    "        mem3 = torch.cuda.torch.cuda.memory_allocated()\n",
    "        # print(\"divided memory\", mem3 -mem2)\n",
    "        \n",
    "        result1 = torch.swapaxes(result, 0, 2)[:, :, :, 0, :]\n",
    "        \n",
    "#         print(\"result\", result.shape)\n",
    "        \n",
    "        end = datetime.now()\n",
    "        print(f\"forward {end - st_loop}\")\n",
    "        \n",
    "        ctx.save_for_backward(score, V, result)\n",
    "\n",
    "        return result1\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \n",
    "        st_loop = datetime.now()\n",
    "        \n",
    "        score, V, result = ctx.saved_tensors\n",
    "        \n",
    "        B, NH, T_flatten, HS = V.shape\n",
    "\n",
    "        # V weight shape is [1, 1, T, T]\n",
    "        d = V.get_device()\n",
    "\n",
    "        if d == 0:\n",
    "            V_weight = focusAttention.V_weight_cuda0\n",
    "        else:\n",
    "            V_weight = focusAttention.V_weight_cuda1\n",
    "\n",
    "        # ï¼ˆ1, B, NH, T, T)\n",
    "        score = score[None, :, :, :, :]\n",
    "\n",
    "        # (T, B, NH, T, 1)\n",
    "        score = score.permute(3, 1, 2, 4, 0)\n",
    "        print(score.shape, V_weight.shape)\n",
    "        \n",
    "        # (1, B, NH, T, HS)\n",
    "        grad_output = grad_output[None, :, :, :, :]\n",
    "\n",
    "        # (T, B, NH, 1, HS)\n",
    "        grad_output = torch.swapaxes(grad_output, 0, 3)\n",
    "\n",
    "        grad_V = torch.empty(V.shape, dtype = torch.float32, device = f\"cuda:{d}\")\n",
    "        grad_V_total = torch.empty((T_flatten, B, NH, T_flatten, HS), dtype = torch.float32, device = f\"cuda:{d}\")\n",
    "        # Shape is (T, B, NH, T, HS)\n",
    "        \n",
    "        div = 16\n",
    "        mem2 = torch.cuda.torch.cuda.memory_allocated()\n",
    "        for sub in np.arange(div):\n",
    "            base = int(focusAttention.T_flatten / div)\n",
    "            grad_V_total[base*sub:base*(sub+1), :, :, :, :] = torch.matmul((V_weight * score)[base*sub:base*(sub+1), :, :, :, :], grad_output[base*sub:base*(sub+1), :, :, :, :])\n",
    "            \n",
    "        mem3 = torch.cuda.torch.cuda.memory_allocated()\n",
    "        \n",
    "        for i in np.arange(focusAttention.T_flatten):\n",
    "            grad_V[:, :, i, :] = grad_V_total[i, :, :, i, :]\n",
    "\n",
    "        del grad_V_total\n",
    "\n",
    "        # (T, B, NH, T, 1) = (B, NH, T, 1) * (T, B, NH, 1, 1) * (T, 1, 1, T, 1)\n",
    "\n",
    "        grad_score = V[:, :, :, 0][:, :, :, None] * grad_output[:, :, :, :, 0][:, :, :, :, None] * V_weight\n",
    "\n",
    "        # (T, B, NH, T)\n",
    "        grad_score = grad_score[:, :, :, :, 0]\n",
    "\n",
    "        # (B, NH, T, T)\n",
    "        grad_score = grad_score.permute(1, 2, 0, 3)\n",
    "        \n",
    "        end = datetime.now()\n",
    "        \n",
    "        print(f\"backward {end - st_loop}\")\n",
    "        # print(f\"grad_score {grad_score.shape}, grad_V {grad_V.shape}\")\n",
    "        return grad_score, grad_V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 4, 16, 1024, 1]) torch.Size([1024, 1, 1, 1024, 1])\n",
      "backward 0:00:00.059186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 1.0659e-02, -7.1752e-02, -3.2511e-02,  ...,  5.8555e-02,\n",
       "             2.2246e-02, -1.0939e-01],\n",
       "           [-1.0925e-01,  7.3552e-01,  3.3329e-01,  ..., -6.0096e-01,\n",
       "            -2.2834e-01,  1.1229e+00],\n",
       "           [-1.8819e-01,  1.2671e+00,  5.7421e-01,  ..., -1.0365e+00,\n",
       "            -3.9386e-01,  1.9371e+00],\n",
       "           ...,\n",
       "           [ 6.5541e-02, -4.4176e-01, -2.0042e-01,  ...,  3.7484e-01,\n",
       "             1.4259e-01, -7.0208e-01],\n",
       "           [ 5.8338e-03, -3.9325e-02, -1.7843e-02,  ...,  3.3409e-02,\n",
       "             1.2710e-02, -6.2587e-02],\n",
       "           [-3.9475e-02,  2.6612e-01,  1.2076e-01,  ..., -2.2636e-01,\n",
       "            -8.6125e-02,  4.2413e-01]],\n",
       " \n",
       "          [[ 6.2830e-02,  5.0019e-01,  9.7075e-02,  ..., -3.7757e-01,\n",
       "            -3.3462e-01, -1.1313e-01],\n",
       "           [-1.3270e-01, -1.0566e+00, -2.0508e-01,  ...,  7.9852e-01,\n",
       "             7.0775e-01,  2.3930e-01],\n",
       "           [-1.5364e-01, -1.2234e+00, -2.3748e-01,  ...,  9.2571e-01,\n",
       "             8.2057e-01,  2.7747e-01],\n",
       "           ...,\n",
       "           [-3.4084e-01, -2.7170e+00, -5.2798e-01,  ...,  2.1325e+00,\n",
       "             1.8924e+00,  6.4059e-01],\n",
       "           [-1.3211e-01, -1.0532e+00, -2.0468e-01,  ...,  8.2760e-01,\n",
       "             7.3448e-01,  2.4866e-01],\n",
       "           [-8.2079e-02, -6.5442e-01, -1.2720e-01,  ...,  5.1487e-01,\n",
       "             4.5699e-01,  1.5473e-01]],\n",
       " \n",
       "          [[ 3.3938e-02, -2.0609e+00, -5.1709e-01,  ...,  5.6236e-01,\n",
       "             2.7196e-01, -1.6367e-01],\n",
       "           [ 7.2127e-02, -4.3802e+00, -1.0992e+00,  ...,  1.1967e+00,\n",
       "             5.7878e-01, -3.4835e-01],\n",
       "           [-1.8771e-02,  1.1401e+00,  2.8611e-01,  ..., -3.1185e-01,\n",
       "            -1.5084e-01,  9.0794e-02],\n",
       "           ...,\n",
       "           [ 1.2417e-02, -7.5501e-01, -1.8969e-01,  ...,  2.1422e-01,\n",
       "             1.0373e-01, -6.2507e-02],\n",
       "           [-1.9128e-02,  1.1632e+00,  2.9226e-01,  ..., -3.3042e-01,\n",
       "            -1.6001e-01,  9.6432e-02],\n",
       "           [ 1.4337e-04, -8.7189e-03, -2.1910e-03,  ...,  2.4798e-03,\n",
       "             1.2010e-03, -7.2386e-04]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.2122e+00, -1.0994e+00, -1.9453e+00,  ...,  1.5526e+00,\n",
       "            -1.2651e+00,  8.1622e-01],\n",
       "           [-3.1227e-01,  2.8325e-01,  5.0123e-01,  ..., -4.0048e-01,\n",
       "             3.2635e-01, -2.1058e-01],\n",
       "           [ 8.0300e-01, -7.2844e-01, -1.2891e+00,  ...,  1.0312e+00,\n",
       "            -8.4039e-01,  5.4232e-01],\n",
       "           ...,\n",
       "           [-2.6363e-01,  2.3942e-01,  4.2417e-01,  ..., -3.5154e-01,\n",
       "             2.8682e-01, -1.8530e-01],\n",
       "           [ 9.9280e-01, -9.0169e-01, -1.5977e+00,  ...,  1.3256e+00,\n",
       "            -1.0816e+00,  6.9884e-01],\n",
       "           [-2.3623e+00,  2.1457e+00,  3.8023e+00,  ..., -3.1582e+00,\n",
       "             2.5773e+00, -1.6653e+00]],\n",
       " \n",
       "          [[-1.0061e+00, -1.4177e+00,  3.4334e-01,  ..., -8.4861e-01,\n",
       "            -1.1698e+00,  1.0601e+00],\n",
       "           [ 2.6646e-01,  3.7550e-01, -9.0950e-02,  ...,  2.2504e-01,\n",
       "             3.1024e-01, -2.8119e-01],\n",
       "           [-5.2991e-01, -7.4684e-01,  1.8091e-01,  ..., -4.4812e-01,\n",
       "            -6.1785e-01,  5.6005e-01],\n",
       "           ...,\n",
       "           [-1.2699e+00, -1.7917e+00,  4.3449e-01,  ..., -1.1151e+00,\n",
       "            -1.5392e+00,  1.3967e+00],\n",
       "           [-1.1188e-01, -1.5787e-01,  3.8288e-02,  ..., -9.8377e-02,\n",
       "            -1.3580e-01,  1.2324e-01],\n",
       "           [ 4.7489e-01,  6.7017e-01, -1.6255e-01,  ...,  4.1811e-01,\n",
       "             5.7722e-01, -5.2390e-01]],\n",
       " \n",
       "          [[ 5.8266e-02,  9.7804e-03,  2.1980e-02,  ..., -1.5662e-02,\n",
       "             8.8974e-03, -5.0328e-04],\n",
       "           [ 5.4286e-01,  9.1133e-02,  2.0482e-01,  ..., -1.4611e-01,\n",
       "             8.3013e-02, -4.6961e-03],\n",
       "           [-2.6980e+00, -4.5297e-01, -1.0182e+00,  ...,  7.2710e-01,\n",
       "            -4.1315e-01,  2.3374e-02],\n",
       "           ...,\n",
       "           [-5.6561e-01, -9.5066e-02, -2.1392e-01,  ...,  1.5829e-01,\n",
       "            -9.0039e-02,  5.0997e-03],\n",
       "           [-2.4447e+00, -4.1094e-01, -9.2481e-01,  ...,  6.8504e-01,\n",
       "            -3.8972e-01,  2.2075e-02],\n",
       "           [-1.5616e+00, -2.6251e-01, -5.9084e-01,  ...,  4.3814e-01,\n",
       "            -2.4928e-01,  1.4122e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.8298e-01,  2.6941e-01, -1.1460e-01,  ..., -1.8588e-01,\n",
       "             9.7861e-02,  3.9703e-01],\n",
       "           [ 2.9879e-01,  2.1021e-01, -8.9426e-02,  ..., -1.4521e-01,\n",
       "             7.6456e-02,  3.1022e-01],\n",
       "           [-4.7517e-02, -3.3433e-02,  1.4224e-02,  ...,  2.3123e-02,\n",
       "            -1.2176e-02, -4.9408e-02],\n",
       "           ...,\n",
       "           [-3.3942e-01, -2.3908e-01,  1.0183e-01,  ...,  1.7152e-01,\n",
       "            -9.0414e-02, -3.6729e-01],\n",
       "           [-6.3262e-01, -4.4564e-01,  1.8983e-01,  ...,  3.2009e-01,\n",
       "            -1.6875e-01, -6.8560e-01],\n",
       "           [-3.1241e-01, -2.2010e-01,  9.3764e-02,  ...,  1.5828e-01,\n",
       "            -8.3453e-02, -3.3908e-01]],\n",
       " \n",
       "          [[-2.9477e-01, -1.0594e+00, -1.1046e+00,  ...,  1.9530e-02,\n",
       "            -8.3445e-01, -3.1682e-01],\n",
       "           [ 4.1977e-02,  1.5088e-01,  1.5733e-01,  ..., -2.7849e-03,\n",
       "             1.1900e-01,  4.5186e-02],\n",
       "           [ 6.5990e-02,  2.3721e-01,  2.4739e-01,  ..., -4.3837e-03,\n",
       "             1.8733e-01,  7.1141e-02],\n",
       "           ...,\n",
       "           [-4.9116e-01, -1.7675e+00, -1.8453e+00,  ...,  3.3881e-02,\n",
       "            -1.4495e+00, -5.5105e-01],\n",
       "           [-6.9486e-02, -2.5008e-01, -2.6112e-01,  ...,  4.7995e-03,\n",
       "            -2.0535e-01, -7.8076e-02],\n",
       "           [ 7.6559e-02,  2.7556e-01,  2.8775e-01,  ..., -5.2949e-03,\n",
       "             2.2657e-01,  8.6152e-02]],\n",
       " \n",
       "          [[ 9.3915e-01,  9.7986e-01, -7.9105e-01,  ...,  1.3017e-01,\n",
       "             3.1641e-02, -1.8942e+00],\n",
       "           [ 3.3092e-01,  3.4529e-01, -2.7879e-01,  ...,  4.5926e-02,\n",
       "             1.1165e-02, -6.6843e-01],\n",
       "           [ 2.4103e-01,  2.5153e-01, -2.0311e-01,  ...,  3.3495e-02,\n",
       "             8.1435e-03, -4.8760e-01],\n",
       "           ...,\n",
       "           [ 3.1229e-01,  3.2625e-01, -2.6373e-01,  ...,  4.5064e-02,\n",
       "             1.0968e-02, -6.5747e-01],\n",
       "           [ 6.6520e-01,  6.9501e-01, -5.6188e-01,  ...,  9.6115e-02,\n",
       "             2.3396e-02, -1.4026e+00],\n",
       "           [-5.3969e-01, -5.6392e-01,  4.5595e-01,  ..., -7.8081e-02,\n",
       "            -1.9008e-02,  1.1396e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-6.9894e-02,  1.5755e-01, -5.8621e-02,  ...,  4.6747e-02,\n",
       "             2.6899e-02, -4.2542e-02],\n",
       "           [-9.5713e-02,  2.1577e-01, -8.0291e-02,  ...,  6.4099e-02,\n",
       "             3.6886e-02, -5.8344e-02],\n",
       "           [-3.0387e-01,  6.8511e-01, -2.5496e-01,  ...,  2.0377e-01,\n",
       "             1.1727e-01, -1.8551e-01],\n",
       "           ...,\n",
       "           [ 3.2056e-02, -7.2352e-02,  2.6955e-02,  ..., -2.2321e-02,\n",
       "            -1.2861e-02,  2.0366e-02],\n",
       "           [-3.3096e-01,  7.4707e-01, -2.7835e-01,  ...,  2.3075e-01,\n",
       "             1.3296e-01, -2.1058e-01],\n",
       "           [ 5.7804e-02, -1.3049e-01,  4.8626e-02,  ..., -4.0355e-02,\n",
       "            -2.3256e-02,  3.6835e-02]],\n",
       " \n",
       "          [[-7.7604e-01, -5.6977e-01, -2.2694e-01,  ..., -1.8614e-01,\n",
       "             3.2417e-01, -6.2627e-02],\n",
       "           [-2.3234e+00, -1.7061e+00, -6.7958e-01,  ..., -5.5801e-01,\n",
       "             9.7193e-01, -1.8778e-01],\n",
       "           [ 4.1973e-01,  3.0823e-01,  1.2279e-01,  ...,  1.0094e-01,\n",
       "            -1.7582e-01,  3.3974e-02],\n",
       "           ...,\n",
       "           [ 1.5947e+00,  1.1723e+00,  4.6754e-01,  ...,  3.9822e-01,\n",
       "            -6.9443e-01,  1.3433e-01],\n",
       "           [ 5.0544e-01,  3.7162e-01,  1.4822e-01,  ...,  1.2638e-01,\n",
       "            -2.2041e-01,  4.2641e-02],\n",
       "           [-4.7652e-01, -3.5039e-01, -1.3977e-01,  ..., -1.1930e-01,\n",
       "             2.0809e-01, -4.0261e-02]],\n",
       " \n",
       "          [[-2.3273e-01,  2.3987e-01,  5.6309e-01,  ..., -4.2004e-01,\n",
       "            -2.9637e-01, -8.5984e-01],\n",
       "           [-8.2682e-02,  8.5228e-02,  2.0009e-01,  ..., -1.4942e-01,\n",
       "            -1.0544e-01, -3.0594e-01],\n",
       "           [-4.1025e-02,  4.2293e-02,  9.9300e-02,  ..., -7.4236e-02,\n",
       "            -5.2390e-02, -1.5203e-01],\n",
       "           ...,\n",
       "           [-2.3605e-01,  2.4361e-01,  5.7261e-01,  ..., -4.4355e-01,\n",
       "            -3.1337e-01, -9.1034e-01],\n",
       "           [-1.5379e-01,  1.5873e-01,  3.7313e-01,  ..., -2.8935e-01,\n",
       "            -2.0445e-01, -5.9398e-01],\n",
       "           [-9.3724e-02,  9.6745e-02,  2.2745e-01,  ..., -1.7657e-01,\n",
       "            -1.2477e-01, -3.6254e-01]]],\n",
       " \n",
       " \n",
       "         [[[-1.2689e+00, -1.4916e+00, -2.3449e+00,  ...,  1.5862e-02,\n",
       "             1.4618e-01, -2.3947e-01],\n",
       "           [ 1.9342e+00,  2.2739e+00,  3.5751e+00,  ..., -2.4210e-02,\n",
       "            -2.2315e-01,  3.6557e-01],\n",
       "           [ 2.0914e-01,  2.4589e-01,  3.8663e-01,  ..., -2.6211e-03,\n",
       "            -2.4161e-02,  3.9587e-02],\n",
       "           ...,\n",
       "           [-6.1797e-01, -7.2736e-01, -1.1450e+00,  ...,  8.0426e-03,\n",
       "             7.4218e-02, -1.2174e-01],\n",
       "           [ 4.6934e-01,  5.5248e-01,  8.6977e-01,  ..., -6.1163e-03,\n",
       "            -5.6447e-02,  9.2597e-02],\n",
       "           [ 1.4771e+00,  1.7390e+00,  2.7379e+00,  ..., -1.9274e-02,\n",
       "            -1.7790e-01,  2.9186e-01]],\n",
       " \n",
       "          [[ 4.7983e-01, -1.7808e-02, -3.2776e-01,  ...,  4.1689e-01,\n",
       "             9.8747e-01, -8.6128e-01],\n",
       "           [-1.4087e-01,  5.2285e-03,  9.6244e-02,  ..., -1.2255e-01,\n",
       "            -2.9031e-01,  2.5323e-01],\n",
       "           [-5.6951e-02,  2.1140e-03,  3.8918e-02,  ..., -4.9610e-02,\n",
       "            -1.1753e-01,  1.0253e-01],\n",
       "           ...,\n",
       "           [ 6.1055e-01, -2.2688e-02, -4.1814e-01,  ...,  5.5228e-01,\n",
       "             1.3098e+00, -1.1439e+00],\n",
       "           [ 5.4364e-02, -2.0204e-03, -3.7239e-02,  ...,  4.9240e-02,\n",
       "             1.1679e-01, -1.0201e-01],\n",
       "           [-3.8076e-01,  1.4152e-02,  2.6087e-01,  ..., -3.4531e-01,\n",
       "            -8.1915e-01,  7.1554e-01]],\n",
       " \n",
       "          [[ 3.7400e-01, -7.8661e-02,  1.8003e-01,  ...,  2.6434e-01,\n",
       "             5.7511e-01,  3.0771e-01],\n",
       "           [ 2.0815e-01, -4.3784e-02,  1.0022e-01,  ...,  1.4731e-01,\n",
       "             3.2053e-01,  1.7151e-01],\n",
       "           [ 1.8160e+00, -3.8203e-01,  8.7452e-01,  ...,  1.2869e+00,\n",
       "             2.8004e+00,  1.4986e+00],\n",
       "           ...,\n",
       "           [-1.0451e+00,  2.2009e-01, -5.0438e-01,  ..., -7.6903e-01,\n",
       "            -1.6753e+00, -8.9753e-01],\n",
       "           [ 1.0535e+00, -2.2188e-01,  5.0852e-01,  ...,  7.7620e-01,\n",
       "             1.6911e+00,  9.0608e-01],\n",
       "           [ 8.5990e-01, -1.8113e-01,  4.1517e-01,  ...,  6.3440e-01,\n",
       "             1.3823e+00,  7.4070e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.3164e-01, -1.7276e+00,  5.0367e-01,  ..., -5.7912e-01,\n",
       "             1.1856e+00,  1.9981e+00],\n",
       "           [-1.9890e-01, -1.4836e+00,  4.3259e-01,  ..., -4.9793e-01,\n",
       "             1.0195e+00,  1.7183e+00],\n",
       "           [-3.0116e-01, -2.2466e+00,  6.5512e-01,  ..., -7.5491e-01,\n",
       "             1.5458e+00,  2.6057e+00],\n",
       "           ...,\n",
       "           [-7.7215e-02, -5.7663e-01,  1.6834e-01,  ..., -2.0099e-01,\n",
       "             4.1201e-01,  6.9526e-01],\n",
       "           [ 1.0059e-01,  7.5131e-01, -2.1935e-01,  ...,  2.6218e-01,\n",
       "            -5.3751e-01, -9.0713e-01],\n",
       "           [-1.2615e-01, -9.4227e-01,  2.7513e-01,  ..., -3.2922e-01,\n",
       "             6.7501e-01,  1.1393e+00]],\n",
       " \n",
       "          [[-1.5216e-01, -9.0986e-01,  5.7102e-01,  ..., -6.5064e-01,\n",
       "            -1.1668e+00,  1.5621e+00],\n",
       "           [-7.1793e-02, -4.2935e-01,  2.6948e-01,  ..., -3.0739e-01,\n",
       "            -5.5131e-01,  7.3818e-01],\n",
       "           [ 1.6428e-01,  9.8252e-01, -6.1675e-01,  ...,  7.0429e-01,\n",
       "             1.2633e+00, -1.6916e+00],\n",
       "           ...,\n",
       "           [ 3.5023e-01,  2.0970e+00, -1.3178e+00,  ...,  1.5592e+00,\n",
       "             2.7998e+00, -3.7533e+00],\n",
       "           [ 3.2446e-01,  1.9429e+00, -1.2211e+00,  ...,  1.4464e+00,\n",
       "             2.5974e+00, -3.4823e+00],\n",
       "           [-1.0252e-01, -6.1394e-01,  3.8589e-01,  ..., -4.5759e-01,\n",
       "            -8.2183e-01,  1.1019e+00]],\n",
       " \n",
       "          [[-3.5076e-01, -1.3939e+00, -2.4729e+00,  ...,  7.9307e-01,\n",
       "             6.3373e-01,  6.1479e-01],\n",
       "           [-7.6050e-02, -3.0225e-01, -5.3626e-01,  ...,  1.7217e-01,\n",
       "             1.3759e-01,  1.3350e-01],\n",
       "           [ 7.9227e-02,  3.1491e-01,  5.5877e-01,  ..., -1.7960e-01,\n",
       "            -1.4354e-01, -1.3928e-01],\n",
       "           ...,\n",
       "           [-1.4780e-01, -5.8810e-01, -1.0447e+00,  ...,  3.4791e-01,\n",
       "             2.7837e-01,  2.7040e-01],\n",
       "           [-1.8813e-01, -7.4865e-01, -1.3300e+00,  ...,  4.4342e-01,\n",
       "             3.5482e-01,  3.4470e-01],\n",
       "           [ 7.0915e-02,  2.8224e-01,  5.0145e-01,  ..., -1.6737e-01,\n",
       "            -1.3394e-01, -1.3013e-01]]],\n",
       " \n",
       " \n",
       "         [[[ 1.5912e-01,  9.4251e-01,  1.2907e+00,  ..., -3.0901e-01,\n",
       "             1.0828e+00, -2.7609e-03],\n",
       "           [-6.6280e-02, -3.9263e-01, -5.3771e-01,  ...,  1.2888e-01,\n",
       "            -4.5165e-01,  1.1517e-03],\n",
       "           [ 9.0598e-02,  5.3674e-01,  7.3515e-01,  ..., -1.7640e-01,\n",
       "             6.1822e-01, -1.5767e-03],\n",
       "           ...,\n",
       "           [-3.3335e-01, -1.9770e+00, -2.7108e+00,  ...,  6.7397e-01,\n",
       "            -2.3647e+00,  6.0374e-03],\n",
       "           [-1.7163e-01, -1.0180e+00, -1.3960e+00,  ...,  3.4745e-01,\n",
       "            -1.2192e+00,  3.1131e-03],\n",
       "           [-1.4488e-01, -8.5942e-01, -1.1786e+00,  ...,  2.9368e-01,\n",
       "            -1.0306e+00,  2.6318e-03]],\n",
       " \n",
       "          [[-2.1080e-02, -1.2701e-01,  2.3838e-01,  ...,  2.2168e-01,\n",
       "            -7.5296e-02, -1.7943e-01],\n",
       "           [-3.3684e-03, -2.0297e-02,  3.8099e-02,  ...,  3.5469e-02,\n",
       "            -1.2048e-02, -2.8714e-02],\n",
       "           [ 6.9364e-02,  4.1799e-01, -7.8470e-01,  ..., -7.3133e-01,\n",
       "             2.4845e-01,  5.9218e-01],\n",
       "           ...,\n",
       "           [-1.8441e-01, -1.1125e+00,  2.0907e+00,  ...,  2.0190e+00,\n",
       "            -6.8665e-01, -1.6384e+00],\n",
       "           [ 9.1511e-02,  5.5211e-01, -1.0377e+00,  ..., -1.0032e+00,\n",
       "             3.4123e-01,  8.1427e-01],\n",
       "           [-1.6682e-01, -1.0066e+00,  1.8921e+00,  ...,  1.8311e+00,\n",
       "            -6.2290e-01, -1.4866e+00]],\n",
       " \n",
       "          [[-7.8015e-01,  1.1394e+00, -4.1438e-01,  ..., -6.5880e-01,\n",
       "             5.1781e-01,  1.2915e+00],\n",
       "           [-6.8374e-01,  9.9870e-01, -3.6325e-01,  ..., -5.7814e-01,\n",
       "             4.5446e-01,  1.1336e+00],\n",
       "           [-1.1477e+00,  1.6765e+00, -6.0984e-01,  ..., -9.7167e-01,\n",
       "             7.6387e-01,  1.9055e+00],\n",
       "           ...,\n",
       "           [-4.7101e-01,  6.8880e-01, -2.5083e-01,  ..., -4.1410e-01,\n",
       "             3.2590e-01,  8.1388e-01],\n",
       "           [-6.3340e-01,  9.2638e-01, -3.3738e-01,  ..., -5.5760e-01,\n",
       "             4.3888e-01,  1.0961e+00],\n",
       "           [-2.1238e+00,  3.1065e+00, -1.1315e+00,  ..., -1.8721e+00,\n",
       "             1.4737e+00,  3.6810e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 2.3179e-01, -7.9515e-02, -6.8646e-02,  ..., -3.4710e-02,\n",
       "             1.1395e-01,  7.4423e-02],\n",
       "           [-2.3167e+00,  7.9483e-01,  6.8625e-01,  ...,  3.4737e-01,\n",
       "            -1.1405e+00, -7.4498e-01],\n",
       "           [ 1.4483e+00, -4.9694e-01, -4.2909e-01,  ..., -2.1744e-01,\n",
       "             7.1399e-01,  4.6642e-01],\n",
       "           ...,\n",
       "           [-8.3690e-01,  2.8747e-01,  2.4850e-01,  ...,  1.3048e-01,\n",
       "            -4.2891e-01, -2.8049e-01],\n",
       "           [ 2.2622e+00, -7.7715e-01, -6.7186e-01,  ..., -3.5315e-01,\n",
       "             1.1610e+00,  7.5934e-01],\n",
       "           [-2.2621e+00,  7.7717e-01,  6.7195e-01,  ...,  3.5359e-01,\n",
       "            -1.1626e+00, -7.6043e-01]],\n",
       " \n",
       "          [[ 2.8943e+00, -2.2587e+00,  5.9896e-01,  ..., -1.8152e+00,\n",
       "             7.6411e-01,  1.7204e+00],\n",
       "           [ 1.6921e+00, -1.3206e+00,  3.5024e-01,  ..., -1.0626e+00,\n",
       "             4.4735e-01,  1.0073e+00],\n",
       "           [-3.8237e-01,  2.9845e-01, -7.9159e-02,  ...,  2.4043e-01,\n",
       "            -1.0123e-01, -2.2796e-01],\n",
       "           ...,\n",
       "           [ 5.0962e-01, -3.9822e-01,  1.0574e-01,  ..., -3.3275e-01,\n",
       "             1.4026e-01,  3.1619e-01],\n",
       "           [-3.3765e-01,  2.6386e-01, -7.0069e-02,  ...,  2.2075e-01,\n",
       "            -9.3056e-02, -2.0981e-01],\n",
       "           [ 1.8535e-01, -1.4486e-01,  3.8472e-02,  ..., -1.2134e-01,\n",
       "             5.1154e-02,  1.1535e-01]],\n",
       " \n",
       "          [[-2.8425e-01,  1.4686e-01,  3.2602e-02,  ...,  5.4609e-01,\n",
       "            -1.7764e-01,  5.3276e-01],\n",
       "           [-7.6663e-02,  3.9611e-02,  8.7944e-03,  ...,  1.4747e-01,\n",
       "            -4.7975e-02,  1.4390e-01],\n",
       "           [-2.1553e-02,  1.1138e-02,  2.4730e-03,  ...,  4.1515e-02,\n",
       "            -1.3507e-02,  4.0518e-02],\n",
       "           ...,\n",
       "           [-8.9985e-02,  4.6550e-02,  1.0348e-02,  ...,  1.7998e-01,\n",
       "            -5.8622e-02,  1.7605e-01],\n",
       "           [-5.5180e-01,  2.8548e-01,  6.3465e-02,  ...,  1.1051e+00,\n",
       "            -3.5999e-01,  1.0812e+00],\n",
       "           [ 5.8325e-01, -3.0178e-01, -6.7096e-02,  ..., -1.1696e+00,\n",
       "             3.8104e-01, -1.1445e+00]]]], device='cuda:0',\n",
       "        grad_fn=<PermuteBackward0>),\n",
       " tensor([[[[-7.4025e-03,  1.6752e-01, -1.1661e-01,  ..., -1.2046e-01,\n",
       "             9.0753e-02, -1.5698e-01],\n",
       "           [ 3.5913e-01, -4.3101e-01,  2.4679e-01,  ..., -9.4226e-02,\n",
       "             2.1845e-01, -6.4281e-01],\n",
       "           [ 2.5762e-01,  1.2452e-01, -1.7974e-01,  ...,  3.6121e-02,\n",
       "             4.8063e-02,  2.0713e-01],\n",
       "           ...,\n",
       "           [-7.4810e-01,  1.6011e-01,  5.9660e-01,  ...,  1.3932e+00,\n",
       "            -3.9050e-01,  1.5445e+00],\n",
       "           [-1.6385e-02,  2.0419e-01, -1.9570e-01,  ...,  7.9801e-01,\n",
       "            -2.7987e-01,  6.5131e-02],\n",
       "           [ 3.9747e-01, -1.0574e+00, -4.6510e-01,  ...,  1.9219e+00,\n",
       "            -1.6911e+00, -6.3095e-01]],\n",
       " \n",
       "          [[ 3.2465e-01,  2.1484e-01,  9.2289e-01,  ...,  1.0279e+00,\n",
       "            -1.1841e+00,  3.6214e+00],\n",
       "           [-3.6238e-01, -5.0435e-01, -1.9373e+00,  ..., -1.7223e-01,\n",
       "             5.0559e-01,  4.8643e-01],\n",
       "           [-1.4246e-01,  1.6282e-01,  5.6780e-02,  ..., -4.4296e-01,\n",
       "            -1.9489e-01, -2.1104e-01],\n",
       "           ...,\n",
       "           [-1.7245e+00,  1.4978e+00,  1.5294e+00,  ...,  2.6796e+00,\n",
       "            -4.4884e-01,  6.7061e-01],\n",
       "           [ 4.3518e-01,  4.1689e-02, -9.6184e-01,  ..., -4.8749e-01,\n",
       "            -1.0512e+00,  9.3889e-02],\n",
       "           [-3.7542e-01,  1.2594e+00,  1.0292e+00,  ...,  2.9211e-01,\n",
       "            -7.3088e-01,  2.8544e-01]],\n",
       " \n",
       "          [[ 1.3204e+00, -3.0743e-02,  1.2860e+00,  ..., -7.9433e-01,\n",
       "             1.0251e-01,  3.0835e-01],\n",
       "           [ 1.6474e+00,  1.2952e+00, -1.0042e+00,  ...,  7.3240e-01,\n",
       "            -1.9439e+00, -1.2913e+00],\n",
       "           [-3.3442e-01,  7.7286e-01, -3.1121e-01,  ...,  6.8901e-02,\n",
       "             1.9602e-01,  1.6568e-03],\n",
       "           ...,\n",
       "           [ 3.1060e-01,  7.8580e-01,  2.2773e-01,  ...,  2.1366e+00,\n",
       "             5.0646e-01,  4.2573e-01],\n",
       "           [ 7.3184e-02, -8.4089e-02,  4.8492e-02,  ...,  4.7252e-02,\n",
       "            -1.7674e-01, -1.7465e-01],\n",
       "           [ 5.8279e-04, -1.4640e-01,  7.9848e-02,  ..., -4.0479e-01,\n",
       "            -1.6093e-01, -2.8609e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-7.6890e-01,  3.2656e-01, -5.3102e-01,  ...,  2.7781e-01,\n",
       "            -9.0354e-02, -1.1446e+00],\n",
       "           [ 1.3527e-01, -1.7481e-01, -7.7940e-01,  ..., -1.2697e-01,\n",
       "             4.6904e-02,  3.2568e-01],\n",
       "           [-2.1859e-01,  4.8095e-01, -2.8404e-01,  ...,  4.8119e-01,\n",
       "            -1.6900e-02, -5.5962e-01],\n",
       "           ...,\n",
       "           [-1.0209e-01, -3.1073e-01, -6.5271e-01,  ..., -1.8543e-01,\n",
       "            -1.3913e+00, -3.7514e-01],\n",
       "           [-1.8700e-01, -2.7767e-01, -9.6581e-02,  ...,  3.2447e-02,\n",
       "             1.9215e-01, -2.8892e-01],\n",
       "           [-8.3542e-01,  3.4075e-01, -1.6867e-01,  ...,  6.7474e-01,\n",
       "            -3.7413e-01, -2.9845e-01]],\n",
       " \n",
       "          [[ 1.0309e+00, -8.0214e-01, -9.3844e-01,  ..., -5.9622e-01,\n",
       "             9.3684e-01,  1.2986e+00],\n",
       "           [ 2.7475e-01,  7.5440e-01,  3.7296e-01,  ...,  1.3992e+00,\n",
       "             9.4902e-01, -1.4501e+00],\n",
       "           [ 5.8028e-01, -1.1775e+00, -6.8588e-01,  ...,  1.2867e-01,\n",
       "             1.1575e-01, -4.6513e-01],\n",
       "           ...,\n",
       "           [-6.4181e-01, -2.1087e-01, -2.4043e-01,  ..., -7.9903e-01,\n",
       "            -4.9942e-03, -7.4238e-02],\n",
       "           [ 8.8319e-02,  3.8050e-01, -3.5189e-01,  ...,  6.2394e-01,\n",
       "             8.7085e-01, -7.3281e-01],\n",
       "           [ 2.3987e-01, -1.1292e-01, -2.4600e-01,  ...,  1.5500e-01,\n",
       "             6.0376e-01, -4.5643e-01]],\n",
       " \n",
       "          [[ 7.6852e-03,  6.4181e-02, -8.1301e-02,  ...,  3.1741e-01,\n",
       "            -4.8978e-01, -4.4456e-01],\n",
       "           [ 2.5874e-01, -7.7373e-01,  1.4743e+00,  ...,  1.9786e+00,\n",
       "             1.1403e+00, -6.1272e-01],\n",
       "           [ 2.7359e-01, -4.2885e-02, -2.5894e-02,  ..., -4.4813e-01,\n",
       "            -3.3314e-01,  1.3528e-01],\n",
       "           ...,\n",
       "           [ 4.9141e-01,  1.4857e+00, -3.0906e+00,  ...,  1.7342e+00,\n",
       "             1.7115e+00, -3.6100e-01],\n",
       "           [-2.0845e+00,  2.3905e+00, -2.3974e+00,  ..., -1.9287e+00,\n",
       "             2.0779e+00,  1.3772e-01],\n",
       "           [-2.7723e-01, -2.2717e-02,  2.3888e-01,  ...,  1.9118e-01,\n",
       "             1.8830e-01, -5.5928e-01]]],\n",
       " \n",
       " \n",
       "         [[[-8.3423e-01, -7.5680e-01, -2.2160e+00,  ...,  2.2926e+00,\n",
       "             1.7652e+00,  2.3677e+00],\n",
       "           [-1.0308e-01, -1.1363e-01, -9.3317e-02,  ...,  1.6085e-01,\n",
       "             2.4139e-01, -1.5778e-01],\n",
       "           [ 3.6084e-02, -2.2167e-01,  8.6883e-02,  ...,  5.6044e-02,\n",
       "             1.1373e-01, -5.4225e-01],\n",
       "           ...,\n",
       "           [ 8.4104e-01,  2.1068e+00,  3.4847e+00,  ..., -1.5222e+00,\n",
       "            -3.2641e-02, -9.4167e-01],\n",
       "           [-6.5620e-01,  4.0468e-01, -1.3353e-01,  ...,  7.5952e-01,\n",
       "             7.9172e-01, -5.8064e-02],\n",
       "           [-6.5499e-02,  2.3297e-02, -3.0295e-02,  ...,  1.9417e-01,\n",
       "            -1.6261e-01,  1.0729e-01]],\n",
       " \n",
       "          [[-5.1740e-01, -8.7357e-01,  2.9980e-01,  ..., -1.1975e+00,\n",
       "            -2.0253e-01, -1.3110e-01],\n",
       "           [-1.8124e-02,  3.9769e-02, -2.4890e-02,  ...,  1.7201e-01,\n",
       "             1.5421e-01,  2.1324e-02],\n",
       "           [ 3.9706e-01, -1.0642e+00, -7.9652e-01,  ...,  1.1079e+00,\n",
       "            -3.7438e+00,  3.7986e+00],\n",
       "           ...,\n",
       "           [-9.8854e-01,  1.0370e+00, -7.9655e-01,  ..., -3.8339e-01,\n",
       "            -1.2047e+00, -7.0385e-01],\n",
       "           [-2.3283e-01, -2.9259e-02,  1.0417e+00,  ...,  2.0845e+00,\n",
       "             1.7948e+00,  6.6248e-01],\n",
       "           [ 2.9897e-01,  4.6705e-01,  1.8448e+00,  ...,  1.8539e+00,\n",
       "            -1.5623e+00, -5.4754e-01]],\n",
       " \n",
       "          [[ 1.7971e+00, -1.7424e+00,  1.5812e+00,  ...,  2.1115e-01,\n",
       "            -1.9528e+00, -7.2177e-01],\n",
       "           [ 3.0378e-01,  1.8701e-01, -3.2535e-01,  ...,  3.1959e-01,\n",
       "             1.3689e-01, -2.4671e-01],\n",
       "           [-8.1876e-01,  1.4248e+00,  2.6136e-01,  ..., -2.5707e-01,\n",
       "            -6.9111e-01,  1.0591e+00],\n",
       "           ...,\n",
       "           [ 1.8723e+00, -1.9861e+00, -4.1861e-01,  ...,  1.3497e+00,\n",
       "            -4.3638e-01,  1.6384e+00],\n",
       "           [ 2.2284e+00,  2.5658e+00,  9.5986e-02,  ...,  1.0693e+00,\n",
       "            -1.3694e+00,  2.2892e+00],\n",
       "           [-1.1869e+00, -9.3480e-01, -3.7667e-01,  ...,  4.9633e-03,\n",
       "             3.4680e-01, -8.1419e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.8198e-02,  7.9126e-02,  4.2414e-02,  ..., -1.8428e-01,\n",
       "            -2.0142e-01, -1.9082e-01],\n",
       "           [ 1.4172e-02, -1.3866e-02, -8.2689e-02,  ..., -3.4894e-02,\n",
       "             1.8330e-01,  1.7297e-01],\n",
       "           [ 6.5817e-01, -4.5684e-01, -1.1887e-01,  ..., -7.2011e-01,\n",
       "             1.0413e+00,  5.8180e-01],\n",
       "           ...,\n",
       "           [ 2.6227e-02, -6.6492e-01,  4.6982e-02,  ..., -3.7434e-01,\n",
       "             4.0047e-01, -3.3303e-01],\n",
       "           [ 1.7904e-01,  3.2306e-01,  3.6742e-02,  ...,  6.6430e-02,\n",
       "            -1.2138e-01,  3.4055e-03],\n",
       "           [-1.1710e-01,  1.2561e+00,  2.4351e-03,  ..., -8.7070e-01,\n",
       "             3.2859e-01,  3.2210e-01]],\n",
       " \n",
       "          [[-6.1500e-01,  1.8539e+00,  1.2072e+00,  ..., -5.6486e-01,\n",
       "             5.3075e-03,  2.1435e+00],\n",
       "           [ 1.7651e+00, -7.3255e-01,  7.5376e-01,  ...,  8.0883e-02,\n",
       "             1.3665e+00, -3.0794e-01],\n",
       "           [-6.5036e-02,  9.1339e-02, -2.0199e-02,  ..., -2.0995e-01,\n",
       "             1.9832e-01,  3.1490e-01],\n",
       "           ...,\n",
       "           [-4.0649e-01, -5.0832e-01, -5.4405e-01,  ..., -4.7917e-03,\n",
       "             1.5175e-01, -1.0160e+00],\n",
       "           [-3.2071e-01, -8.7237e-03,  3.1847e-01,  ..., -4.7196e-01,\n",
       "             5.1665e-01,  1.4723e+00],\n",
       "           [-2.0568e-01, -1.1307e+00, -3.3395e-01,  ...,  6.7424e-01,\n",
       "            -8.0705e-02, -1.3810e+00]],\n",
       " \n",
       "          [[-8.2786e-01, -5.7094e-02,  6.7393e-01,  ...,  2.4093e-01,\n",
       "             4.5591e-01,  4.5954e-01],\n",
       "           [-2.8960e-01,  1.2629e-01, -1.6738e-01,  ..., -2.9256e-01,\n",
       "             7.1823e-01,  4.4270e-01],\n",
       "           [-2.1211e-01,  1.1832e+00, -1.5145e+00,  ..., -4.8773e-01,\n",
       "            -2.4801e-01,  2.8925e-01],\n",
       "           ...,\n",
       "           [ 2.4016e-01, -1.3081e-01,  2.8033e-02,  ...,  5.4250e-02,\n",
       "            -1.8721e-01, -8.9953e-02],\n",
       "           [-2.6787e-01,  2.3561e-01,  5.9926e-01,  ...,  3.8061e-01,\n",
       "            -4.0937e-01, -6.9741e-01],\n",
       "           [ 3.8423e-01, -1.2817e+00,  2.9300e-01,  ..., -1.2931e+00,\n",
       "             9.3037e-01,  1.3921e-01]]],\n",
       " \n",
       " \n",
       "         [[[-9.5449e-01, -1.6469e-02,  9.8620e-01,  ..., -1.2072e+00,\n",
       "             1.9661e-01, -2.3511e-01],\n",
       "           [-1.3571e+00,  5.6214e-01,  2.6915e-01,  ...,  1.1788e+00,\n",
       "             4.6870e-01, -7.6330e-01],\n",
       "           [ 3.4478e-01, -6.8071e-01,  9.0920e-01,  ..., -6.4349e-03,\n",
       "             1.1007e+00,  1.1028e+00],\n",
       "           ...,\n",
       "           [ 1.6911e+00,  3.6076e+00, -4.0234e+00,  ...,  3.1023e-01,\n",
       "            -3.1861e-02,  1.8438e+00],\n",
       "           [-4.4364e-01,  5.8739e-01,  9.5652e-01,  ...,  1.2464e-01,\n",
       "             1.5425e-02,  4.8290e-01],\n",
       "           [-1.6491e+00,  1.2955e-01,  2.1895e-01,  ..., -1.4915e+00,\n",
       "            -2.4563e-01, -1.6031e-01]],\n",
       " \n",
       "          [[ 2.0855e+00,  9.8602e+00,  3.7881e+00,  ..., -2.6233e+00,\n",
       "             2.4143e+00, -2.8000e+00],\n",
       "           [-8.3282e-02,  6.3587e-02,  3.2442e-01,  ..., -1.2600e-01,\n",
       "             1.3695e-01,  3.3713e-01],\n",
       "           [-1.6453e-02,  2.5838e-01,  5.8499e-02,  ...,  1.5834e-01,\n",
       "             5.0872e-02, -1.1751e-01],\n",
       "           ...,\n",
       "           [-7.1405e-02, -9.2455e-02, -5.7491e-02,  ..., -5.6261e-02,\n",
       "             2.1334e-02,  1.2938e-02],\n",
       "           [ 3.7297e-02,  3.5965e-03,  6.0758e-01,  ...,  3.2477e-01,\n",
       "            -1.4478e-01, -2.4392e-01],\n",
       "           [ 3.5652e-01, -4.6011e-01, -3.7911e-01,  ...,  1.9713e-01,\n",
       "            -8.6335e-02,  4.1024e-01]],\n",
       " \n",
       "          [[ 2.8706e-01, -1.8361e+00, -4.5520e-02,  ..., -1.4849e-01,\n",
       "             1.9271e+00, -3.1787e-02],\n",
       "           [ 1.2159e-01,  3.9444e-03,  1.6824e-01,  ..., -1.8617e+00,\n",
       "             2.4698e-01, -5.8356e-01],\n",
       "           [ 4.3837e+00, -1.3589e+00, -3.7800e+00,  ..., -1.7105e+00,\n",
       "             3.9941e+00,  4.3211e+00],\n",
       "           ...,\n",
       "           [-1.0407e-01, -8.1763e-02, -2.8977e-02,  ...,  3.0311e-02,\n",
       "            -1.3642e-01, -1.1163e-01],\n",
       "           [ 1.6767e+00, -1.3287e+00,  7.8966e-01,  ..., -1.0052e+00,\n",
       "             4.4785e+00,  6.7402e-01],\n",
       "           [ 1.5957e+00, -2.5535e+00,  3.2599e+00,  ...,  1.1632e+00,\n",
       "            -4.0552e-01,  7.8562e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 1.5200e+00,  3.4641e-01, -5.1416e-01,  ...,  2.4998e+00,\n",
       "             1.5249e+00, -1.7134e+00],\n",
       "           [-4.3719e-01,  8.0745e-01, -2.2236e-01,  ..., -1.6294e-01,\n",
       "            -1.4458e-01, -2.8475e-01],\n",
       "           [ 5.0072e-02, -3.3006e-02,  2.2357e-03,  ...,  4.7769e-02,\n",
       "            -4.7003e-02,  4.3421e-02],\n",
       "           ...,\n",
       "           [ 2.9184e-01,  5.3491e-01, -3.2917e-01,  ...,  7.7951e-01,\n",
       "             5.2590e-01, -1.3794e+00],\n",
       "           [-2.2156e-01,  3.6316e-01, -1.2350e-01,  ..., -1.5238e-01,\n",
       "             7.1795e-01,  2.5387e-01],\n",
       "           [-9.8554e-02,  2.2292e-01,  8.8306e-02,  ...,  3.1682e-02,\n",
       "            -4.6942e-02,  2.9431e-01]],\n",
       " \n",
       "          [[-9.6348e-01,  1.4408e+00,  1.2460e-01,  ..., -2.3908e-02,\n",
       "             1.0437e+00,  1.1220e+00],\n",
       "           [ 4.9451e-01, -6.0249e-01, -1.7481e+00,  ...,  7.9434e-01,\n",
       "            -1.5688e+00, -1.4288e+00],\n",
       "           [-8.0197e-01,  7.3304e-01, -1.1436e+00,  ..., -9.2199e-01,\n",
       "            -2.9228e-01, -4.8428e-01],\n",
       "           ...,\n",
       "           [ 4.7007e-01, -1.3815e-01, -2.9628e-01,  ...,  1.1616e-01,\n",
       "            -1.9207e-01, -1.6167e-01],\n",
       "           [ 1.3026e+00,  2.0854e-02, -9.5837e-01,  ..., -9.4972e-01,\n",
       "             7.7152e-01, -7.4718e-01],\n",
       "           [ 7.0718e-01, -8.0049e-01, -1.6142e-01,  ..., -1.0794e+00,\n",
       "            -1.2771e+00, -9.8115e-01]],\n",
       " \n",
       "          [[ 1.0924e+00,  1.7015e+00,  1.6970e+00,  ..., -9.0696e-01,\n",
       "            -2.3509e+00,  6.3916e-02],\n",
       "           [ 1.1696e-02,  1.5595e-03, -8.5657e-02,  ...,  5.3426e-02,\n",
       "             1.0087e-02, -4.7413e-02],\n",
       "           [-4.0379e-01,  4.3361e-01, -2.4567e-01,  ...,  1.0970e+00,\n",
       "             3.1120e+00, -1.5068e+00],\n",
       "           ...,\n",
       "           [ 1.4101e-01,  4.1901e-01, -2.7256e-01,  ...,  3.2978e-01,\n",
       "             2.2306e-01,  2.0432e-01],\n",
       "           [ 7.9606e-01, -4.4819e-01, -1.7899e+00,  ..., -1.0124e+00,\n",
       "            -1.9934e+00, -2.0578e+00],\n",
       "           [ 8.6277e-02,  4.5725e-01, -5.0266e-02,  ..., -1.1134e-01,\n",
       "             7.5790e-02,  5.6260e-02]]],\n",
       " \n",
       " \n",
       "         [[[-4.6421e-01,  5.2758e-02,  8.0501e-01,  ...,  4.7036e-01,\n",
       "            -1.0328e+00, -1.4299e+00],\n",
       "           [ 2.5168e-01, -2.2015e-01,  8.1215e-01,  ..., -1.4292e-01,\n",
       "             7.4398e-01,  9.1517e-01],\n",
       "           [-4.0352e-01,  6.6681e-01, -1.6990e+00,  ...,  2.5267e+00,\n",
       "            -2.9758e-01, -7.8078e-01],\n",
       "           ...,\n",
       "           [-5.5147e-01,  3.1289e-01,  6.0420e-02,  ...,  4.8717e-01,\n",
       "            -7.6630e-01,  5.8516e-01],\n",
       "           [-2.5067e-01, -3.3491e-01, -2.0182e-01,  ..., -2.5143e-01,\n",
       "            -8.8022e-02, -1.5802e-01],\n",
       "           [-1.1462e-01,  4.5315e-01, -3.5194e-02,  ..., -1.5353e-01,\n",
       "            -1.5087e-01,  1.1988e-01]],\n",
       " \n",
       "          [[-2.0351e-02, -8.7165e-02,  1.2302e-01,  ..., -9.1855e-02,\n",
       "            -2.0142e-01, -2.1952e-01],\n",
       "           [-2.1455e-02, -6.7595e-01,  3.4490e-01,  ...,  1.4412e+00,\n",
       "             5.0346e-01, -4.7608e-01],\n",
       "           [-6.5515e-01, -1.0508e+00,  9.2987e-01,  ...,  1.0216e+00,\n",
       "            -1.1754e+00, -7.7102e-01],\n",
       "           ...,\n",
       "           [-1.5140e+00, -7.9570e-01, -8.2981e-01,  ..., -9.4906e-01,\n",
       "             2.0095e+00,  1.5987e+00],\n",
       "           [-3.9343e-01, -1.0885e+00, -5.3184e-01,  ..., -1.4013e-01,\n",
       "            -7.2242e-01,  1.0255e+00],\n",
       "           [-7.2199e-03,  1.7458e-03, -1.1474e-02,  ...,  1.9985e-03,\n",
       "             5.7501e-04,  6.0187e-03]],\n",
       " \n",
       "          [[-1.7693e-01,  1.6637e-01,  1.9749e-01,  ...,  3.3673e-01,\n",
       "             8.8017e-02, -2.3190e-01],\n",
       "           [-3.8918e-01,  2.7464e-02, -7.1507e-02,  ...,  5.7787e-01,\n",
       "             8.6777e-01,  2.7036e-01],\n",
       "           [ 9.5330e-01, -3.4652e-01,  7.0373e-01,  ...,  1.3596e+00,\n",
       "            -1.0545e+00, -8.9515e-01],\n",
       "           ...,\n",
       "           [-2.4361e-01,  9.8248e-01,  1.8737e-01,  ...,  3.3205e-01,\n",
       "             1.0323e+00, -4.7096e-02],\n",
       "           [ 1.1570e-01, -1.1113e-02,  9.0770e-02,  ..., -6.5545e-03,\n",
       "             1.0731e-02, -7.0758e-02],\n",
       "           [-6.0852e-01, -4.9010e-01, -1.9811e-01,  ..., -4.4054e-01,\n",
       "             1.8334e-01, -3.4057e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.9981e-01,  5.0155e-01,  2.4617e-01,  ..., -3.2750e-01,\n",
       "            -1.7799e+00, -2.4989e+00],\n",
       "           [ 3.5658e+00,  1.1889e+00, -6.2005e-01,  ..., -2.1623e+00,\n",
       "             2.5995e+00,  4.3491e+00],\n",
       "           [ 9.2966e-01, -1.0028e+00, -1.0865e+00,  ..., -1.3627e+00,\n",
       "            -1.1954e+00, -3.2057e-01],\n",
       "           ...,\n",
       "           [-1.2842e-01,  1.7345e-01,  2.0966e-01,  ...,  6.8719e-02,\n",
       "             3.4054e-01, -5.0954e-02],\n",
       "           [-3.0627e+00, -2.3454e+00,  2.5011e+00,  ..., -4.6471e+00,\n",
       "             3.9355e+00, -2.4404e+00],\n",
       "           [ 2.0583e+00,  6.2094e-01,  2.0064e+00,  ...,  1.9172e-01,\n",
       "            -4.8795e-02,  1.7882e+00]],\n",
       " \n",
       "          [[ 3.4666e+00, -5.8002e-01, -4.2880e-01,  ...,  3.7167e-01,\n",
       "             6.0054e-01, -4.0265e-01],\n",
       "           [-2.1647e+00, -3.5824e-01,  1.5095e+00,  ..., -3.2471e-01,\n",
       "            -3.6152e+00, -1.7018e+00],\n",
       "           [-9.0443e-03,  3.9695e-02, -6.4878e-03,  ..., -3.9946e-02,\n",
       "             2.5412e-02,  1.3771e-02],\n",
       "           ...,\n",
       "           [-6.4949e-01,  2.6623e+00, -3.9753e-01,  ..., -6.3765e-02,\n",
       "             1.1921e+00,  2.0808e+00],\n",
       "           [ 4.4470e-01,  2.8761e+00, -6.6685e-01,  ...,  1.7836e+00,\n",
       "             1.2773e+00,  1.4269e+00],\n",
       "           [-1.3470e-01,  7.3804e-02, -2.7624e-01,  ..., -9.8734e-01,\n",
       "            -1.0595e-01,  8.2531e-02]],\n",
       " \n",
       "          [[-2.2135e-01, -4.7911e-01,  5.6629e-01,  ...,  3.0813e-01,\n",
       "             1.3928e+00, -2.6761e-01],\n",
       "           [ 1.4037e-01, -6.6925e-01,  8.6780e-01,  ...,  2.3469e+00,\n",
       "            -6.3214e-01,  1.0122e+00],\n",
       "           [-2.8878e-02, -1.6203e+00,  2.2000e+00,  ...,  9.3690e-01,\n",
       "             5.5848e-01,  5.0456e-01],\n",
       "           ...,\n",
       "           [-4.0674e-02, -1.7191e-01,  2.3917e-01,  ...,  2.5650e-01,\n",
       "            -7.0610e-01, -2.6216e-01],\n",
       "           [-3.9240e-01,  3.3647e-01, -7.6731e-01,  ...,  7.5421e-03,\n",
       "             5.4046e-01,  4.4591e-01],\n",
       "           [ 6.4464e-01, -1.3371e+00,  1.6274e+00,  ..., -4.6580e-01,\n",
       "            -7.1205e-01, -9.6050e-02]]]], device='cuda:0', grad_fn=<CopySlices>))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ctx = {}\n",
    "\n",
    "score = torch.randn(4, 16, 1024, 1024, dtype=torch.float32, requires_grad=True, device='cuda:0')\n",
    "\n",
    "V = torch.randn(4, 16, 1024, 16, dtype=torch.float32, requires_grad=True, device='cuda:0')\n",
    "\n",
    "result = torch.randn(4, 16, 1024, 16, dtype=torch.float32, requires_grad=True, device='cuda:0')\n",
    "\n",
    "grad_output = torch.randn(4, 16, 1024, 16, dtype=torch.float32, requires_grad=True, device='cuda:0')\n",
    "\n",
    "class ctx_class:\n",
    "    def __init__(self):\n",
    "        self.saved_tensors = [score, V, result]\n",
    "\n",
    "\n",
    "ctx = ctx_class()\n",
    "\n",
    "# focusAttention.forward(ctx, score, V)\n",
    "\n",
    "focusAttention.backward(ctx, grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before V clone 17047552\n",
      "After V clone 17049600\n",
      "before loop 17051136\n",
      "start iter 17051136\n",
      "end iter 17053696\n",
      "start iter 17053696\n",
      "end iter 17054208\n",
      "start iter 17054208\n",
      "end iter 17054720\n",
      "start iter 17054720\n",
      "end iter 17055232\n",
      "start iter 17055232\n",
      "end iter 17055744\n",
      "start iter 17055744\n",
      "end iter 17056256\n",
      "start iter 17056256\n",
      "end iter 17056768\n",
      "start iter 17056768\n",
      "end iter 17057280\n",
      "start iter 17057280\n",
      "end iter 17057792\n",
      "start iter 17057792\n",
      "end iter 17058304\n",
      "start iter 17058304\n",
      "end iter 17058816\n",
      "start iter 17058816\n",
      "end iter 17059328\n",
      "start iter 17059328\n",
      "end iter 17059840\n",
      "start iter 17059840\n",
      "end iter 17060352\n",
      "start iter 17060352\n",
      "end iter 17060864\n",
      "start iter 17060864\n",
      "end iter 17061376\n",
      "start iter 17061376\n",
      "end iter 17061888\n",
      "start iter 17061888\n",
      "end iter 17062400\n",
      "start iter 17062400\n",
      "end iter 17062912\n",
      "start iter 17062912\n",
      "end iter 17063424\n",
      "start iter 17063424\n",
      "end iter 17063936\n",
      "start iter 17063936\n",
      "end iter 17064448\n",
      "start iter 17064448\n",
      "end iter 17064960\n",
      "start iter 17064960\n",
      "end iter 17065472\n",
      "start iter 17065472\n",
      "end iter 17065984\n",
      "start iter 17065984\n",
      "end iter 17066496\n",
      "start iter 17066496\n",
      "end iter 17067008\n",
      "start iter 17067008\n",
      "end iter 17067520\n",
      "start iter 17067520\n",
      "end iter 17068032\n",
      "start iter 17068032\n",
      "end iter 17068544\n",
      "start iter 17068544\n",
      "end iter 17069056\n",
      "start iter 17069056\n",
      "end iter 17069568\n",
      "start iter 17069568\n",
      "end iter 17070080\n",
      "start iter 17070080\n",
      "end iter 17070592\n",
      "start iter 17070592\n",
      "end iter 17071104\n",
      "start iter 17071104\n",
      "end iter 17071616\n",
      "start iter 17071616\n",
      "end iter 17072128\n",
      "start iter 17072128\n",
      "end iter 17072640\n",
      "start iter 17072640\n",
      "end iter 17073152\n",
      "start iter 17073152\n",
      "end iter 17073664\n",
      "start iter 17073664\n",
      "end iter 17074176\n",
      "start iter 17074176\n",
      "end iter 17074688\n",
      "start iter 17074688\n",
      "end iter 17075200\n",
      "start iter 17075200\n",
      "end iter 17075712\n",
      "start iter 17075712\n",
      "end iter 17076224\n",
      "start iter 17076224\n",
      "end iter 17076736\n",
      "start iter 17076736\n",
      "end iter 17077248\n",
      "start iter 17077248\n",
      "end iter 17077760\n",
      "start iter 17077760\n",
      "end iter 17078272\n",
      "start iter 17078272\n",
      "end iter 17078784\n",
      "start iter 17078784\n",
      "end iter 17079296\n",
      "start iter 17079296\n",
      "end iter 17079808\n",
      "start iter 17079808\n",
      "end iter 17080320\n",
      "start iter 17080320\n",
      "end iter 17080832\n",
      "start iter 17080832\n",
      "end iter 17081344\n",
      "start iter 17081344\n",
      "end iter 17081856\n",
      "start iter 17081856\n",
      "end iter 17082368\n",
      "start iter 17082368\n",
      "end iter 17082880\n",
      "start iter 17082880\n",
      "end iter 17083392\n",
      "start iter 17083392\n",
      "end iter 17083904\n",
      "start iter 17083904\n",
      "end iter 17084416\n",
      "start iter 17084416\n",
      "end iter 17084928\n",
      "start iter 17084928\n",
      "end iter 17085440\n",
      "start iter 17085440\n",
      "end iter 17085952\n",
      "end loop used 34816\n",
      "before V clone 17047552\n",
      "After V clone 17049600\n",
      "before loop 17051136\n",
      "start iter 17051136\n",
      "end iter 17053696\n",
      "start iter 17053696\n",
      "end iter 17054208\n",
      "start iter 17054208\n",
      "end iter 17054720\n",
      "start iter 17054720\n",
      "end iter 17055232\n",
      "start iter 17055232\n",
      "end iter 17055744\n",
      "start iter 17055744\n",
      "end iter 17056256\n",
      "start iter 17056256\n",
      "end iter 17056768\n",
      "start iter 17056768\n",
      "end iter 17057280\n",
      "start iter 17057280\n",
      "end iter 17057792\n",
      "start iter 17057792\n",
      "end iter 17058304\n",
      "start iter 17058304\n",
      "end iter 17058816\n",
      "start iter 17058816\n",
      "end iter 17059328\n",
      "start iter 17059328\n",
      "end iter 17059840\n",
      "start iter 17059840\n",
      "end iter 17060352\n",
      "start iter 17060352\n",
      "end iter 17060864\n",
      "start iter 17060864\n",
      "end iter 17061376\n",
      "start iter 17061376\n",
      "end iter 17061888\n",
      "start iter 17061888\n",
      "end iter 17062400\n",
      "start iter 17062400\n",
      "end iter 17062912\n",
      "start iter 17062912\n",
      "end iter 17063424\n",
      "start iter 17063424\n",
      "end iter 17063936\n",
      "start iter 17063936\n",
      "end iter 17064448\n",
      "start iter 17064448\n",
      "end iter 17064960\n",
      "start iter 17064960\n",
      "end iter 17065472\n",
      "start iter 17065472\n",
      "end iter 17065984\n",
      "start iter 17065984\n",
      "end iter 17066496\n",
      "start iter 17066496\n",
      "end iter 17067008\n",
      "start iter 17067008\n",
      "end iter 17067520\n",
      "start iter 17067520\n",
      "end iter 17068032\n",
      "start iter 17068032\n",
      "end iter 17068544\n",
      "start iter 17068544\n",
      "end iter 17069056\n",
      "start iter 17069056\n",
      "end iter 17069568\n",
      "start iter 17069568\n",
      "end iter 17070080\n",
      "start iter 17070080\n",
      "end iter 17070592\n",
      "start iter 17070592\n",
      "end iter 17071104\n",
      "start iter 17071104\n",
      "end iter 17071616\n",
      "start iter 17071616\n",
      "end iter 17072128\n",
      "start iter 17072128\n",
      "end iter 17072640\n",
      "start iter 17072640\n",
      "end iter 17073152\n",
      "start iter 17073152\n",
      "end iter 17073664\n",
      "start iter 17073664\n",
      "end iter 17074176\n",
      "start iter 17074176\n",
      "end iter 17074688\n",
      "start iter 17074688\n",
      "end iter 17075200\n",
      "start iter 17075200\n",
      "end iter 17075712\n",
      "start iter 17075712\n",
      "end iter 17076224\n",
      "start iter 17076224\n",
      "end iter 17076736\n",
      "start iter 17076736\n",
      "end iter 17077248\n",
      "start iter 17077248\n",
      "end iter 17077760\n",
      "start iter 17077760\n",
      "end iter 17078272\n",
      "start iter 17078272\n",
      "end iter 17078784\n",
      "start iter 17078784\n",
      "end iter 17079296\n",
      "start iter 17079296\n",
      "end iter 17079808\n",
      "start iter 17079808\n",
      "end iter 17080320\n",
      "start iter 17080320\n",
      "end iter 17080832\n",
      "start iter 17080832\n",
      "end iter 17081344\n",
      "start iter 17081344\n",
      "end iter 17081856\n",
      "start iter 17081856\n",
      "end iter 17082368\n",
      "start iter 17082368\n",
      "end iter 17082880\n",
      "start iter 17082880\n",
      "end iter 17083392\n",
      "start iter 17083392\n",
      "end iter 17083904\n",
      "start iter 17083904\n",
      "end iter 17084416\n",
      "start iter 17084416\n",
      "end iter 17084928\n",
      "start iter 17084928\n",
      "end iter 17085440\n",
      "start iter 17085440\n",
      "end iter 17085952\n",
      "end loop used 34816\n",
      "tensor([[[[17.4191, 16.3169],\n",
      "          [15.6101, 14.2356],\n",
      "          [16.0185, 13.7510],\n",
      "          [14.7444, 14.7128],\n",
      "          [13.3954, 13.2356],\n",
      "          [16.1997, 18.0319],\n",
      "          [16.9160, 16.2648],\n",
      "          [13.8602, 12.7434],\n",
      "          [15.0947, 15.5169],\n",
      "          [15.4313, 16.1146],\n",
      "          [16.2102, 15.5109],\n",
      "          [15.2971, 16.2284],\n",
      "          [16.4217, 16.0739],\n",
      "          [17.6391, 17.4518],\n",
      "          [16.3083, 15.0231],\n",
      "          [17.0068, 16.4986],\n",
      "          [14.9259, 16.1133],\n",
      "          [15.9568, 16.6757],\n",
      "          [15.0296, 15.6000],\n",
      "          [14.8210, 13.7928],\n",
      "          [17.7585, 17.7076],\n",
      "          [14.1601, 14.1066],\n",
      "          [14.7384, 15.3292],\n",
      "          [15.8839, 15.4306],\n",
      "          [16.6239, 15.9097],\n",
      "          [14.3041, 13.0153],\n",
      "          [15.7981, 15.0480],\n",
      "          [13.9335, 13.6556],\n",
      "          [16.3247, 14.6794],\n",
      "          [14.4621, 14.0951],\n",
      "          [14.9112, 15.9874],\n",
      "          [14.9763, 15.5496],\n",
      "          [15.2100, 14.8648],\n",
      "          [17.0470, 16.2941],\n",
      "          [16.1749, 15.4641],\n",
      "          [15.9328, 14.0403],\n",
      "          [15.7112, 16.3857],\n",
      "          [14.3359, 14.1463],\n",
      "          [15.9552, 15.8491],\n",
      "          [17.3538, 15.7612],\n",
      "          [16.6612, 16.8530],\n",
      "          [16.3937, 17.1890],\n",
      "          [16.2257, 15.8294],\n",
      "          [14.5466, 14.0046],\n",
      "          [15.8192, 15.0362],\n",
      "          [15.7531, 15.1658],\n",
      "          [16.4984, 16.0874],\n",
      "          [14.0336, 15.1942],\n",
      "          [16.1531, 15.2905],\n",
      "          [14.3818, 15.6641],\n",
      "          [16.4701, 15.2789],\n",
      "          [16.9449, 15.1901],\n",
      "          [14.8928, 15.6138],\n",
      "          [15.2616, 16.7531],\n",
      "          [16.0690, 15.2002],\n",
      "          [15.0701, 14.6543],\n",
      "          [15.4905, 14.3244],\n",
      "          [15.1456, 14.7341],\n",
      "          [15.7702, 16.2423],\n",
      "          [17.1478, 17.5375],\n",
      "          [16.7885, 17.3183],\n",
      "          [16.1946, 17.1408],\n",
      "          [16.4945, 15.8734],\n",
      "          [13.2419, 13.8956]],\n",
      "\n",
      "         [[15.7385, 13.1109],\n",
      "          [16.5641, 14.0753],\n",
      "          [14.9806, 14.4624],\n",
      "          [16.4818, 14.9865],\n",
      "          [15.1232, 14.2418],\n",
      "          [14.2834, 12.9092],\n",
      "          [17.3048, 16.2664],\n",
      "          [16.0245, 14.9887],\n",
      "          [15.0506, 12.4652],\n",
      "          [14.9396, 13.7689],\n",
      "          [17.5148, 14.3237],\n",
      "          [13.2835, 13.6052],\n",
      "          [15.1752, 13.9092],\n",
      "          [15.7679, 13.1233],\n",
      "          [16.0703, 14.9850],\n",
      "          [15.2473, 12.9591],\n",
      "          [17.2326, 14.9509],\n",
      "          [15.8083, 14.3894],\n",
      "          [18.2390, 14.6574],\n",
      "          [16.4639, 15.6969],\n",
      "          [15.6592, 15.3655],\n",
      "          [18.0863, 15.5805],\n",
      "          [14.1426, 13.3948],\n",
      "          [14.8479, 13.8322],\n",
      "          [15.5880, 12.5676],\n",
      "          [16.3768, 14.2382],\n",
      "          [16.3410, 14.3575],\n",
      "          [16.4480, 14.8350],\n",
      "          [14.4117, 12.9878],\n",
      "          [14.0140, 12.9982],\n",
      "          [17.3960, 14.9890],\n",
      "          [14.4466, 12.4249],\n",
      "          [16.6377, 13.9025],\n",
      "          [15.5737, 14.3422],\n",
      "          [14.7006, 13.6347],\n",
      "          [12.6270, 12.4664],\n",
      "          [14.1797, 12.0186],\n",
      "          [15.2088, 13.7891],\n",
      "          [14.6567, 14.3617],\n",
      "          [15.1310, 14.1375],\n",
      "          [17.0709, 14.6120],\n",
      "          [16.7250, 14.3536],\n",
      "          [14.2336, 12.5784],\n",
      "          [16.7428, 14.3501],\n",
      "          [13.9295, 13.5346],\n",
      "          [15.9039, 15.3692],\n",
      "          [15.7303, 14.8006],\n",
      "          [17.8150, 14.6586],\n",
      "          [13.3411, 12.5796],\n",
      "          [15.6479, 13.7979],\n",
      "          [14.2321, 12.5538],\n",
      "          [14.4844, 13.5390],\n",
      "          [14.3095, 12.1352],\n",
      "          [15.4100, 13.8844],\n",
      "          [14.9337, 13.6316],\n",
      "          [16.5540, 15.1235],\n",
      "          [15.6225, 13.4869],\n",
      "          [15.0925, 14.3960],\n",
      "          [14.2661, 13.0217],\n",
      "          [14.0711, 14.0308],\n",
      "          [17.4717, 15.2008],\n",
      "          [16.0612, 13.8311],\n",
      "          [14.5224, 13.4933],\n",
      "          [14.6778, 13.0196]]],\n",
      "\n",
      "\n",
      "        [[[18.1202, 16.2478],\n",
      "          [16.9453, 16.0468],\n",
      "          [15.6319, 16.0014],\n",
      "          [15.7520, 15.2579],\n",
      "          [16.7635, 17.2703],\n",
      "          [16.9449, 17.1818],\n",
      "          [16.7911, 15.3966],\n",
      "          [17.5839, 16.6283],\n",
      "          [15.5447, 14.3002],\n",
      "          [16.6311, 15.2006],\n",
      "          [16.3162, 14.5213],\n",
      "          [15.1700, 14.5355],\n",
      "          [15.8747, 15.5312],\n",
      "          [13.8182, 13.7078],\n",
      "          [15.9954, 15.0328],\n",
      "          [14.7567, 15.8301],\n",
      "          [15.9886, 14.5239],\n",
      "          [17.6541, 16.6654],\n",
      "          [16.6619, 15.7748],\n",
      "          [16.4455, 16.7796],\n",
      "          [17.3845, 16.1545],\n",
      "          [18.3250, 18.8106],\n",
      "          [15.3438, 14.9518],\n",
      "          [17.9842, 17.7865],\n",
      "          [13.7892, 13.5716],\n",
      "          [19.3856, 17.7472],\n",
      "          [16.9385, 16.6553],\n",
      "          [15.5130, 13.2654],\n",
      "          [15.3866, 15.9627],\n",
      "          [16.7489, 14.5173],\n",
      "          [17.3557, 17.0930],\n",
      "          [18.2129, 17.2692],\n",
      "          [16.4571, 17.3454],\n",
      "          [19.0063, 17.4249],\n",
      "          [17.3333, 16.2804],\n",
      "          [17.4329, 16.3503],\n",
      "          [16.6614, 14.8055],\n",
      "          [19.0551, 18.1808],\n",
      "          [16.1838, 15.4434],\n",
      "          [15.5209, 14.9813],\n",
      "          [18.4468, 16.9900],\n",
      "          [18.0500, 16.6771],\n",
      "          [15.7767, 15.0528],\n",
      "          [17.3788, 17.6334],\n",
      "          [17.6388, 16.7678],\n",
      "          [17.0314, 16.3864],\n",
      "          [20.3665, 18.4501],\n",
      "          [18.2706, 16.3829],\n",
      "          [16.5181, 17.5824],\n",
      "          [15.3188, 15.8512],\n",
      "          [17.6311, 16.2964],\n",
      "          [13.9420, 13.3786],\n",
      "          [19.1063, 17.2332],\n",
      "          [16.0011, 15.9379],\n",
      "          [14.9111, 14.3514],\n",
      "          [15.2077, 16.1613],\n",
      "          [19.0633, 17.9025],\n",
      "          [16.0221, 16.0091],\n",
      "          [16.9213, 15.5989],\n",
      "          [16.2633, 15.4337],\n",
      "          [16.4540, 15.8133],\n",
      "          [15.9982, 15.5383],\n",
      "          [16.1014, 15.3968],\n",
      "          [17.6171, 16.5278]],\n",
      "\n",
      "         [[16.4799, 15.0383],\n",
      "          [15.7761, 16.1566],\n",
      "          [16.4497, 16.4783],\n",
      "          [18.3208, 17.1108],\n",
      "          [15.5555, 15.8768],\n",
      "          [16.7331, 16.3390],\n",
      "          [15.8624, 14.1462],\n",
      "          [16.9892, 14.5039],\n",
      "          [16.5252, 16.0781],\n",
      "          [15.0871, 13.0328],\n",
      "          [16.7434, 16.4726],\n",
      "          [17.3608, 15.5928],\n",
      "          [13.8807, 13.9621],\n",
      "          [15.1331, 14.2629],\n",
      "          [15.2766, 15.1541],\n",
      "          [14.7497, 14.1273],\n",
      "          [15.8960, 14.7369],\n",
      "          [16.8812, 15.3017],\n",
      "          [15.6619, 15.8918],\n",
      "          [14.2227, 14.2194],\n",
      "          [15.5322, 14.2813],\n",
      "          [14.9333, 13.6002],\n",
      "          [15.2256, 14.8873],\n",
      "          [16.4074, 15.1693],\n",
      "          [15.3135, 13.2356],\n",
      "          [16.5990, 16.3201],\n",
      "          [17.1005, 16.9220],\n",
      "          [16.7203, 15.8228],\n",
      "          [12.5711, 12.6603],\n",
      "          [14.1457, 15.4709],\n",
      "          [17.7267, 16.9351],\n",
      "          [16.4437, 17.1370],\n",
      "          [15.4407, 15.0989],\n",
      "          [16.9946, 15.3702],\n",
      "          [15.9214, 15.0272],\n",
      "          [15.8851, 14.4223],\n",
      "          [16.6708, 15.5961],\n",
      "          [15.5278, 14.2156],\n",
      "          [14.2324, 15.4201],\n",
      "          [16.2425, 16.0438],\n",
      "          [13.8678, 12.6991],\n",
      "          [16.3849, 14.3619],\n",
      "          [15.4847, 14.3021],\n",
      "          [15.5103, 15.0225],\n",
      "          [15.1862, 14.9282],\n",
      "          [15.2113, 14.6868],\n",
      "          [15.6113, 14.2463],\n",
      "          [14.7048, 15.0373],\n",
      "          [17.6401, 16.0127],\n",
      "          [13.8876, 13.8942],\n",
      "          [18.1348, 17.1613],\n",
      "          [15.5502, 14.4693],\n",
      "          [15.3878, 13.8327],\n",
      "          [16.1134, 13.9376],\n",
      "          [14.3596, 12.9579],\n",
      "          [14.2653, 15.6380],\n",
      "          [15.3375, 13.3232],\n",
      "          [16.1991, 14.4440],\n",
      "          [17.1878, 16.5641],\n",
      "          [16.7109, 14.4953],\n",
      "          [14.9764, 13.8139],\n",
      "          [18.3987, 18.4328],\n",
      "          [16.2245, 15.2192],\n",
      "          [15.0133, 13.8013]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "cuda = torch.device('cuda', 0)\n",
    "B = 2\n",
    "NH = 2\n",
    "T_flatten = 64\n",
    "HS = 2\n",
    "Q = torch.rand(B, NH, T_flatten, HS, device = cuda)\n",
    "K = torch.rand(B, NH, T_flatten, HS, device = cuda)\n",
    "V = torch.rand(B, NH, T_flatten, HS, device = cuda)\n",
    "\n",
    "score = torch.rand(B, NH, T_flatten, T_flatten, device = cuda)\n",
    "\n",
    "A = FocusedAttention(score, V)\n",
    "B = FocusedAttention(score, V)\n",
    "\n",
    "\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import cupy\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import softmax\n",
    "from torch.autograd import Function\n",
    "\n",
    "\n",
    "def getGaussian(T, H, W, beta, d):\n",
    "    diag = np.diag([beta[0], beta[1], beta[1]])\n",
    "    rv = multivariate_normal([T - 1, H - 1, W - 1], diag)\n",
    "    tensor = torch.tensor((), dtype=torch.float32)\n",
    "\n",
    "    NT = 2 * T - 1\n",
    "    NH = 2 * H - 1\n",
    "    NW = 2 * W - 1\n",
    "\n",
    "    weight = tensor.new_ones((NT, NW, NH), device=d)\n",
    "\n",
    "    for pos in np.arange(0, NT * NH * NW):\n",
    "        i = math.floor(pos / (NH * NW))\n",
    "        j = math.floor((pos - i * NH * NW) / NH)\n",
    "        k = pos - i * NH * NW - j * NW\n",
    "        weight[i, j, k] = rv.pdf([i, j, k])\n",
    "\n",
    "        weight = weight / torch.max(weight)\n",
    "\n",
    "    return weight\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class focusAttention(Function):\n",
    "\n",
    "    T, H, W = 4, 4, 4\n",
    "    T_flatten = T * H * W\n",
    "    center_T, center_H, center_W = T - 1, H - 1, W - 1\n",
    "    beta = [100, 100]\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    weight = getGaussian(T, H, W, beta, device)\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, score, V):\n",
    "\n",
    "        att=[]\n",
    "\n",
    "        st = torch.cuda.memory_allocated()\n",
    "\n",
    "        for pos in np.arange(0, focusAttention.T_flatten):\n",
    "\n",
    "            # print(f\"start of loop {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "            i = math.floor(pos / (focusAttention.H * focusAttention.W))\n",
    "            j = math.floor((pos - i * focusAttention.H * focusAttention.W) / focusAttention.H)\n",
    "            k = pos - i * focusAttention.H * focusAttention.W - j * focusAttention.W\n",
    "\n",
    "            # print(f\"Before weight_xyz {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "            weight_xyz = focusAttention.weight[focusAttention.center_T - i:2 * focusAttention.center_T - i + 1, focusAttention.center_W - j:2 * focusAttention.center_W - j + 1,\n",
    "                         focusAttention.center_H - k:2 * focusAttention.center_H - k + 1].reshape(-1)\n",
    "\n",
    "            # print(f\"After sub indexing weight {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "            weight_xyz = weight_xyz[None, None, :, None]\n",
    "\n",
    "            # print(f\"After add axis {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "            # V_focused = V * weight_xyz\n",
    "\n",
    "            # print(f\"After multiply weight {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "            # qk shape (B, NH, 1, T)\n",
    "            qk = score[:, :, pos, :]\n",
    "\n",
    "            # print(f\"After index qk{torch.cuda.memory_allocated()}\")\n",
    "\n",
    "            qk = qk[:, :, None, :]\n",
    "\n",
    "            # print(f\"After add axis to qk {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "            att_pos = torch.matmul(qk, (V * weight_xyz)).detach()\n",
    "\n",
    "            att.append(att_pos)\n",
    "            # V = torch.clone(V_ori)\n",
    "\n",
    "\n",
    "        # print(f\"Before cat {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "        result = torch.cat(att, dim=2).detach()\n",
    "\n",
    "        end = torch.cuda.memory_allocated()\n",
    "\n",
    "        # print(f\"result memory usage is {result.element_size() * result.nelement()}, memory used {end - st}, memory for v is {V.element_size() * V.nelement()}\")\n",
    "\n",
    "        # print(f\"After focused attention, memory usage is {end}, memory used {end - st}\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # print(f\"After empty cache, memory usage is {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "        ctx.save_for_backward(score, V, result)\n",
    "\n",
    "        # print(f\"After save for backwards, memory usage is {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        score, V, result = ctx.saved_tensor\n",
    "\n",
    "        grad_score = []\n",
    "        grad_V = []\n",
    "\n",
    "        for pos in np.arange(0, focusAttention.T_flatten):\n",
    "            grad_att_pos = grad_output[:, :, pos, :]\n",
    "\n",
    "            grad_att_pos = grad_att_pos[:, :, None, :]\n",
    "\n",
    "            i = math.floor(pos / (focusAttention.H * focusAttention.W))\n",
    "            j = math.floor((pos - i * focusAttention.H * focusAttention.W) / focusAttention.H)\n",
    "            k = pos - i * focusAttention.H * focusAttention.W - j * focusAttention.W\n",
    "\n",
    "            weight_xyz = focusAttention.weight[focusAttention.center_T - i:2 * focusAttention.center_T - i + 1, focusAttention.center_W - j:2 * focusAttention.center_W - j + 1,\n",
    "                         focusAttention.center_H - k:2 * focusAttention.center_H - k + 1].reshape(-1)\n",
    "\n",
    "            qk = score[:, :, pos, :]\n",
    "\n",
    "            qk = torch.swapaxes(qk, 2, 3)\n",
    "\n",
    "            grad_V.append(torch.matmul((qk * weight_xyz), grad_att_pos)[:, :, pos, :])\n",
    "            grad_score.append(torch.matmul(weight_xyz, grad_att_pos) * V[:, :, :, 0])\n",
    "\n",
    "            # grad_qk = grad_att_pos @ torch.linalg.inv(V_focus)\n",
    "            # grad_V_focus = torch.linalg.inv(qk) @ grad_att_pos\n",
    "            #\n",
    "            # grad_score.append(grad_qk)\n",
    "            # grad_V_focus = grad_V_focus * weight_xyz\n",
    "            # grad_V.append(grad_V_focus)\n",
    "\n",
    "        # Shape should be B, NH, T, T\n",
    "        grad_score = torch.cat(grad_score, dim=2)\n",
    "\n",
    "        # Shape should be B, NH, T, HS\n",
    "        grad_V = torch.cat(grad_V, dim=2)\n",
    "\n",
    "        return grad_score, grad_V\n",
    "\n",
    "focus = focusAttention.apply\n",
    "\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "input = (torch.randn(1, 1, 64, 64, dtype=torch.float64, requires_grad=True, device='cuda:0'), torch.randn(1, 1, 64, 4, dtype=torch.float64, requires_grad=True, device='cuda:0'))\n",
    "\n",
    "test = gradcheck(focus, input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
