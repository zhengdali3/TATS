{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n",
    "import sys\n",
    "sys.path.insert(0, '/userhome/42/msd21003/TATS')\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from tats import Net2NetTransformer, VideoData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_path:<class 'str'>,sequence_len:<class 'int'>,dataset:<class 'str'>,train:<class 'bool'>,dataset:<class 'type'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'istrain'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 16>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     14\u001B[0m data \u001B[38;5;241m=\u001B[39m VideoData(args)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# pre-make relevant cached files if necessary\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m data\u001B[38;5;241m.\u001B[39mtest_dataloader()\n\u001B[1;32m     19\u001B[0m args\u001B[38;5;241m.\u001B[39mclass_cond_dim \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mn_classes \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args\u001B[38;5;241m.\u001B[39munconditional \u001B[38;5;129;01mand\u001B[39;00m args\u001B[38;5;241m.\u001B[39mcond_stage_key\u001B[38;5;241m==\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/TATS/tats/data.py:318\u001B[0m, in \u001B[0;36mVideoData.train_dataloader\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_dataloader\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 318\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataloader\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/TATS/tats/data.py:297\u001B[0m, in \u001B[0;36mVideoData._dataloader\u001B[0;34m(self, train)\u001B[0m\n\u001B[1;32m    296\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_dataloader\u001B[39m(\u001B[38;5;28mself\u001B[39m, train):\n\u001B[0;32m--> 297\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    298\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dist\u001B[38;5;241m.\u001B[39mis_initialized():\n\u001B[1;32m    299\u001B[0m         sampler \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mdistributed\u001B[38;5;241m.\u001B[39mDistributedSampler(\n\u001B[1;32m    300\u001B[0m             dataset, num_replicas\u001B[38;5;241m=\u001B[39mdist\u001B[38;5;241m.\u001B[39mget_world_size(), rank\u001B[38;5;241m=\u001B[39mdist\u001B[38;5;241m.\u001B[39mget_rank()\n\u001B[1;32m    301\u001B[0m         )\n",
      "File \u001B[0;32m~/TATS/tats/data.py:292\u001B[0m, in \u001B[0;36mVideoData._dataset\u001B[0;34m(self, train)\u001B[0m\n\u001B[1;32m    288\u001B[0m         Dataset \u001B[38;5;241m=\u001B[39m VideoDataset \u001B[38;5;28;01mif\u001B[39;00m osp\u001B[38;5;241m.\u001B[39misdir(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdata_path) \u001B[38;5;28;01melse\u001B[39;00m HDF5Dataset\n\u001B[1;32m    290\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata_path:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdata_path)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,sequence_len:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39msequence_length)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,dataset:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdataset)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,train:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(train)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,dataset:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(Dataset)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 292\u001B[0m         dataset \u001B[38;5;241m=\u001B[39m \u001B[43mDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msequence_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    293\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mistrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresolution\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresolution\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m dataset\n",
      "\u001B[0;31mTypeError\u001B[0m: __init__() got an unexpected keyword argument 'istrain'"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(1234)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser = pl.Trainer.add_argparse_args(parser)\n",
    "parser = Net2NetTransformer.add_model_specific_args(parser)\n",
    "parser = VideoData.add_data_specific_args(parser)\n",
    "\n",
    "args = parser.parse_args(args=[\"--num_workers\", \"32\", \"--val_check_interval\", \" 0.5\", \"--progress_bar_refresh_rate\", \" 500\",\n",
    "                    \"--gpus\", \" 8\" ,\"--sync_batchnorm\" ,\"--batch_size\", \" 3\",  \"--unconditional\",\n",
    "                    \"--vqvae\", \" ../../ckpt/vqgan_ucf.ckpt\", \"--data_path\", \" ../../ucf101\", \"--dataset\", \"ucf101\", \"--default_root_dir\", \" ../../trainGPT_ckpt\",\n",
    "                    \"--vocab_size\", \" 16384\", \"--block_size\", \" 1024\", \"--n_layer\", \" 24\", \"--n_head\", \" 16\", \"--n_embd\", \" 1024\",\n",
    "                    \"--resolution\", \" 128\", \"--sequence_length\", \" 16\", \"--max_steps\", \" 2000000\"])\n",
    "\n",
    "data = VideoData(args)\n",
    "# pre-make relevant cached files if necessary\n",
    "data.train_dataloader()\n",
    "data.test_dataloader()\n",
    "\n",
    "args.class_cond_dim = data.n_classes if not args.unconditional and args.cond_stage_key=='label' else None\n",
    "model = Net2NetTransformer(args, first_stage_key=args.first_stage_key, cond_stage_key=args.cond_stage_key)\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(ModelCheckpoint(every_n_train_steps=10000, save_top_k=-1, filename='{epoch}-{step}-{train/loss:.2f}'))\n",
    "callbacks.append(ModelCheckpoint(every_n_train_steps=50000, save_top_k=-1, filename='{epoch}-{step}-{train/loss:.2f}'))\n",
    "callbacks.append(ModelCheckpoint(monitor='val/loss', mode='min', save_top_k=3, filename='best_checkpoint'))\n",
    "\n",
    "kwargs = dict()\n",
    "if args.gpus > 1:\n",
    "    # find_unused_parameters = False to support gradient checkpointing\n",
    "    kwargs = dict(gpus=args.gpus,\n",
    "                  # plugins=[\"deepspeed_stage_2\"])\n",
    "                  plugins=[pl.plugins.DDPPlugin(find_unused_parameters=False)])\n",
    "\n",
    "# configure learning rate\n",
    "bs, base_lr = args.batch_size, args.base_lr\n",
    "ngpu = args.gpus\n",
    "accumulate_grad_batches = args.accumulate_grad_batches or 1\n",
    "print(f\"accumulate_grad_batches = {accumulate_grad_batches}\")\n",
    "model.learning_rate = accumulate_grad_batches * ngpu * bs * base_lr\n",
    "print(\"Setting learning rate to {:.2e} = {} (accumulate_grad_batches) * {} (num_gpus) * {} (batchsize) * {:.2e} (base_lr)\".format(\n",
    "    model.learning_rate, accumulate_grad_batches, ngpu, bs, base_lr))\n",
    "\n",
    "# load the most recent checkpoint file\n",
    "base_dir = os.path.join(args.default_root_dir, 'lightning_logs')\n",
    "if os.path.exists(base_dir):\n",
    "    log_folder = ckpt_file = ''\n",
    "    version_id_used = step_used = 0\n",
    "    for folder in os.listdir(base_dir):\n",
    "        version_id = int(folder.split('_')[1])\n",
    "        if version_id > version_id_used:\n",
    "            version_id_used = version_id\n",
    "            log_folder = folder\n",
    "    if len(log_folder) > 0:\n",
    "        ckpt_folder = os.path.join(base_dir, log_folder, 'checkpoints')\n",
    "        for fn in os.listdir(ckpt_folder):\n",
    "            if fn == 'latest_checkpoint.ckpt':\n",
    "                ckpt_file = 'latest_checkpoint_prev.ckpt'\n",
    "                os.rename(os.path.join(ckpt_folder, fn), os.path.join(ckpt_folder, ckpt_file))\n",
    "        if len(ckpt_file) > 0:\n",
    "            args.resume_from_checkpoint = os.path.join(ckpt_folder, ckpt_file)\n",
    "            print('will start from the recent ckpt %s'%args.resume_from_checkpoint)\n",
    "\n",
    "trainer = pl.Trainer.from_argparse_args(args, callbacks=callbacks,\n",
    "                                        max_steps=args.max_steps, **kwargs)\n",
    "\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import cupy\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from datetime import datetime\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import softmax\n",
    "from torch.autograd import Function\n",
    "\n",
    "\n",
    "def getGaussian(T, H, W, beta, d):\n",
    "\n",
    "    diag = np.diag([beta[0], beta[1], beta[1]])\n",
    "    rv = multivariate_normal([T - 1, H - 1, W - 1], diag)\n",
    "    tensor = torch.tensor((), dtype=torch.float32)\n",
    "\n",
    "    NT = 2 * T - 1\n",
    "    NH = 2 * H - 1\n",
    "    NW = 2 * W - 1\n",
    "\n",
    "    weight = tensor.new_ones((NT, NW, NH), device=d)\n",
    "\n",
    "    for pos in np.arange(0, NT * NH * NW):\n",
    "        i = math.floor(pos / (NH * NW))\n",
    "        j = math.floor((pos - i * NH * NW) / NH)\n",
    "        k = pos - i * NH * NW - j * NW\n",
    "        weight[i, j, k] = rv.pdf([i, j, k])\n",
    "\n",
    "        weight = weight / torch.max(weight)\n",
    "\n",
    "    return weight\n",
    "\n",
    "class focusAttention(Function):\n",
    "\n",
    "    T, H, W = 4, 4, 4\n",
    "    T_flatten = T * H * W\n",
    "    center_T, center_H, center_W = T - 1, H - 1, W - 1\n",
    "    beta = [100, 100]\n",
    "    \n",
    "    diag = np.diag([beta[0], beta[1], beta[1]])\n",
    "    rv = multivariate_normal([T - 1, H - 1, W - 1], diag)\n",
    "    tensor = torch.tensor((), dtype=torch.float32)\n",
    "\n",
    "    NT = 2 * T - 1\n",
    "    NH = 2 * H - 1\n",
    "    NW = 2 * W - 1\n",
    "\n",
    "    weight_cuda0 = tensor.new_ones((NT, NW, NH), device=torch.device(\"cuda:0\"))\n",
    "\n",
    "    for pos in np.arange(0, NT * NH * NW):\n",
    "        i = math.floor(pos / (NH * NW))\n",
    "        j = math.floor((pos - i * NH * NW) / NH)\n",
    "        k = pos - i * NH * NW - j * NW\n",
    "        weight_cuda0[i, j, k] = rv.pdf([i, j, k])\n",
    "\n",
    "        weight_cuda0 = weight_cuda0 / torch.max(weight_cuda0)\n",
    "    \n",
    "    weight_cuda1 = weight_cuda0.detach().to(\"cuda:1\")\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, score, V):\n",
    "\n",
    "        att=[]\n",
    "        \n",
    "        if V.get_device() == 0:\n",
    "            weight = focusAttention.weight_cuda0\n",
    "        else:\n",
    "            weight = focusAttention.weight_cuda1\n",
    "        \n",
    "        st = torch.cuda.memory_allocated()\n",
    "        \n",
    "        st_loop = datetime.now()\n",
    "        \n",
    "        for pos in np.arange(0, focusAttention.T_flatten):\n",
    "            \n",
    "            st = datetime.now()\n",
    "            print(f\"start of loop {st}\")\n",
    "            \n",
    "            i = math.floor(pos / (focusAttention.H * focusAttention.W))\n",
    "            j = math.floor((pos - i * focusAttention.H * focusAttention.W) / focusAttention.H)\n",
    "            k = pos - i * focusAttention.H * focusAttention.W - j * focusAttention.W\n",
    "            \n",
    "            t1 = datetime.now()\n",
    "            \n",
    "            print(f\"pos calculate {t1 - st}\")\n",
    "            \n",
    "\n",
    "            weight_xyz = weight[focusAttention.center_T - i:2 * focusAttention.center_T - i + 1, focusAttention.center_W - j:2 * focusAttention.center_W - j + 1,\n",
    "                         focusAttention.center_H - k:2 * focusAttention.center_H - k + 1].reshape(-1)\n",
    "            \n",
    "            t2 = datetime.now()\n",
    "            print(f\"weight idx {t2 - t1}\")\n",
    "            \n",
    "            weight_xyz = weight_xyz[None, None, :, None]\n",
    "            t3 = datetime.now()\n",
    "            \n",
    "            print(f\"weight boardcast {t3 - t2}\")\n",
    "            \n",
    "            # qk shape (B, NH, 1, T)\n",
    "            qk = score[:, :, pos, :]\n",
    "            qk = qk[:, :, None, :]\n",
    "            \n",
    "            t4 = datetime.now()\n",
    "            \n",
    "            print(f\"qk index and boardcast {t4 - t3}\")\n",
    "\n",
    "            att_pos = torch.matmul(qk, (V * weight_xyz)).detach()\n",
    "            \n",
    "            t5 = datetime.now()\n",
    "            \n",
    "            print(f\"att cal {t5 - t4}\")\n",
    "\n",
    "            att.append(att_pos)\n",
    "            \n",
    "            t6 = datetime.now()\n",
    "            \n",
    "            print(f\"append {t6 -t5}\")\n",
    "            # V = torch.clone(V_ori)\n",
    "\n",
    "        \n",
    "        end = datetime.now()\n",
    "        \n",
    "        print(f\"complete loop {end - st_loop}\")\n",
    "        \n",
    "        # print(f\"Before cat {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "        result = torch.cat(att, dim=2)\n",
    "        \n",
    "        # print(f\"result shape {result.shape}\")\n",
    "\n",
    "        end = torch.cuda.memory_allocated()\n",
    "\n",
    "        # print(f\"result memory usage is {result.element_size() * result.nelement()}, memory used {end - st}, memory for v is {V.element_size() * V.nelement()}\")\n",
    "\n",
    "        # print(f\"After focused attention, memory usage is {end}, memory used {end - st}\")\n",
    "\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "        # print(f\"After empty cache, memory usage is {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "        # ctx.save_for_backward(score, V, result)\n",
    "\n",
    "        # print(f\"After save for backwards, memory usage is {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        score, V, result = ctx.saved_tensors\n",
    "        \n",
    "        if V.get_device() == 0:\n",
    "            weight = focusAttention.weight_cuda0\n",
    "        else:\n",
    "            weight = focusAttention.weight_cuda1\n",
    "\n",
    "        grad_score = []\n",
    "        grad_V = []\n",
    "\n",
    "        for pos in np.arange(0, focusAttention.T_flatten):\n",
    "            grad_att_pos = grad_output[:, :, pos, :]\n",
    "\n",
    "            grad_att_pos = grad_att_pos[:, :, None, :]\n",
    "\n",
    "            i = math.floor(pos / (focusAttention.H * focusAttention.W))\n",
    "            j = math.floor((pos - i * focusAttention.H * focusAttention.W) / focusAttention.H)\n",
    "            k = pos - i * focusAttention.H * focusAttention.W - j * focusAttention.W\n",
    "\n",
    "            weight_xyz = weight[focusAttention.center_T - i:2 * focusAttention.center_T - i + 1, focusAttention.center_W - j:2 * focusAttention.center_W - j + 1,\n",
    "                         focusAttention.center_H - k:2 * focusAttention.center_H - k + 1].reshape(-1)\n",
    "\n",
    "            qk = score[:, :, pos, :]\n",
    "            \n",
    "            qk = qk[:, :, None, :]\n",
    "            \n",
    "            print(f\"shape qk {qk.shape}\")\n",
    "            \n",
    "            qk = torch.swapaxes(qk, 2, 3)\n",
    "            \n",
    "            weight_xyz = weight_xyz[None, None, :, None]\n",
    "            \n",
    "            print(f\"shape qk {qk.shape}, weight_xyz {weight_xyz.shape}\")\n",
    "            \n",
    "            print(f\"shape qk*weight {(qk * weight_xyz).shape}, grad_att_pos {grad_att_pos.shape}\")\n",
    "            \n",
    "            grad_V_pos = torch.matmul((qk * weight_xyz), grad_att_pos)[:, :, pos, :]\n",
    "            \n",
    "            grad_V.append(grad_V_pos[:, :, None, :])\n",
    "            \n",
    "            print(f\"V_pos shape {grad_V_pos.shape}\")\n",
    "            \n",
    "            grad_score_pos = (torch.matmul(weight_xyz, grad_att_pos) * V)[:, :, :, 0]\n",
    "            \n",
    "            print(f\"grad_score_pos shape {grad_score_pos.shape}\")\n",
    "            \n",
    "            grad_score.append(grad_score_pos[:, :, None, :])\n",
    "\n",
    "            # grad_qk = grad_att_pos @ torch.linalg.inv(V_focus)\n",
    "            # grad_V_focus = torch.linalg.inv(qk) @ grad_att_pos\n",
    "            #\n",
    "            # grad_score.append(grad_qk)\n",
    "            # grad_V_focus = grad_V_focus * weight_xyz\n",
    "            # grad_V.append(grad_V_focus)\n",
    "\n",
    "        # Shape should be B, NH, T, T\n",
    "        grad_score = torch.cat(grad_score, dim=2)\n",
    "\n",
    "        # Shape should be B, NH, T, HS\n",
    "        grad_V = torch.cat(grad_V, dim=2)\n",
    "        \n",
    "        # print(f\"grad_score {grad_score.shape}, grad_V {grad_V.shape}\")\n",
    "\n",
    "        return grad_score, grad_V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import cupy\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from datetime import datetime\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import softmax\n",
    "from torch.autograd import Function\n",
    "\n",
    "\n",
    "def getGaussian(T, H, W, beta, d):\n",
    "\n",
    "    diag = np.diag([beta[0], beta[1], beta[1]])\n",
    "    rv = multivariate_normal([T - 1, H - 1, W - 1], diag)\n",
    "    tensor = torch.tensor((), dtype=torch.float32)\n",
    "\n",
    "    NT = 2 * T - 1\n",
    "    NH = 2 * H - 1\n",
    "    NW = 2 * W - 1\n",
    "\n",
    "    weight = tensor.new_ones((NT, NW, NH), device=d)\n",
    "\n",
    "    for pos in np.arange(0, NT * NH * NW):\n",
    "        i = math.floor(pos / (NH * NW))\n",
    "        j = math.floor((pos - i * NH * NW) / NH)\n",
    "        k = pos - i * NH * NW - j * NW\n",
    "        weight[i, j, k] = rv.pdf([i, j, k])\n",
    "\n",
    "        weight = weight / torch.max(weight)\n",
    "\n",
    "    return weight\n",
    "\n",
    "class focusAttention(Function):\n",
    "\n",
    "    T, H, W = 4, 4, 4\n",
    "    T_flatten = T * H * W\n",
    "    center_T, center_H, center_W = T - 1, H - 1, W - 1\n",
    "    beta = [100, 100]\n",
    "    \n",
    "    diag = np.diag([beta[0], beta[1], beta[1]])\n",
    "    rv = multivariate_normal([T - 1, H - 1, W - 1], diag)\n",
    "    tensor = torch.tensor((), dtype=torch.float32)\n",
    "\n",
    "    NT = 2 * T - 1\n",
    "    NH = 2 * H - 1\n",
    "    NW = 2 * W - 1\n",
    "\n",
    "    weight_cuda0 = tensor.new_ones((NT, NW, NH), device=torch.device(\"cuda:0\"))\n",
    "\n",
    "    for pos in np.arange(0, NT * NH * NW):\n",
    "        i = math.floor(pos / (NH * NW))\n",
    "        j = math.floor((pos - i * NH * NW) / NH)\n",
    "        k = pos - i * NH * NW - j * NW\n",
    "        weight_cuda0[i, j, k] = rv.pdf([i, j, k])\n",
    "\n",
    "        weight_cuda0 = weight_cuda0 / torch.max(weight_cuda0)\n",
    "\n",
    "    # Shape T, 1, 1, T, 1\n",
    "    V_weight_cuda0 = torch.empty((focusAttention.T_flatten,1,1,focusAttention.T_flatten,1), dtype=torch.float32, device =\"cuda:0\")\n",
    "\n",
    "    for pos in np.arange(0, focusAttention.T_flatten):\n",
    "\n",
    "        i = math.floor(pos / (focusAttention.H * focusAttention.W))\n",
    "        j = math.floor((pos - i * focusAttention.H * focusAttention.W) / focusAttention.H)\n",
    "        k = pos - i * focusAttention.H * focusAttention.W - j * focusAttention.W\n",
    "\n",
    "        weight_xyz = weight_cuda0[focusAttention.center_T - i:2 * focusAttention.center_T - i + 1, focusAttention.center_W - j:2 * focusAttention.center_W - j + 1,\n",
    "                     focusAttention.center_H - k:2 * focusAttention.center_H - k + 1].reshape(-1)\n",
    "\n",
    "        V_weight_cuda0[pos, 0, 0, :, 0] = weight_xyz\n",
    "\n",
    "    V_weight_cuda1 = V_weight_cuda0.detach().to(\"cuda:0\")\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, score, V):\n",
    "\n",
    "        if V.get_device() == 0:\n",
    "            V_weight = focusAttention.V_weight_cuda0\n",
    "        else:\n",
    "            V_weight = focusAttention.V_weight_cuda1\n",
    "\n",
    "        st_loop = datetime.now()\n",
    "\n",
    "        # V shape is B, NH, T, HS\n",
    "        V = V_weight * V\n",
    "        \n",
    "        # V should be T, B, NH, T, HS\n",
    "        \n",
    "        qk = score\n",
    "        \n",
    "        # qk should be T, B, NH, 1 , T \n",
    "        qk = torch.swapaxes(qk, 2, 0)[:, :, :, None, :]\n",
    "\n",
    "        # result should be T, B, NH, 1, HS\n",
    "        result = torch.matmul(qk, V)\n",
    "        result = torch.swapaxes(result, 0, 2)[:, :, :, 0, :]\n",
    "        \n",
    "        print(\"result\", result.shape)\n",
    "        \n",
    "        end = datetime.now()\n",
    "        print(f\"complete loop {end - st_loop}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        score, V, result = ctx.saved_tensors\n",
    "\n",
    "        # V weight shape is [1, 1, T, T]\n",
    "        d = V.get_device()\n",
    "\n",
    "        if d == 0:\n",
    "            V_weight = focusAttention.V_weight_cuda0\n",
    "        else:\n",
    "            V_weight = focusAttention.V_weight_cuda1\n",
    "\n",
    "        # ï¼ˆ1, B, NH, T, T)\n",
    "        score = score[None, :, :, :, :]\n",
    "\n",
    "        # (T, B, NH, T, 1)\n",
    "        score = score.permute(3, 1, 2, 4, 0)\n",
    "\n",
    "        # (1, B, NH, T, HS)\n",
    "        grad_output = grad_output[None, :, :, :, :]\n",
    "\n",
    "        # (T, B, NH, 1, HS)\n",
    "        grad_output = torch.swapaxes(grad_output, 0, 3)\n",
    "\n",
    "        grad_V = torch.empty(V.shape, dtype = torch.float32, device = f\"cuda:{d}\")\n",
    "\n",
    "        # Shape is (T, B, NH, T, HS)\n",
    "        grad_V_total = torch.matmul((V_weight * score), grad_output)\n",
    "\n",
    "        for i in np.arange(focusAttention.T_flatten):\n",
    "            grad_V[:, :, i, :] = grad_V_total[i, :, :, i, :]\n",
    "\n",
    "        del grad_V_total\n",
    "\n",
    "        # (T, B, NH, T, 1) = (B, NH, T, 1) * (T, B, NH, 1, 1) * (T, 1, 1, T, 1)\n",
    "\n",
    "        grad_score = V[:, :, :, 0][:, :, :, None] * grad_output[:, :, :, :, 0][:, :, :, :, None] * V_weight\n",
    "\n",
    "        # (T, B, NH, T)\n",
    "        grad_score = grad_score[:, :, :, :, 0]\n",
    "\n",
    "        # (B, NH, T, T)\n",
    "        grad_score = grad_score.permute(1, 2, 0, 3)\n",
    "\n",
    "        # print(f\"grad_score {grad_score.shape}, grad_V {grad_V.shape}\")\n",
    "\n",
    "        return grad_score, grad_V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of loop 2023-03-25 19:18:46.843157\n",
      "pos calculate 0:00:00.000150\n",
      "weight idx 0:00:00.000256\n",
      "weight boardcast 0:00:00.000046\n",
      "qk index and boardcast 0:00:00.000118\n",
      "att cal 0:00:00.000970\n",
      "append 0:00:00.000038\n",
      "start of loop 2023-03-25 19:18:46.844747\n",
      "pos calculate 0:00:00.000021\n",
      "weight idx 0:00:00.000133\n",
      "weight boardcast 0:00:00.000029\n",
      "qk index and boardcast 0:00:00.000078\n",
      "att cal 0:00:00.000224\n",
      "append 0:00:00.000023\n",
      "start of loop 2023-03-25 19:18:46.845265\n",
      "pos calculate 0:00:00.000017\n",
      "weight idx 0:00:00.000147\n",
      "weight boardcast 0:00:00.000035\n",
      "qk index and boardcast 0:00:00.000058\n",
      "att cal 0:00:00.000228\n",
      "append 0:00:00.000024\n",
      "start of loop 2023-03-25 19:18:46.845784\n",
      "pos calculate 0:00:00.000017\n",
      "weight idx 0:00:00.000167\n",
      "weight boardcast 0:00:00.000037\n",
      "qk index and boardcast 0:00:00.000056\n",
      "att cal 0:00:00.000133\n",
      "append 0:00:00.000016\n",
      "start of loop 2023-03-25 19:18:46.846219\n",
      "pos calculate 0:00:00.000017\n",
      "weight idx 0:00:00.000067\n",
      "weight boardcast 0:00:00.000024\n",
      "qk index and boardcast 0:00:00.000050\n",
      "att cal 0:00:00.000295\n",
      "append 0:00:00.000026\n",
      "start of loop 2023-03-25 19:18:46.846707\n",
      "pos calculate 0:00:00.000018\n",
      "weight idx 0:00:00.000074\n",
      "weight boardcast 0:00:00.000085\n",
      "qk index and boardcast 0:00:00.000061\n",
      "att cal 0:00:00.000255\n",
      "append 0:00:00.000023\n",
      "start of loop 2023-03-25 19:18:46.847233\n",
      "pos calculate 0:00:00.000017\n",
      "weight idx 0:00:00.000091\n",
      "weight boardcast 0:00:00.000027\n",
      "qk index and boardcast 0:00:00.000053\n",
      "att cal 0:00:00.000448\n",
      "append 0:00:00.000028\n",
      "start of loop 2023-03-25 19:18:46.847906\n",
      "pos calculate 0:00:00.000018\n",
      "weight idx 0:00:00.000176\n",
      "weight boardcast 0:00:00.000030\n",
      "qk index and boardcast 0:00:00.000055\n",
      "att cal 0:00:00.000131\n",
      "append 0:00:00.000016\n",
      "start of loop 2023-03-25 19:18:46.848341\n",
      "pos calculate 0:00:00.000016\n",
      "weight idx 0:00:00.000209\n",
      "weight boardcast 0:00:00.000034\n",
      "qk index and boardcast 0:00:00.000057\n",
      "att cal 0:00:00.000121\n",
      "append 0:00:00.000015\n",
      "start of loop 2023-03-25 19:18:46.848802\n",
      "pos calculate 0:00:00.000052\n",
      "weight idx 0:00:00.000090\n",
      "weight boardcast 0:00:00.000090\n",
      "qk index and boardcast 0:00:00.000061\n",
      "att cal 0:00:00.000129\n",
      "append 0:00:00.000082\n",
      "start of loop 2023-03-25 19:18:46.849315\n",
      "pos calculate 0:00:00.000017\n",
      "weight idx 0:00:00.000085\n",
      "weight boardcast 0:00:00.000026\n",
      "qk index and boardcast 0:00:00.000052\n",
      "att cal 0:00:00.000208\n",
      "append 0:00:00.000023\n",
      "start of loop 2023-03-25 19:18:46.849736\n",
      "pos calculate 0:00:00.000017\n",
      "weight idx 0:00:00.000076\n",
      "weight boardcast 0:00:00.000025\n",
      "qk index and boardcast 0:00:00.000112\n",
      "att cal 0:00:00.000132\n",
      "append 0:00:00.000074\n",
      "start of loop 2023-03-25 19:18:46.850181\n",
      "pos calculate 0:00:00.000017\n",
      "weight idx 0:00:00.000085\n",
      "weight boardcast 0:00:00.000031\n",
      "qk index and boardcast 0:00:00.000221\n",
      "att cal 0:00:00.000147\n",
      "append 0:00:00.000016\n",
      "start of loop 2023-03-25 19:18:46.850708\n",
      "pos calculate 0:00:00.000017\n",
      "weight idx 0:00:00.000068\n",
      "weight boardcast 0:00:00.000025\n",
      "qk index and boardcast 0:00:00.000050\n",
      "att cal 0:00:00.000138\n",
      "append 0:00:00.000016\n",
      "start of loop 2023-03-25 19:18:46.851031\n",
      "pos calculate 0:00:00.000016\n",
      "weight idx 0:00:00.000066\n",
      "weight boardcast 0:00:00.000025\n",
      "qk index and boardcast 0:00:00.000050\n",
      "att cal 0:00:00.000104\n",
      "append 0:00:00.000014\n",
      "start of loop 2023-03-25 19:18:46.851315\n",
      "pos calculate 0:00:00.000015\n",
      "weight idx 0:00:00.000059\n",
      "weight boardcast 0:00:00.000024\n",
      "qk index and boardcast 0:00:00.000048\n",
      "att cal 0:00:00.000099\n",
      "append 0:00:00.000013\n",
      "start of loop 2023-03-25 19:18:46.851582\n",
      "pos calculate 0:00:00.000015\n",
      "weight idx 0:00:00.000058\n",
      "weight boardcast 0:00:00.000023\n",
      "qk index and boardcast 0:00:00.000048\n",
      "att cal 0:00:00.000097\n",
      "append 0:00:00.000014\n",
      "start of loop 2023-03-25 19:18:46.851845\n",
      "pos calculate 0:00:00.000015\n",
      "weight idx 0:00:00.000058\n",
      "weight boardcast 0:00:00.000023\n",
      "qk index and boardcast 0:00:00.000047\n",
      "att cal 0:00:00.000097\n",
      "append 0:00:00.000014\n",
      "start of loop 2023-03-25 19:18:46.852107\n",
      "pos calculate 0:00:00.000014\n",
      "weight idx 0:00:00.000058\n",
      "weight boardcast 0:00:00.000024\n",
      "qk index and boardcast 0:00:00.000047\n",
      "att cal 0:00:00.000096\n",
      "append 0:00:00.000013\n",
      "start of loop 2023-03-25 19:18:46.852368\n",
      "pos calculate 0:00:00.000014\n",
      "weight idx 0:00:00.000071\n",
      "weight boardcast 0:00:00.000024\n",
      "qk index and boardcast 0:00:00.000048\n",
      "att cal 0:00:00.000100\n",
      "append 0:00:00.000013\n",
      "start of loop 2023-03-25 19:18:46.852646\n",
      "pos calculate 0:00:00.000015\n",
      "weight idx 0:00:00.000056\n",
      "weight boardcast 0:00:00.000023\n",
      "qk index and boardcast 0:00:00.000047\n",
      "att cal 0:00:00.000134\n",
      "append 0:00:00.000014\n",
      "start of loop 2023-03-25 19:18:46.852941\n",
      "pos calculate 0:00:00.000012\n",
      "weight idx 0:00:00.000051\n",
      "weight boardcast 0:00:00.000019\n",
      "qk index and boardcast 0:00:00.000038\n",
      "att cal 0:00:00.000088\n",
      "append 0:00:00.000012\n",
      "start of loop 2023-03-25 19:18:46.853167\n",
      "pos calculate 0:00:00.000013\n",
      "weight idx 0:00:00.000048\n",
      "weight boardcast 0:00:00.000019\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000080\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.853382\n",
      "pos calculate 0:00:00.000011\n",
      "weight idx 0:00:00.000047\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000036\n",
      "att cal 0:00:00.000080\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.853591\n",
      "pos calculate 0:00:00.000012\n",
      "weight idx 0:00:00.000046\n",
      "weight boardcast 0:00:00.000017\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000109\n",
      "append 0:00:00.000014\n",
      "start of loop 2023-03-25 19:18:46.853834\n",
      "pos calculate 0:00:00.000014\n",
      "weight idx 0:00:00.000056\n",
      "weight boardcast 0:00:00.000024\n",
      "qk index and boardcast 0:00:00.000046\n",
      "att cal 0:00:00.000096\n",
      "append 0:00:00.000015\n",
      "start of loop 2023-03-25 19:18:46.854093\n",
      "pos calculate 0:00:00.000014\n",
      "weight idx 0:00:00.000058\n",
      "weight boardcast 0:00:00.000023\n",
      "qk index and boardcast 0:00:00.000046\n",
      "att cal 0:00:00.000094\n",
      "append 0:00:00.000014\n",
      "start of loop 2023-03-25 19:18:46.854350\n",
      "pos calculate 0:00:00.000014\n",
      "weight idx 0:00:00.000057\n",
      "weight boardcast 0:00:00.000023\n",
      "qk index and boardcast 0:00:00.000046\n",
      "att cal 0:00:00.000095\n",
      "append 0:00:00.000013\n",
      "start of loop 2023-03-25 19:18:46.854606\n",
      "pos calculate 0:00:00.000014\n",
      "weight idx 0:00:00.000056\n",
      "weight boardcast 0:00:00.000023\n",
      "qk index and boardcast 0:00:00.000047\n",
      "att cal 0:00:00.000095\n",
      "append 0:00:00.000013\n",
      "start of loop 2023-03-25 19:18:46.854866\n",
      "pos calculate 0:00:00.000014\n",
      "weight idx 0:00:00.000061\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000122\n",
      "append 0:00:00.000014\n",
      "start of loop 2023-03-25 19:18:46.855139\n",
      "pos calculate 0:00:00.000013\n",
      "weight idx 0:00:00.000054\n",
      "weight boardcast 0:00:00.000020\n",
      "qk index and boardcast 0:00:00.000038\n",
      "att cal 0:00:00.000082\n",
      "append 0:00:00.000012\n",
      "start of loop 2023-03-25 19:18:46.855364\n",
      "pos calculate 0:00:00.000011\n",
      "weight idx 0:00:00.000047\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000036\n",
      "att cal 0:00:00.000079\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.855572\n",
      "pos calculate 0:00:00.000012\n",
      "weight idx 0:00:00.000045\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000077\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.855778\n",
      "pos calculate 0:00:00.000012\n",
      "weight idx 0:00:00.000047\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000078\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.855987\n",
      "pos calculate 0:00:00.000012\n",
      "weight idx 0:00:00.000051\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000038\n",
      "att cal 0:00:00.000080\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.856203\n",
      "pos calculate 0:00:00.000012\n",
      "weight idx 0:00:00.000047\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000036\n",
      "att cal 0:00:00.000078\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.856412\n",
      "pos calculate 0:00:00.000012\n",
      "weight idx 0:00:00.000045\n",
      "weight boardcast 0:00:00.000017\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000078\n",
      "append 0:00:00.000010\n",
      "start of loop 2023-03-25 19:18:46.856617\n",
      "pos calculate 0:00:00.000012\n",
      "weight idx 0:00:00.000046\n",
      "weight boardcast 0:00:00.000017\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000107\n",
      "append 0:00:00.000014\n",
      "start of loop 2023-03-25 19:18:46.856857\n",
      "pos calculate 0:00:00.000012\n",
      "weight idx 0:00:00.000053\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000040\n",
      "att cal 0:00:00.000083\n",
      "append 0:00:00.000012\n",
      "start of loop 2023-03-25 19:18:46.857081\n",
      "pos calculate 0:00:00.000011\n",
      "weight idx 0:00:00.000047\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000080\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.857291\n",
      "pos calculate 0:00:00.000011\n",
      "weight idx 0:00:00.000046\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000078\n",
      "append 0:00:00.000010\n",
      "start of loop 2023-03-25 19:18:46.857498\n",
      "pos calculate 0:00:00.000011\n",
      "weight idx 0:00:00.000045\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000078\n",
      "append 0:00:00.000010\n",
      "start of loop 2023-03-25 19:18:46.857704\n",
      "pos calculate 0:00:00.000011\n",
      "weight idx 0:00:00.000046\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000036\n",
      "att cal 0:00:00.000080\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.857912\n",
      "pos calculate 0:00:00.000012\n",
      "weight idx 0:00:00.000046\n",
      "weight boardcast 0:00:00.000019\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000077\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.858120\n",
      "pos calculate 0:00:00.000011\n",
      "weight idx 0:00:00.000045\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000077\n",
      "append 0:00:00.000010\n",
      "start of loop 2023-03-25 19:18:46.858324\n",
      "pos calculate 0:00:00.000012\n",
      "weight idx 0:00:00.000044\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000082\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.858534\n",
      "pos calculate 0:00:00.000012\n",
      "weight idx 0:00:00.000047\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000078\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.858744\n",
      "pos calculate 0:00:00.000011\n",
      "weight idx 0:00:00.000046\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000077\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.858950\n",
      "pos calculate 0:00:00.000012\n",
      "weight idx 0:00:00.000073\n",
      "weight boardcast 0:00:00.000019\n",
      "qk index and boardcast 0:00:00.000044\n",
      "att cal 0:00:00.000084\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.859200\n",
      "pos calculate 0:00:00.000012\n",
      "weight idx 0:00:00.000046\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000036\n",
      "att cal 0:00:00.000078\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.859408\n",
      "pos calculate 0:00:00.000011\n",
      "weight idx 0:00:00.000045\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000036\n",
      "att cal 0:00:00.000077\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.859612\n",
      "pos calculate 0:00:00.000011\n",
      "weight idx 0:00:00.000046\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000077\n",
      "append 0:00:00.000010\n",
      "start of loop 2023-03-25 19:18:46.859818\n",
      "pos calculate 0:00:00.000011\n",
      "weight idx 0:00:00.000044\n",
      "weight boardcast 0:00:00.000017\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000077\n",
      "append 0:00:00.000010\n",
      "start of loop 2023-03-25 19:18:46.860020\n",
      "pos calculate 0:00:00.000011\n",
      "weight idx 0:00:00.000045\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000036\n",
      "att cal 0:00:00.000082\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.860229\n",
      "pos calculate 0:00:00.000011\n",
      "weight idx 0:00:00.000048\n",
      "weight boardcast 0:00:00.000017\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000079\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.860438\n",
      "pos calculate 0:00:00.000012\n",
      "weight idx 0:00:00.000047\n",
      "weight boardcast 0:00:00.000017\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000079\n",
      "append 0:00:00.000010\n",
      "start of loop 2023-03-25 19:18:46.860647\n",
      "pos calculate 0:00:00.000011\n",
      "weight idx 0:00:00.000046\n",
      "weight boardcast 0:00:00.000017\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000108\n",
      "append 0:00:00.000015\n",
      "start of loop 2023-03-25 19:18:46.860887\n",
      "pos calculate 0:00:00.000013\n",
      "weight idx 0:00:00.000053\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000038\n",
      "att cal 0:00:00.000085\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.861111\n",
      "pos calculate 0:00:00.000012\n",
      "weight idx 0:00:00.000047\n",
      "weight boardcast 0:00:00.000017\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000079\n",
      "append 0:00:00.000012\n",
      "start of loop 2023-03-25 19:18:46.861321\n",
      "pos calculate 0:00:00.000012\n",
      "weight idx 0:00:00.000046\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000036\n",
      "att cal 0:00:00.000078\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.861529\n",
      "pos calculate 0:00:00.000011\n",
      "weight idx 0:00:00.000046\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000077\n",
      "append 0:00:00.000010\n",
      "start of loop 2023-03-25 19:18:46.861735\n",
      "pos calculate 0:00:00.000011\n",
      "weight idx 0:00:00.000045\n",
      "weight boardcast 0:00:00.000017\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000082\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.861945\n",
      "pos calculate 0:00:00.000011\n",
      "weight idx 0:00:00.000049\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000038\n",
      "att cal 0:00:00.000078\n",
      "append 0:00:00.000011\n",
      "start of loop 2023-03-25 19:18:46.862156\n",
      "pos calculate 0:00:00.000011\n",
      "weight idx 0:00:00.000047\n",
      "weight boardcast 0:00:00.000018\n",
      "qk index and boardcast 0:00:00.000037\n",
      "att cal 0:00:00.000077\n",
      "append 0:00:00.000011\n",
      "complete loop 0:00:00.019258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[  4.8046,   6.3077,   3.1247,  ...,  -8.5537,   8.9929, -13.6378],\n",
       "          [ -8.0399,   1.2727,   2.7651,  ...,   1.8120,  -6.1798,  13.0019],\n",
       "          [  0.1015,  -5.2724,  -4.3791,  ...,  -4.9989,  -6.8068,   8.7842],\n",
       "          ...,\n",
       "          [  1.4363,   5.9516,  -2.0048,  ...,   7.5183,  -8.0589,   4.0084],\n",
       "          [  2.6188,   3.1805,   8.7514,  ...,   7.8455,   8.2658,  -6.7710],\n",
       "          [  1.6404,  -3.5059,  -4.0218,  ...,  -2.4481,  -3.4259,   1.2706]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ctx = {}\n",
    "\n",
    "score = torch.randn(1, 1, 64, 64, dtype=torch.float32, requires_grad=True, device='cuda:0')\n",
    "\n",
    "V = torch.randn(1, 1, 64, 16, dtype=torch.float32, requires_grad=True, device='cuda:0')\n",
    "\n",
    "result = torch.randn(1, 1, 64, 16, dtype=torch.float32, requires_grad=True, device='cuda:0')\n",
    "\n",
    "grad_output = torch.randn(1, 1, 64, 16, dtype=torch.float32, requires_grad=True, device='cuda:0')\n",
    "\n",
    "class ctx_class:\n",
    "    def __init__(self):\n",
    "        self.saved_tensors = [score, V, result]\n",
    "\n",
    "\n",
    "ctx = ctx_class()\n",
    "\n",
    "focusAttention.forward(ctx, score, V)\n",
    "\n",
    "# focusAttention.backward(ctx, grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before V clone 17047552\n",
      "After V clone 17049600\n",
      "before loop 17051136\n",
      "start iter 17051136\n",
      "end iter 17053696\n",
      "start iter 17053696\n",
      "end iter 17054208\n",
      "start iter 17054208\n",
      "end iter 17054720\n",
      "start iter 17054720\n",
      "end iter 17055232\n",
      "start iter 17055232\n",
      "end iter 17055744\n",
      "start iter 17055744\n",
      "end iter 17056256\n",
      "start iter 17056256\n",
      "end iter 17056768\n",
      "start iter 17056768\n",
      "end iter 17057280\n",
      "start iter 17057280\n",
      "end iter 17057792\n",
      "start iter 17057792\n",
      "end iter 17058304\n",
      "start iter 17058304\n",
      "end iter 17058816\n",
      "start iter 17058816\n",
      "end iter 17059328\n",
      "start iter 17059328\n",
      "end iter 17059840\n",
      "start iter 17059840\n",
      "end iter 17060352\n",
      "start iter 17060352\n",
      "end iter 17060864\n",
      "start iter 17060864\n",
      "end iter 17061376\n",
      "start iter 17061376\n",
      "end iter 17061888\n",
      "start iter 17061888\n",
      "end iter 17062400\n",
      "start iter 17062400\n",
      "end iter 17062912\n",
      "start iter 17062912\n",
      "end iter 17063424\n",
      "start iter 17063424\n",
      "end iter 17063936\n",
      "start iter 17063936\n",
      "end iter 17064448\n",
      "start iter 17064448\n",
      "end iter 17064960\n",
      "start iter 17064960\n",
      "end iter 17065472\n",
      "start iter 17065472\n",
      "end iter 17065984\n",
      "start iter 17065984\n",
      "end iter 17066496\n",
      "start iter 17066496\n",
      "end iter 17067008\n",
      "start iter 17067008\n",
      "end iter 17067520\n",
      "start iter 17067520\n",
      "end iter 17068032\n",
      "start iter 17068032\n",
      "end iter 17068544\n",
      "start iter 17068544\n",
      "end iter 17069056\n",
      "start iter 17069056\n",
      "end iter 17069568\n",
      "start iter 17069568\n",
      "end iter 17070080\n",
      "start iter 17070080\n",
      "end iter 17070592\n",
      "start iter 17070592\n",
      "end iter 17071104\n",
      "start iter 17071104\n",
      "end iter 17071616\n",
      "start iter 17071616\n",
      "end iter 17072128\n",
      "start iter 17072128\n",
      "end iter 17072640\n",
      "start iter 17072640\n",
      "end iter 17073152\n",
      "start iter 17073152\n",
      "end iter 17073664\n",
      "start iter 17073664\n",
      "end iter 17074176\n",
      "start iter 17074176\n",
      "end iter 17074688\n",
      "start iter 17074688\n",
      "end iter 17075200\n",
      "start iter 17075200\n",
      "end iter 17075712\n",
      "start iter 17075712\n",
      "end iter 17076224\n",
      "start iter 17076224\n",
      "end iter 17076736\n",
      "start iter 17076736\n",
      "end iter 17077248\n",
      "start iter 17077248\n",
      "end iter 17077760\n",
      "start iter 17077760\n",
      "end iter 17078272\n",
      "start iter 17078272\n",
      "end iter 17078784\n",
      "start iter 17078784\n",
      "end iter 17079296\n",
      "start iter 17079296\n",
      "end iter 17079808\n",
      "start iter 17079808\n",
      "end iter 17080320\n",
      "start iter 17080320\n",
      "end iter 17080832\n",
      "start iter 17080832\n",
      "end iter 17081344\n",
      "start iter 17081344\n",
      "end iter 17081856\n",
      "start iter 17081856\n",
      "end iter 17082368\n",
      "start iter 17082368\n",
      "end iter 17082880\n",
      "start iter 17082880\n",
      "end iter 17083392\n",
      "start iter 17083392\n",
      "end iter 17083904\n",
      "start iter 17083904\n",
      "end iter 17084416\n",
      "start iter 17084416\n",
      "end iter 17084928\n",
      "start iter 17084928\n",
      "end iter 17085440\n",
      "start iter 17085440\n",
      "end iter 17085952\n",
      "end loop used 34816\n",
      "before V clone 17047552\n",
      "After V clone 17049600\n",
      "before loop 17051136\n",
      "start iter 17051136\n",
      "end iter 17053696\n",
      "start iter 17053696\n",
      "end iter 17054208\n",
      "start iter 17054208\n",
      "end iter 17054720\n",
      "start iter 17054720\n",
      "end iter 17055232\n",
      "start iter 17055232\n",
      "end iter 17055744\n",
      "start iter 17055744\n",
      "end iter 17056256\n",
      "start iter 17056256\n",
      "end iter 17056768\n",
      "start iter 17056768\n",
      "end iter 17057280\n",
      "start iter 17057280\n",
      "end iter 17057792\n",
      "start iter 17057792\n",
      "end iter 17058304\n",
      "start iter 17058304\n",
      "end iter 17058816\n",
      "start iter 17058816\n",
      "end iter 17059328\n",
      "start iter 17059328\n",
      "end iter 17059840\n",
      "start iter 17059840\n",
      "end iter 17060352\n",
      "start iter 17060352\n",
      "end iter 17060864\n",
      "start iter 17060864\n",
      "end iter 17061376\n",
      "start iter 17061376\n",
      "end iter 17061888\n",
      "start iter 17061888\n",
      "end iter 17062400\n",
      "start iter 17062400\n",
      "end iter 17062912\n",
      "start iter 17062912\n",
      "end iter 17063424\n",
      "start iter 17063424\n",
      "end iter 17063936\n",
      "start iter 17063936\n",
      "end iter 17064448\n",
      "start iter 17064448\n",
      "end iter 17064960\n",
      "start iter 17064960\n",
      "end iter 17065472\n",
      "start iter 17065472\n",
      "end iter 17065984\n",
      "start iter 17065984\n",
      "end iter 17066496\n",
      "start iter 17066496\n",
      "end iter 17067008\n",
      "start iter 17067008\n",
      "end iter 17067520\n",
      "start iter 17067520\n",
      "end iter 17068032\n",
      "start iter 17068032\n",
      "end iter 17068544\n",
      "start iter 17068544\n",
      "end iter 17069056\n",
      "start iter 17069056\n",
      "end iter 17069568\n",
      "start iter 17069568\n",
      "end iter 17070080\n",
      "start iter 17070080\n",
      "end iter 17070592\n",
      "start iter 17070592\n",
      "end iter 17071104\n",
      "start iter 17071104\n",
      "end iter 17071616\n",
      "start iter 17071616\n",
      "end iter 17072128\n",
      "start iter 17072128\n",
      "end iter 17072640\n",
      "start iter 17072640\n",
      "end iter 17073152\n",
      "start iter 17073152\n",
      "end iter 17073664\n",
      "start iter 17073664\n",
      "end iter 17074176\n",
      "start iter 17074176\n",
      "end iter 17074688\n",
      "start iter 17074688\n",
      "end iter 17075200\n",
      "start iter 17075200\n",
      "end iter 17075712\n",
      "start iter 17075712\n",
      "end iter 17076224\n",
      "start iter 17076224\n",
      "end iter 17076736\n",
      "start iter 17076736\n",
      "end iter 17077248\n",
      "start iter 17077248\n",
      "end iter 17077760\n",
      "start iter 17077760\n",
      "end iter 17078272\n",
      "start iter 17078272\n",
      "end iter 17078784\n",
      "start iter 17078784\n",
      "end iter 17079296\n",
      "start iter 17079296\n",
      "end iter 17079808\n",
      "start iter 17079808\n",
      "end iter 17080320\n",
      "start iter 17080320\n",
      "end iter 17080832\n",
      "start iter 17080832\n",
      "end iter 17081344\n",
      "start iter 17081344\n",
      "end iter 17081856\n",
      "start iter 17081856\n",
      "end iter 17082368\n",
      "start iter 17082368\n",
      "end iter 17082880\n",
      "start iter 17082880\n",
      "end iter 17083392\n",
      "start iter 17083392\n",
      "end iter 17083904\n",
      "start iter 17083904\n",
      "end iter 17084416\n",
      "start iter 17084416\n",
      "end iter 17084928\n",
      "start iter 17084928\n",
      "end iter 17085440\n",
      "start iter 17085440\n",
      "end iter 17085952\n",
      "end loop used 34816\n",
      "tensor([[[[17.4191, 16.3169],\n",
      "          [15.6101, 14.2356],\n",
      "          [16.0185, 13.7510],\n",
      "          [14.7444, 14.7128],\n",
      "          [13.3954, 13.2356],\n",
      "          [16.1997, 18.0319],\n",
      "          [16.9160, 16.2648],\n",
      "          [13.8602, 12.7434],\n",
      "          [15.0947, 15.5169],\n",
      "          [15.4313, 16.1146],\n",
      "          [16.2102, 15.5109],\n",
      "          [15.2971, 16.2284],\n",
      "          [16.4217, 16.0739],\n",
      "          [17.6391, 17.4518],\n",
      "          [16.3083, 15.0231],\n",
      "          [17.0068, 16.4986],\n",
      "          [14.9259, 16.1133],\n",
      "          [15.9568, 16.6757],\n",
      "          [15.0296, 15.6000],\n",
      "          [14.8210, 13.7928],\n",
      "          [17.7585, 17.7076],\n",
      "          [14.1601, 14.1066],\n",
      "          [14.7384, 15.3292],\n",
      "          [15.8839, 15.4306],\n",
      "          [16.6239, 15.9097],\n",
      "          [14.3041, 13.0153],\n",
      "          [15.7981, 15.0480],\n",
      "          [13.9335, 13.6556],\n",
      "          [16.3247, 14.6794],\n",
      "          [14.4621, 14.0951],\n",
      "          [14.9112, 15.9874],\n",
      "          [14.9763, 15.5496],\n",
      "          [15.2100, 14.8648],\n",
      "          [17.0470, 16.2941],\n",
      "          [16.1749, 15.4641],\n",
      "          [15.9328, 14.0403],\n",
      "          [15.7112, 16.3857],\n",
      "          [14.3359, 14.1463],\n",
      "          [15.9552, 15.8491],\n",
      "          [17.3538, 15.7612],\n",
      "          [16.6612, 16.8530],\n",
      "          [16.3937, 17.1890],\n",
      "          [16.2257, 15.8294],\n",
      "          [14.5466, 14.0046],\n",
      "          [15.8192, 15.0362],\n",
      "          [15.7531, 15.1658],\n",
      "          [16.4984, 16.0874],\n",
      "          [14.0336, 15.1942],\n",
      "          [16.1531, 15.2905],\n",
      "          [14.3818, 15.6641],\n",
      "          [16.4701, 15.2789],\n",
      "          [16.9449, 15.1901],\n",
      "          [14.8928, 15.6138],\n",
      "          [15.2616, 16.7531],\n",
      "          [16.0690, 15.2002],\n",
      "          [15.0701, 14.6543],\n",
      "          [15.4905, 14.3244],\n",
      "          [15.1456, 14.7341],\n",
      "          [15.7702, 16.2423],\n",
      "          [17.1478, 17.5375],\n",
      "          [16.7885, 17.3183],\n",
      "          [16.1946, 17.1408],\n",
      "          [16.4945, 15.8734],\n",
      "          [13.2419, 13.8956]],\n",
      "\n",
      "         [[15.7385, 13.1109],\n",
      "          [16.5641, 14.0753],\n",
      "          [14.9806, 14.4624],\n",
      "          [16.4818, 14.9865],\n",
      "          [15.1232, 14.2418],\n",
      "          [14.2834, 12.9092],\n",
      "          [17.3048, 16.2664],\n",
      "          [16.0245, 14.9887],\n",
      "          [15.0506, 12.4652],\n",
      "          [14.9396, 13.7689],\n",
      "          [17.5148, 14.3237],\n",
      "          [13.2835, 13.6052],\n",
      "          [15.1752, 13.9092],\n",
      "          [15.7679, 13.1233],\n",
      "          [16.0703, 14.9850],\n",
      "          [15.2473, 12.9591],\n",
      "          [17.2326, 14.9509],\n",
      "          [15.8083, 14.3894],\n",
      "          [18.2390, 14.6574],\n",
      "          [16.4639, 15.6969],\n",
      "          [15.6592, 15.3655],\n",
      "          [18.0863, 15.5805],\n",
      "          [14.1426, 13.3948],\n",
      "          [14.8479, 13.8322],\n",
      "          [15.5880, 12.5676],\n",
      "          [16.3768, 14.2382],\n",
      "          [16.3410, 14.3575],\n",
      "          [16.4480, 14.8350],\n",
      "          [14.4117, 12.9878],\n",
      "          [14.0140, 12.9982],\n",
      "          [17.3960, 14.9890],\n",
      "          [14.4466, 12.4249],\n",
      "          [16.6377, 13.9025],\n",
      "          [15.5737, 14.3422],\n",
      "          [14.7006, 13.6347],\n",
      "          [12.6270, 12.4664],\n",
      "          [14.1797, 12.0186],\n",
      "          [15.2088, 13.7891],\n",
      "          [14.6567, 14.3617],\n",
      "          [15.1310, 14.1375],\n",
      "          [17.0709, 14.6120],\n",
      "          [16.7250, 14.3536],\n",
      "          [14.2336, 12.5784],\n",
      "          [16.7428, 14.3501],\n",
      "          [13.9295, 13.5346],\n",
      "          [15.9039, 15.3692],\n",
      "          [15.7303, 14.8006],\n",
      "          [17.8150, 14.6586],\n",
      "          [13.3411, 12.5796],\n",
      "          [15.6479, 13.7979],\n",
      "          [14.2321, 12.5538],\n",
      "          [14.4844, 13.5390],\n",
      "          [14.3095, 12.1352],\n",
      "          [15.4100, 13.8844],\n",
      "          [14.9337, 13.6316],\n",
      "          [16.5540, 15.1235],\n",
      "          [15.6225, 13.4869],\n",
      "          [15.0925, 14.3960],\n",
      "          [14.2661, 13.0217],\n",
      "          [14.0711, 14.0308],\n",
      "          [17.4717, 15.2008],\n",
      "          [16.0612, 13.8311],\n",
      "          [14.5224, 13.4933],\n",
      "          [14.6778, 13.0196]]],\n",
      "\n",
      "\n",
      "        [[[18.1202, 16.2478],\n",
      "          [16.9453, 16.0468],\n",
      "          [15.6319, 16.0014],\n",
      "          [15.7520, 15.2579],\n",
      "          [16.7635, 17.2703],\n",
      "          [16.9449, 17.1818],\n",
      "          [16.7911, 15.3966],\n",
      "          [17.5839, 16.6283],\n",
      "          [15.5447, 14.3002],\n",
      "          [16.6311, 15.2006],\n",
      "          [16.3162, 14.5213],\n",
      "          [15.1700, 14.5355],\n",
      "          [15.8747, 15.5312],\n",
      "          [13.8182, 13.7078],\n",
      "          [15.9954, 15.0328],\n",
      "          [14.7567, 15.8301],\n",
      "          [15.9886, 14.5239],\n",
      "          [17.6541, 16.6654],\n",
      "          [16.6619, 15.7748],\n",
      "          [16.4455, 16.7796],\n",
      "          [17.3845, 16.1545],\n",
      "          [18.3250, 18.8106],\n",
      "          [15.3438, 14.9518],\n",
      "          [17.9842, 17.7865],\n",
      "          [13.7892, 13.5716],\n",
      "          [19.3856, 17.7472],\n",
      "          [16.9385, 16.6553],\n",
      "          [15.5130, 13.2654],\n",
      "          [15.3866, 15.9627],\n",
      "          [16.7489, 14.5173],\n",
      "          [17.3557, 17.0930],\n",
      "          [18.2129, 17.2692],\n",
      "          [16.4571, 17.3454],\n",
      "          [19.0063, 17.4249],\n",
      "          [17.3333, 16.2804],\n",
      "          [17.4329, 16.3503],\n",
      "          [16.6614, 14.8055],\n",
      "          [19.0551, 18.1808],\n",
      "          [16.1838, 15.4434],\n",
      "          [15.5209, 14.9813],\n",
      "          [18.4468, 16.9900],\n",
      "          [18.0500, 16.6771],\n",
      "          [15.7767, 15.0528],\n",
      "          [17.3788, 17.6334],\n",
      "          [17.6388, 16.7678],\n",
      "          [17.0314, 16.3864],\n",
      "          [20.3665, 18.4501],\n",
      "          [18.2706, 16.3829],\n",
      "          [16.5181, 17.5824],\n",
      "          [15.3188, 15.8512],\n",
      "          [17.6311, 16.2964],\n",
      "          [13.9420, 13.3786],\n",
      "          [19.1063, 17.2332],\n",
      "          [16.0011, 15.9379],\n",
      "          [14.9111, 14.3514],\n",
      "          [15.2077, 16.1613],\n",
      "          [19.0633, 17.9025],\n",
      "          [16.0221, 16.0091],\n",
      "          [16.9213, 15.5989],\n",
      "          [16.2633, 15.4337],\n",
      "          [16.4540, 15.8133],\n",
      "          [15.9982, 15.5383],\n",
      "          [16.1014, 15.3968],\n",
      "          [17.6171, 16.5278]],\n",
      "\n",
      "         [[16.4799, 15.0383],\n",
      "          [15.7761, 16.1566],\n",
      "          [16.4497, 16.4783],\n",
      "          [18.3208, 17.1108],\n",
      "          [15.5555, 15.8768],\n",
      "          [16.7331, 16.3390],\n",
      "          [15.8624, 14.1462],\n",
      "          [16.9892, 14.5039],\n",
      "          [16.5252, 16.0781],\n",
      "          [15.0871, 13.0328],\n",
      "          [16.7434, 16.4726],\n",
      "          [17.3608, 15.5928],\n",
      "          [13.8807, 13.9621],\n",
      "          [15.1331, 14.2629],\n",
      "          [15.2766, 15.1541],\n",
      "          [14.7497, 14.1273],\n",
      "          [15.8960, 14.7369],\n",
      "          [16.8812, 15.3017],\n",
      "          [15.6619, 15.8918],\n",
      "          [14.2227, 14.2194],\n",
      "          [15.5322, 14.2813],\n",
      "          [14.9333, 13.6002],\n",
      "          [15.2256, 14.8873],\n",
      "          [16.4074, 15.1693],\n",
      "          [15.3135, 13.2356],\n",
      "          [16.5990, 16.3201],\n",
      "          [17.1005, 16.9220],\n",
      "          [16.7203, 15.8228],\n",
      "          [12.5711, 12.6603],\n",
      "          [14.1457, 15.4709],\n",
      "          [17.7267, 16.9351],\n",
      "          [16.4437, 17.1370],\n",
      "          [15.4407, 15.0989],\n",
      "          [16.9946, 15.3702],\n",
      "          [15.9214, 15.0272],\n",
      "          [15.8851, 14.4223],\n",
      "          [16.6708, 15.5961],\n",
      "          [15.5278, 14.2156],\n",
      "          [14.2324, 15.4201],\n",
      "          [16.2425, 16.0438],\n",
      "          [13.8678, 12.6991],\n",
      "          [16.3849, 14.3619],\n",
      "          [15.4847, 14.3021],\n",
      "          [15.5103, 15.0225],\n",
      "          [15.1862, 14.9282],\n",
      "          [15.2113, 14.6868],\n",
      "          [15.6113, 14.2463],\n",
      "          [14.7048, 15.0373],\n",
      "          [17.6401, 16.0127],\n",
      "          [13.8876, 13.8942],\n",
      "          [18.1348, 17.1613],\n",
      "          [15.5502, 14.4693],\n",
      "          [15.3878, 13.8327],\n",
      "          [16.1134, 13.9376],\n",
      "          [14.3596, 12.9579],\n",
      "          [14.2653, 15.6380],\n",
      "          [15.3375, 13.3232],\n",
      "          [16.1991, 14.4440],\n",
      "          [17.1878, 16.5641],\n",
      "          [16.7109, 14.4953],\n",
      "          [14.9764, 13.8139],\n",
      "          [18.3987, 18.4328],\n",
      "          [16.2245, 15.2192],\n",
      "          [15.0133, 13.8013]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "cuda = torch.device('cuda', 0)\n",
    "B = 2\n",
    "NH = 2\n",
    "T_flatten = 64\n",
    "HS = 2\n",
    "Q = torch.rand(B, NH, T_flatten, HS, device = cuda)\n",
    "K = torch.rand(B, NH, T_flatten, HS, device = cuda)\n",
    "V = torch.rand(B, NH, T_flatten, HS, device = cuda)\n",
    "\n",
    "score = torch.rand(B, NH, T_flatten, T_flatten, device = cuda)\n",
    "\n",
    "A = FocusedAttention(score, V)\n",
    "B = FocusedAttention(score, V)\n",
    "\n",
    "\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import cupy\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.special import softmax\n",
    "from torch.autograd import Function\n",
    "\n",
    "\n",
    "def getGaussian(T, H, W, beta, d):\n",
    "    diag = np.diag([beta[0], beta[1], beta[1]])\n",
    "    rv = multivariate_normal([T - 1, H - 1, W - 1], diag)\n",
    "    tensor = torch.tensor((), dtype=torch.float32)\n",
    "\n",
    "    NT = 2 * T - 1\n",
    "    NH = 2 * H - 1\n",
    "    NW = 2 * W - 1\n",
    "\n",
    "    weight = tensor.new_ones((NT, NW, NH), device=d)\n",
    "\n",
    "    for pos in np.arange(0, NT * NH * NW):\n",
    "        i = math.floor(pos / (NH * NW))\n",
    "        j = math.floor((pos - i * NH * NW) / NH)\n",
    "        k = pos - i * NH * NW - j * NW\n",
    "        weight[i, j, k] = rv.pdf([i, j, k])\n",
    "\n",
    "        weight = weight / torch.max(weight)\n",
    "\n",
    "    return weight\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class focusAttention(Function):\n",
    "\n",
    "    T, H, W = 4, 4, 4\n",
    "    T_flatten = T * H * W\n",
    "    center_T, center_H, center_W = T - 1, H - 1, W - 1\n",
    "    beta = [100, 100]\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    weight = getGaussian(T, H, W, beta, device)\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, score, V):\n",
    "\n",
    "        att=[]\n",
    "\n",
    "        st = torch.cuda.memory_allocated()\n",
    "\n",
    "        for pos in np.arange(0, focusAttention.T_flatten):\n",
    "\n",
    "            # print(f\"start of loop {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "            i = math.floor(pos / (focusAttention.H * focusAttention.W))\n",
    "            j = math.floor((pos - i * focusAttention.H * focusAttention.W) / focusAttention.H)\n",
    "            k = pos - i * focusAttention.H * focusAttention.W - j * focusAttention.W\n",
    "\n",
    "            # print(f\"Before weight_xyz {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "            weight_xyz = focusAttention.weight[focusAttention.center_T - i:2 * focusAttention.center_T - i + 1, focusAttention.center_W - j:2 * focusAttention.center_W - j + 1,\n",
    "                         focusAttention.center_H - k:2 * focusAttention.center_H - k + 1].reshape(-1)\n",
    "\n",
    "            # print(f\"After sub indexing weight {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "            weight_xyz = weight_xyz[None, None, :, None]\n",
    "\n",
    "            # print(f\"After add axis {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "            # V_focused = V * weight_xyz\n",
    "\n",
    "            # print(f\"After multiply weight {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "            # qk shape (B, NH, 1, T)\n",
    "            qk = score[:, :, pos, :]\n",
    "\n",
    "            # print(f\"After index qk{torch.cuda.memory_allocated()}\")\n",
    "\n",
    "            qk = qk[:, :, None, :]\n",
    "\n",
    "            # print(f\"After add axis to qk {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "            att_pos = torch.matmul(qk, (V * weight_xyz)).detach()\n",
    "\n",
    "            att.append(att_pos)\n",
    "            # V = torch.clone(V_ori)\n",
    "\n",
    "\n",
    "        # print(f\"Before cat {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "        result = torch.cat(att, dim=2).detach()\n",
    "\n",
    "        end = torch.cuda.memory_allocated()\n",
    "\n",
    "        # print(f\"result memory usage is {result.element_size() * result.nelement()}, memory used {end - st}, memory for v is {V.element_size() * V.nelement()}\")\n",
    "\n",
    "        # print(f\"After focused attention, memory usage is {end}, memory used {end - st}\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # print(f\"After empty cache, memory usage is {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "        ctx.save_for_backward(score, V, result)\n",
    "\n",
    "        # print(f\"After save for backwards, memory usage is {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        score, V, result = ctx.saved_tensor\n",
    "\n",
    "        grad_score = []\n",
    "        grad_V = []\n",
    "\n",
    "        for pos in np.arange(0, focusAttention.T_flatten):\n",
    "            grad_att_pos = grad_output[:, :, pos, :]\n",
    "\n",
    "            grad_att_pos = grad_att_pos[:, :, None, :]\n",
    "\n",
    "            i = math.floor(pos / (focusAttention.H * focusAttention.W))\n",
    "            j = math.floor((pos - i * focusAttention.H * focusAttention.W) / focusAttention.H)\n",
    "            k = pos - i * focusAttention.H * focusAttention.W - j * focusAttention.W\n",
    "\n",
    "            weight_xyz = focusAttention.weight[focusAttention.center_T - i:2 * focusAttention.center_T - i + 1, focusAttention.center_W - j:2 * focusAttention.center_W - j + 1,\n",
    "                         focusAttention.center_H - k:2 * focusAttention.center_H - k + 1].reshape(-1)\n",
    "\n",
    "            qk = score[:, :, pos, :]\n",
    "\n",
    "            qk = torch.swapaxes(qk, 2, 3)\n",
    "\n",
    "            grad_V.append(torch.matmul((qk * weight_xyz), grad_att_pos)[:, :, pos, :])\n",
    "            grad_score.append(torch.matmul(weight_xyz, grad_att_pos) * V[:, :, :, 0])\n",
    "\n",
    "            # grad_qk = grad_att_pos @ torch.linalg.inv(V_focus)\n",
    "            # grad_V_focus = torch.linalg.inv(qk) @ grad_att_pos\n",
    "            #\n",
    "            # grad_score.append(grad_qk)\n",
    "            # grad_V_focus = grad_V_focus * weight_xyz\n",
    "            # grad_V.append(grad_V_focus)\n",
    "\n",
    "        # Shape should be B, NH, T, T\n",
    "        grad_score = torch.cat(grad_score, dim=2)\n",
    "\n",
    "        # Shape should be B, NH, T, HS\n",
    "        grad_V = torch.cat(grad_V, dim=2)\n",
    "\n",
    "        return grad_score, grad_V\n",
    "\n",
    "focus = focusAttention.apply\n",
    "\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "input = (torch.randn(1, 1, 64, 64, dtype=torch.float64, requires_grad=True, device='cuda:0'), torch.randn(1, 1, 64, 4, dtype=torch.float64, requires_grad=True, device='cuda:0'))\n",
    "\n",
    "test = gradcheck(focus, input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
