{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n",
    "import sys\n",
    "sys.path.insert(0, '/userhome/42/msd21003/TATS')\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from tats import Net2NetTransformer, VideoData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_path:<class 'str'>,sequence_len:<class 'int'>,dataset:<class 'str'>,train:<class 'bool'>,dataset:<class 'type'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'istrain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m VideoData(args)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# pre-make relevant cached files if necessary\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m data\u001b[38;5;241m.\u001b[39mtest_dataloader()\n\u001b[1;32m     19\u001b[0m args\u001b[38;5;241m.\u001b[39mclass_cond_dim \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mn_classes \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39munconditional \u001b[38;5;129;01mand\u001b[39;00m args\u001b[38;5;241m.\u001b[39mcond_stage_key\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/TATS/tats/data.py:318\u001b[0m, in \u001b[0;36mVideoData.train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TATS/tats/data.py:297\u001b[0m, in \u001b[0;36mVideoData._dataloader\u001b[0;34m(self, train)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m, train):\n\u001b[0;32m--> 297\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dist\u001b[38;5;241m.\u001b[39mis_initialized():\n\u001b[1;32m    299\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mDistributedSampler(\n\u001b[1;32m    300\u001b[0m             dataset, num_replicas\u001b[38;5;241m=\u001b[39mdist\u001b[38;5;241m.\u001b[39mget_world_size(), rank\u001b[38;5;241m=\u001b[39mdist\u001b[38;5;241m.\u001b[39mget_rank()\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/TATS/tats/data.py:292\u001b[0m, in \u001b[0;36mVideoData._dataset\u001b[0;34m(self, train)\u001b[0m\n\u001b[1;32m    288\u001b[0m         Dataset \u001b[38;5;241m=\u001b[39m VideoDataset \u001b[38;5;28;01mif\u001b[39;00m osp\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata_path) \u001b[38;5;28;01melse\u001b[39;00m HDF5Dataset\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_path:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,sequence_len:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msequence_length)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,dataset:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,train:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,dataset:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(Dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 292\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mistrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'istrain'"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(1234)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser = pl.Trainer.add_argparse_args(parser)\n",
    "parser = Net2NetTransformer.add_model_specific_args(parser)\n",
    "parser = VideoData.add_data_specific_args(parser)\n",
    "\n",
    "args = parser.parse_args(args=[\"--num_workers\", \"32\", \"--val_check_interval\", \" 0.5\", \"--progress_bar_refresh_rate\", \" 500\",\n",
    "                    \"--gpus\", \" 8\" ,\"--sync_batchnorm\" ,\"--batch_size\", \" 3\",  \"--unconditional\",\n",
    "                    \"--vqvae\", \" ../../ckpt/vqgan_ucf.ckpt\", \"--data_path\", \" ../../ucf101\", \"--dataset\", \"ucf101\", \"--default_root_dir\", \" ../../trainGPT_ckpt\",\n",
    "                    \"--vocab_size\", \" 16384\", \"--block_size\", \" 1024\", \"--n_layer\", \" 24\", \"--n_head\", \" 16\", \"--n_embd\", \" 1024\",\n",
    "                    \"--resolution\", \" 128\", \"--sequence_length\", \" 16\", \"--max_steps\", \" 2000000\"])\n",
    "\n",
    "data = VideoData(args)\n",
    "# pre-make relevant cached files if necessary\n",
    "data.train_dataloader()\n",
    "data.test_dataloader()\n",
    "\n",
    "args.class_cond_dim = data.n_classes if not args.unconditional and args.cond_stage_key=='label' else None\n",
    "model = Net2NetTransformer(args, first_stage_key=args.first_stage_key, cond_stage_key=args.cond_stage_key)\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(ModelCheckpoint(every_n_train_steps=10000, save_top_k=-1, filename='{epoch}-{step}-{train/loss:.2f}'))\n",
    "callbacks.append(ModelCheckpoint(every_n_train_steps=50000, save_top_k=-1, filename='{epoch}-{step}-{train/loss:.2f}'))\n",
    "callbacks.append(ModelCheckpoint(monitor='val/loss', mode='min', save_top_k=3, filename='best_checkpoint'))\n",
    "\n",
    "kwargs = dict()\n",
    "if args.gpus > 1:\n",
    "    # find_unused_parameters = False to support gradient checkpointing\n",
    "    kwargs = dict(gpus=args.gpus,\n",
    "                  # plugins=[\"deepspeed_stage_2\"])\n",
    "                  plugins=[pl.plugins.DDPPlugin(find_unused_parameters=False)])\n",
    "\n",
    "# configure learning rate\n",
    "bs, base_lr = args.batch_size, args.base_lr\n",
    "ngpu = args.gpus\n",
    "accumulate_grad_batches = args.accumulate_grad_batches or 1\n",
    "print(f\"accumulate_grad_batches = {accumulate_grad_batches}\")\n",
    "model.learning_rate = accumulate_grad_batches * ngpu * bs * base_lr\n",
    "print(\"Setting learning rate to {:.2e} = {} (accumulate_grad_batches) * {} (num_gpus) * {} (batchsize) * {:.2e} (base_lr)\".format(\n",
    "    model.learning_rate, accumulate_grad_batches, ngpu, bs, base_lr))\n",
    "\n",
    "# load the most recent checkpoint file\n",
    "base_dir = os.path.join(args.default_root_dir, 'lightning_logs')\n",
    "if os.path.exists(base_dir):\n",
    "    log_folder = ckpt_file = ''\n",
    "    version_id_used = step_used = 0\n",
    "    for folder in os.listdir(base_dir):\n",
    "        version_id = int(folder.split('_')[1])\n",
    "        if version_id > version_id_used:\n",
    "            version_id_used = version_id\n",
    "            log_folder = folder\n",
    "    if len(log_folder) > 0:\n",
    "        ckpt_folder = os.path.join(base_dir, log_folder, 'checkpoints')\n",
    "        for fn in os.listdir(ckpt_folder):\n",
    "            if fn == 'latest_checkpoint.ckpt':\n",
    "                ckpt_file = 'latest_checkpoint_prev.ckpt'\n",
    "                os.rename(os.path.join(ckpt_folder, fn), os.path.join(ckpt_folder, ckpt_file))\n",
    "        if len(ckpt_file) > 0:\n",
    "            args.resume_from_checkpoint = os.path.join(ckpt_folder, ckpt_file)\n",
    "            print('will start from the recent ckpt %s'%args.resume_from_checkpoint)\n",
    "\n",
    "trainer = pl.Trainer.from_argparse_args(args, callbacks=callbacks,\n",
    "                                        max_steps=args.max_steps, **kwargs)\n",
    "\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
