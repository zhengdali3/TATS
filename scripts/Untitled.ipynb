{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n",
    "import sys\n",
    "sys.path.insert(0, '/userhome/42/msd21003/TATS')\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from tats import Net2NetTransformer, VideoData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_path:<class 'str'>,sequence_len:<class 'int'>,dataset:<class 'str'>,train:<class 'bool'>,dataset:<class 'type'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'istrain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m VideoData(args)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# pre-make relevant cached files if necessary\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m data\u001b[38;5;241m.\u001b[39mtest_dataloader()\n\u001b[1;32m     19\u001b[0m args\u001b[38;5;241m.\u001b[39mclass_cond_dim \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mn_classes \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39munconditional \u001b[38;5;129;01mand\u001b[39;00m args\u001b[38;5;241m.\u001b[39mcond_stage_key\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/TATS/tats/data.py:318\u001b[0m, in \u001b[0;36mVideoData.train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TATS/tats/data.py:297\u001b[0m, in \u001b[0;36mVideoData._dataloader\u001b[0;34m(self, train)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m, train):\n\u001b[0;32m--> 297\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dist\u001b[38;5;241m.\u001b[39mis_initialized():\n\u001b[1;32m    299\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mDistributedSampler(\n\u001b[1;32m    300\u001b[0m             dataset, num_replicas\u001b[38;5;241m=\u001b[39mdist\u001b[38;5;241m.\u001b[39mget_world_size(), rank\u001b[38;5;241m=\u001b[39mdist\u001b[38;5;241m.\u001b[39mget_rank()\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/TATS/tats/data.py:292\u001b[0m, in \u001b[0;36mVideoData._dataset\u001b[0;34m(self, train)\u001b[0m\n\u001b[1;32m    288\u001b[0m         Dataset \u001b[38;5;241m=\u001b[39m VideoDataset \u001b[38;5;28;01mif\u001b[39;00m osp\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata_path) \u001b[38;5;28;01melse\u001b[39;00m HDF5Dataset\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_path:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,sequence_len:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msequence_length)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,dataset:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,train:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,dataset:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(Dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 292\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mistrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'istrain'"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(1234)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser = pl.Trainer.add_argparse_args(parser)\n",
    "parser = Net2NetTransformer.add_model_specific_args(parser)\n",
    "parser = VideoData.add_data_specific_args(parser)\n",
    "\n",
    "args = parser.parse_args(args=[\"--num_workers\", \"32\", \"--val_check_interval\", \" 0.5\", \"--progress_bar_refresh_rate\", \" 500\",\n",
    "                    \"--gpus\", \" 8\" ,\"--sync_batchnorm\" ,\"--batch_size\", \" 3\",  \"--unconditional\",\n",
    "                    \"--vqvae\", \" ../../ckpt/vqgan_ucf.ckpt\", \"--data_path\", \" ../../ucf101\", \"--dataset\", \"ucf101\", \"--default_root_dir\", \" ../../trainGPT_ckpt\",\n",
    "                    \"--vocab_size\", \" 16384\", \"--block_size\", \" 1024\", \"--n_layer\", \" 24\", \"--n_head\", \" 16\", \"--n_embd\", \" 1024\",\n",
    "                    \"--resolution\", \" 128\", \"--sequence_length\", \" 16\", \"--max_steps\", \" 2000000\"])\n",
    "\n",
    "data = VideoData(args)\n",
    "# pre-make relevant cached files if necessary\n",
    "data.train_dataloader()\n",
    "data.test_dataloader()\n",
    "\n",
    "args.class_cond_dim = data.n_classes if not args.unconditional and args.cond_stage_key=='label' else None\n",
    "model = Net2NetTransformer(args, first_stage_key=args.first_stage_key, cond_stage_key=args.cond_stage_key)\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(ModelCheckpoint(every_n_train_steps=10000, save_top_k=-1, filename='{epoch}-{step}-{train/loss:.2f}'))\n",
    "callbacks.append(ModelCheckpoint(every_n_train_steps=50000, save_top_k=-1, filename='{epoch}-{step}-{train/loss:.2f}'))\n",
    "callbacks.append(ModelCheckpoint(monitor='val/loss', mode='min', save_top_k=3, filename='best_checkpoint'))\n",
    "\n",
    "kwargs = dict()\n",
    "if args.gpus > 1:\n",
    "    # find_unused_parameters = False to support gradient checkpointing\n",
    "    kwargs = dict(gpus=args.gpus,\n",
    "                  # plugins=[\"deepspeed_stage_2\"])\n",
    "                  plugins=[pl.plugins.DDPPlugin(find_unused_parameters=False)])\n",
    "\n",
    "# configure learning rate\n",
    "bs, base_lr = args.batch_size, args.base_lr\n",
    "ngpu = args.gpus\n",
    "accumulate_grad_batches = args.accumulate_grad_batches or 1\n",
    "print(f\"accumulate_grad_batches = {accumulate_grad_batches}\")\n",
    "model.learning_rate = accumulate_grad_batches * ngpu * bs * base_lr\n",
    "print(\"Setting learning rate to {:.2e} = {} (accumulate_grad_batches) * {} (num_gpus) * {} (batchsize) * {:.2e} (base_lr)\".format(\n",
    "    model.learning_rate, accumulate_grad_batches, ngpu, bs, base_lr))\n",
    "\n",
    "# load the most recent checkpoint file\n",
    "base_dir = os.path.join(args.default_root_dir, 'lightning_logs')\n",
    "if os.path.exists(base_dir):\n",
    "    log_folder = ckpt_file = ''\n",
    "    version_id_used = step_used = 0\n",
    "    for folder in os.listdir(base_dir):\n",
    "        version_id = int(folder.split('_')[1])\n",
    "        if version_id > version_id_used:\n",
    "            version_id_used = version_id\n",
    "            log_folder = folder\n",
    "    if len(log_folder) > 0:\n",
    "        ckpt_folder = os.path.join(base_dir, log_folder, 'checkpoints')\n",
    "        for fn in os.listdir(ckpt_folder):\n",
    "            if fn == 'latest_checkpoint.ckpt':\n",
    "                ckpt_file = 'latest_checkpoint_prev.ckpt'\n",
    "                os.rename(os.path.join(ckpt_folder, fn), os.path.join(ckpt_folder, ckpt_file))\n",
    "        if len(ckpt_file) > 0:\n",
    "            args.resume_from_checkpoint = os.path.join(ckpt_folder, ckpt_file)\n",
    "            print('will start from the recent ckpt %s'%args.resume_from_checkpoint)\n",
    "\n",
    "trainer = pl.Trainer.from_argparse_args(args, callbacks=callbacks,\n",
    "                                        max_steps=args.max_steps, **kwargs)\n",
    "\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "import math\n",
    "import numpy\n",
    "from tqdm import trange\n",
    "from torch.utils.dlpack import to_dlpack\n",
    "from torch.utils.dlpack import from_dlpack\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# def getGaussian(T, H, W, beta, d):\n",
    "    \n",
    "#     tensor = torch.tensor((), dtype=torch.float32)\n",
    "#     weight = tensor.new_ones((T*H*W, T*H*W), device=d)\n",
    "#     diag = numpy.diag([beta[0], beta[1], beta[1]])\n",
    "    \n",
    "#     rv_list = []\n",
    "    \n",
    "#     for x in range(T):\n",
    "#         for y in range(H):\n",
    "#             for z in range(W):\n",
    "#                 rv_list.append(multivariate_normal([x, y, z], diag))\n",
    "    \n",
    "#     for idx in range(T*H*W):\n",
    "#         for pos in range(T*H*W):\n",
    "#             i = math.floor(pos/(H*W))\n",
    "#             j = math.floor((pos - i * H * W) / H)\n",
    "#             k = pos - i * H * W - j * W\n",
    "#             # print(f\"i {i}, j {j}, k {k}\")\n",
    "#             weight[idx, pos] = rv_list[idx].pdf([i, j, k])\n",
    "        \n",
    "#         # print(weight[idx, :])\n",
    "#         weight[idx, :] = weight[idx, :] / torch.max(weight[idx, :])\n",
    "    \n",
    "#     return weight\n",
    "\n",
    "# def getGaussian(T, H, W, beta, x, y, z, d):\n",
    "    \n",
    "#     diag = numpy.diag([beta[0], beta[1], beta[1]])\n",
    "\n",
    "#     rv = multivariate_normal([x, y, z], diag)\n",
    "    \n",
    "#     tensor = torch.tensor((), dtype=torch.float32)\n",
    "#     weight = tensor.new_ones((T*H*W,), device=d)\n",
    "    \n",
    "#     # gau = trange(T*H*W, desc='Gaussian Matrix', leave=False)\n",
    "    \n",
    "#     for pos in numpy.arange(0, T*H*W):\n",
    "#         i = math.floor(pos/(H*W))\n",
    "#         j = math.floor((pos - i * H * W) / H)\n",
    "#         k = pos - i * H * W - j * W\n",
    "#         # print(f\"i {i}, j {j}, k {k}\")\n",
    "#         weight[pos] = rv.pdf([i, j, k])\n",
    "        \n",
    "#         weight = weight / torch.max(weight)\n",
    "\n",
    "#     return weight\n",
    "\n",
    "def getGaussian(T, H, W, beta, d):\n",
    "    \n",
    "    diag = numpy.diag([beta[0], beta[1], beta[1]])\n",
    "\n",
    "    rv = multivariate_normal([T-1, H-1, W-1], diag)\n",
    "    \n",
    "    tensor = torch.tensor((), dtype=torch.float32)\n",
    "    \n",
    "    NT = 2*T-1\n",
    "    NH = 2*H-1\n",
    "    NW = 2*W-1\n",
    "    \n",
    "    weight = tensor.new_ones((NT, NW, NH), device=d)\n",
    "\n",
    "    # gau = trange(T*H*W, desc='Gaussian Matrix', leave=False)\n",
    "    \n",
    "    for pos in numpy.arange(0, NT*NH*NW):\n",
    "        i = math.floor(pos/(NH*NW))\n",
    "        j = math.floor((pos - i * NH * NW) / NH)\n",
    "        k = pos - i * NH * NW - j * NW\n",
    "        # print(f\"i {i}, j {j}, k {k}\")\n",
    "        weight[i, j, k] = rv.pdf([i, j, k])\n",
    "        \n",
    "        weight = weight / torch.max(weight)\n",
    "\n",
    "    return weight\n",
    "\n",
    "\n",
    "# def FocusedAttention(score, V):\n",
    "#     V = cupy.asarray(V)\n",
    "#     # Q = cupy.asarray(Q)\n",
    "#     # K = cupy.asarray(K)\n",
    "    \n",
    "#     B = V.shape[0]\n",
    "#     NH = V.shape[1]\n",
    "#     T_flatten = V.shape[2]\n",
    "#     HS = V.shape[3]\n",
    "\n",
    "#     T = 4\n",
    "#     H = 4\n",
    "#     W = 4\n",
    "#     beta = [100, 100]\n",
    "    \n",
    "#     # V = V.reshape(B * NH, T, H, W, HS)\n",
    "#     # Q = Q.reshape(B * NH, T, H, W, HS)\n",
    "#     # K = K.reshape(B * NH, T, H, W, HS)\n",
    "#     # A = cupy.zeros((B, NH, T, H, W, HS))\n",
    "    \n",
    "#     V_full = cupy.ones((T_flatten, B, NH, T_flatten, HS))\n",
    "    \n",
    "#     V_boardcast = V[cupy.newaxis, :]\n",
    "    \n",
    "#     V_full = V_full * V_boardcast\n",
    "    \n",
    "#     weight = getGaussian(T, H, W, beta)\n",
    "    \n",
    "#     weight = weight[:, cupy.newaxis, cupy.newaxis, :, cupy.newaxis]\n",
    "    \n",
    "#     print(f\"weight {weight}\")\n",
    "    \n",
    "#     print(f\"before {V_full}\")\n",
    "    \n",
    "#     # V_full shape is (T_flatten, B, NH, T_flatten, HS)\n",
    "#     V_full = V_full * weight\n",
    "    \n",
    "#     print(f\"After {V_full}\")\n",
    "    \n",
    "#     # print(f\"v full shape is {V_full.shape}, \")\n",
    "    \n",
    "#     att = cupy.ones((B, NH, T_flatten, HS))\n",
    "    \n",
    "#     score = cupy.asarray(score)\n",
    "    \n",
    "#     for pos in range(T_flatten):\n",
    "        \n",
    "#         # qk shape (B, NH, 1, T)\n",
    "#         qk = score[:, :, pos, :]\n",
    "#         qk = qk[:, :, cupy.newaxis, :]\n",
    "        \n",
    "#         # att_pos shape (B, NH, 1, HS)\n",
    "#         att_pos = qk @ V_full[pos, :, :, :, :]\n",
    "        \n",
    "#         # print(f\"qk {qk.shape}, V_full {V_full[pos, :, :, :, :].shape}, att_pos {att_pos.shape}\")\n",
    "        \n",
    "#         att[:,:,pos,:] = att_pos[:, :, 0, :]\n",
    "    \n",
    "#     att = from_dlpack(att.toDlpack())\n",
    "    \n",
    "#     return att\n",
    "\n",
    "def FocusedAttention(score, V):\n",
    "    d = V.get_device()\n",
    "    \n",
    "    # V = cupy.asarray(V)\n",
    "    print(f\"before V clone {torch.cuda.memory_allocated()}\")\n",
    "    V_ori = torch.clone(V)\n",
    "    print(f\"After V clone {torch.cuda.memory_allocated()}\")\n",
    "\n",
    "    B = V.shape[0]\n",
    "    NH = V.shape[1]\n",
    "    T_flatten = V.shape[2]\n",
    "    HS = V.shape[3]\n",
    "\n",
    "    T = 4\n",
    "    H = 4\n",
    "    W = 4\n",
    "    beta = [100, 100]\n",
    "    \n",
    "    att = []\n",
    "    \n",
    "    center_T = T-1\n",
    "    center_H = H-1\n",
    "    center_W = W-1\n",
    "    \n",
    "    weight = getGaussian(T, H, W, beta, d)\n",
    "\n",
    "    # foc = trange(T_flatten, desc='Focused Attention', leave=False)\n",
    "    \n",
    "    mem_loop_st = torch.cuda.memory_allocated()\n",
    "    print(f\"before loop {mem_loop_st}\")\n",
    "    \n",
    "    for pos in numpy.arange(0, T_flatten):\n",
    "        \n",
    "        # print(f\"start iter {torch.cuda.memory_allocated()}\")\n",
    "        \n",
    "        i = math.floor(pos/(H*W))\n",
    "        j = math.floor((pos - i * H * W) / H)\n",
    "        k = pos - i * H * W - j * W\n",
    "        \n",
    "        # weight_xyz = weight[center_T-i:2*center_T-i + 1, center_W-j:2*center_W-j + 1, center_H-k:2*center_H-k + 1]\n",
    "        \n",
    "        # print(weight_xyz.shape)\n",
    "        \n",
    "        weight_xyz = weight[center_T-i:2*center_T-i + 1, center_W-j:2*center_W-j + 1, center_H-k:2*center_H-k + 1].reshape(-1)\n",
    "\n",
    "        weight_xyz = weight_xyz[None, None, :, None]\n",
    "        # print(V.shape)\n",
    "        \n",
    "        V = V*weight_xyz\n",
    "        \n",
    "        del weight_xyz\n",
    "        \n",
    "        # qk shape (B, NH, 1, T)\n",
    "        qk = score[:, :, pos, :]\n",
    "        qk = qk[:, :, None, :]\n",
    "\n",
    "        att_pos = qk @ V\n",
    "        att.append(att_pos)\n",
    "        \n",
    "        V = torch.clone(V_ori)\n",
    "        \n",
    "        # print(f\"end iter {torch.cuda.memory_allocated()}\")\n",
    "    \n",
    "    print(f\"end loop used {torch.cuda.memory_allocated() - mem_loop_st}\")\n",
    "    result = torch.cat(att, dim=2)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# def FocusedAttention(score, V):\n",
    "#     d = V.get_device()\n",
    "    \n",
    "#     # V = cupy.asarray(V)\n",
    "#     V_ori = torch.clone(V)\n",
    "\n",
    "#     B = V.shape[0]\n",
    "#     NH = V.shape[1]\n",
    "#     T_flatten = V.shape[2]\n",
    "#     HS = V.shape[3]\n",
    "\n",
    "#     T = 4\n",
    "#     H = 4\n",
    "#     W = 4\n",
    "#     beta = [100, 100]\n",
    "    \n",
    "#     # att = tensor.new_ones((B, NH, T_flatten, HS), requires_grad=True, device=d)\n",
    "    \n",
    "#     att = []\n",
    "#     # score = cupy.asarray(score)\n",
    "    \n",
    "#     # foc = trange(T_flatten, desc='Focused Attention', leave=True)\n",
    "    \n",
    "#     weight = getGaussian(T, H, W, beta, d)\n",
    "    \n",
    "#     # print(f\"weight {weight}\")\n",
    "    \n",
    "#     weight = weight[:, None, None, :, None]\n",
    "    \n",
    "    \n",
    "#     for pos in range(T_flatten):\n",
    "        \n",
    "#         i = math.floor(pos/(H*W))\n",
    "#         j = math.floor((pos - i * H * W) / H)\n",
    "#         k = pos - i * H * W - j * W\n",
    "#         V = V*weight[pos, :, :, :, :]\n",
    "        \n",
    "#         # qk shape (B, NH, 1, T)\n",
    "#         qk = score[:, :, pos, :]\n",
    "#         qk = qk[:, :, None, :]\n",
    "\n",
    "#         att_pos = qk @ V\n",
    "#         att.append(att_pos)\n",
    "        \n",
    "#         # att[:,:,pos,:] = att[:,:,pos,:] * att_pos[:, :, 0, :]\n",
    "        \n",
    "#         V = torch.clone(V_ori)\n",
    "    \n",
    "#     # att = from_dlpack(att.toDlpack())\n",
    "    \n",
    "#     result = torch.cat(att, dim=2)\n",
    "    \n",
    "#     # print(att, result.shape)\n",
    "    \n",
    "#     return result\n",
    "\n",
    "# def FocusedAttention(score, V):\n",
    "#     d = V.get_device()\n",
    "    \n",
    "#     # V = cupy.asarray(V)\n",
    "#     V_ori = torch.clone(V)\n",
    "\n",
    "#     B = V.shape[0]\n",
    "#     NH = V.shape[1]\n",
    "#     T_flatten = V.shape[2]\n",
    "#     HS = V.shape[3]\n",
    "\n",
    "#     T = 4\n",
    "#     H = 4\n",
    "#     W = 4\n",
    "#     beta = [100, 100]\n",
    "    \n",
    "#     # att = tensor.new_ones((B, NH, T_flatten, HS), requires_grad=True, device=d)\n",
    "    \n",
    "#     att = []\n",
    "    \n",
    "#     center_T = T-1\n",
    "#     center_H = H-1\n",
    "#     center_W = W-1\n",
    "    \n",
    "#     # score = cupy.asarray(score)\n",
    "    \n",
    "#     # foc = trange(T_flatten, desc='Focused Attention', leave=True)\n",
    "    \n",
    "#     weight = getGaussian(T, H, W, beta, d)\n",
    "    \n",
    "#     # print(f\"weight {weight}\")\n",
    "    \n",
    "#     weight_xyz = weight[:, None, None, :, None]\n",
    "    \n",
    "    \n",
    "#     for pos in range(T_flatten):\n",
    "        \n",
    "#         i = math.floor(pos/(H*W))\n",
    "#         j = math.floor((pos - i * H * W) / H)\n",
    "#         k = pos - i * H * W - j * W\n",
    "        \n",
    "#         V = V*weight[pos, :, :, :, :]\n",
    "        \n",
    "#         # qk shape (B, NH, 1, T)\n",
    "#         qk = score[:, :, pos, :]\n",
    "#         qk = qk[:, :, None, :]\n",
    "\n",
    "#         att_pos = qk @ V\n",
    "#         att.append(att_pos)\n",
    "        \n",
    "#         # att[:,:,pos,:] = att[:,:,pos,:] * att_pos[:, :, 0, :]\n",
    "        \n",
    "#         V = torch.clone(V_ori)\n",
    "    \n",
    "#     # att = from_dlpack(att.toDlpack())\n",
    "    \n",
    "#     result = torch.cat(att, dim=2)\n",
    "    \n",
    "#     # print(att, result.shape)\n",
    "    \n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before V clone 17047552\n",
      "After V clone 17049600\n",
      "before loop 17051136\n",
      "start iter 17051136\n",
      "end iter 17053696\n",
      "start iter 17053696\n",
      "end iter 17054208\n",
      "start iter 17054208\n",
      "end iter 17054720\n",
      "start iter 17054720\n",
      "end iter 17055232\n",
      "start iter 17055232\n",
      "end iter 17055744\n",
      "start iter 17055744\n",
      "end iter 17056256\n",
      "start iter 17056256\n",
      "end iter 17056768\n",
      "start iter 17056768\n",
      "end iter 17057280\n",
      "start iter 17057280\n",
      "end iter 17057792\n",
      "start iter 17057792\n",
      "end iter 17058304\n",
      "start iter 17058304\n",
      "end iter 17058816\n",
      "start iter 17058816\n",
      "end iter 17059328\n",
      "start iter 17059328\n",
      "end iter 17059840\n",
      "start iter 17059840\n",
      "end iter 17060352\n",
      "start iter 17060352\n",
      "end iter 17060864\n",
      "start iter 17060864\n",
      "end iter 17061376\n",
      "start iter 17061376\n",
      "end iter 17061888\n",
      "start iter 17061888\n",
      "end iter 17062400\n",
      "start iter 17062400\n",
      "end iter 17062912\n",
      "start iter 17062912\n",
      "end iter 17063424\n",
      "start iter 17063424\n",
      "end iter 17063936\n",
      "start iter 17063936\n",
      "end iter 17064448\n",
      "start iter 17064448\n",
      "end iter 17064960\n",
      "start iter 17064960\n",
      "end iter 17065472\n",
      "start iter 17065472\n",
      "end iter 17065984\n",
      "start iter 17065984\n",
      "end iter 17066496\n",
      "start iter 17066496\n",
      "end iter 17067008\n",
      "start iter 17067008\n",
      "end iter 17067520\n",
      "start iter 17067520\n",
      "end iter 17068032\n",
      "start iter 17068032\n",
      "end iter 17068544\n",
      "start iter 17068544\n",
      "end iter 17069056\n",
      "start iter 17069056\n",
      "end iter 17069568\n",
      "start iter 17069568\n",
      "end iter 17070080\n",
      "start iter 17070080\n",
      "end iter 17070592\n",
      "start iter 17070592\n",
      "end iter 17071104\n",
      "start iter 17071104\n",
      "end iter 17071616\n",
      "start iter 17071616\n",
      "end iter 17072128\n",
      "start iter 17072128\n",
      "end iter 17072640\n",
      "start iter 17072640\n",
      "end iter 17073152\n",
      "start iter 17073152\n",
      "end iter 17073664\n",
      "start iter 17073664\n",
      "end iter 17074176\n",
      "start iter 17074176\n",
      "end iter 17074688\n",
      "start iter 17074688\n",
      "end iter 17075200\n",
      "start iter 17075200\n",
      "end iter 17075712\n",
      "start iter 17075712\n",
      "end iter 17076224\n",
      "start iter 17076224\n",
      "end iter 17076736\n",
      "start iter 17076736\n",
      "end iter 17077248\n",
      "start iter 17077248\n",
      "end iter 17077760\n",
      "start iter 17077760\n",
      "end iter 17078272\n",
      "start iter 17078272\n",
      "end iter 17078784\n",
      "start iter 17078784\n",
      "end iter 17079296\n",
      "start iter 17079296\n",
      "end iter 17079808\n",
      "start iter 17079808\n",
      "end iter 17080320\n",
      "start iter 17080320\n",
      "end iter 17080832\n",
      "start iter 17080832\n",
      "end iter 17081344\n",
      "start iter 17081344\n",
      "end iter 17081856\n",
      "start iter 17081856\n",
      "end iter 17082368\n",
      "start iter 17082368\n",
      "end iter 17082880\n",
      "start iter 17082880\n",
      "end iter 17083392\n",
      "start iter 17083392\n",
      "end iter 17083904\n",
      "start iter 17083904\n",
      "end iter 17084416\n",
      "start iter 17084416\n",
      "end iter 17084928\n",
      "start iter 17084928\n",
      "end iter 17085440\n",
      "start iter 17085440\n",
      "end iter 17085952\n",
      "end loop used 34816\n",
      "before V clone 17047552\n",
      "After V clone 17049600\n",
      "before loop 17051136\n",
      "start iter 17051136\n",
      "end iter 17053696\n",
      "start iter 17053696\n",
      "end iter 17054208\n",
      "start iter 17054208\n",
      "end iter 17054720\n",
      "start iter 17054720\n",
      "end iter 17055232\n",
      "start iter 17055232\n",
      "end iter 17055744\n",
      "start iter 17055744\n",
      "end iter 17056256\n",
      "start iter 17056256\n",
      "end iter 17056768\n",
      "start iter 17056768\n",
      "end iter 17057280\n",
      "start iter 17057280\n",
      "end iter 17057792\n",
      "start iter 17057792\n",
      "end iter 17058304\n",
      "start iter 17058304\n",
      "end iter 17058816\n",
      "start iter 17058816\n",
      "end iter 17059328\n",
      "start iter 17059328\n",
      "end iter 17059840\n",
      "start iter 17059840\n",
      "end iter 17060352\n",
      "start iter 17060352\n",
      "end iter 17060864\n",
      "start iter 17060864\n",
      "end iter 17061376\n",
      "start iter 17061376\n",
      "end iter 17061888\n",
      "start iter 17061888\n",
      "end iter 17062400\n",
      "start iter 17062400\n",
      "end iter 17062912\n",
      "start iter 17062912\n",
      "end iter 17063424\n",
      "start iter 17063424\n",
      "end iter 17063936\n",
      "start iter 17063936\n",
      "end iter 17064448\n",
      "start iter 17064448\n",
      "end iter 17064960\n",
      "start iter 17064960\n",
      "end iter 17065472\n",
      "start iter 17065472\n",
      "end iter 17065984\n",
      "start iter 17065984\n",
      "end iter 17066496\n",
      "start iter 17066496\n",
      "end iter 17067008\n",
      "start iter 17067008\n",
      "end iter 17067520\n",
      "start iter 17067520\n",
      "end iter 17068032\n",
      "start iter 17068032\n",
      "end iter 17068544\n",
      "start iter 17068544\n",
      "end iter 17069056\n",
      "start iter 17069056\n",
      "end iter 17069568\n",
      "start iter 17069568\n",
      "end iter 17070080\n",
      "start iter 17070080\n",
      "end iter 17070592\n",
      "start iter 17070592\n",
      "end iter 17071104\n",
      "start iter 17071104\n",
      "end iter 17071616\n",
      "start iter 17071616\n",
      "end iter 17072128\n",
      "start iter 17072128\n",
      "end iter 17072640\n",
      "start iter 17072640\n",
      "end iter 17073152\n",
      "start iter 17073152\n",
      "end iter 17073664\n",
      "start iter 17073664\n",
      "end iter 17074176\n",
      "start iter 17074176\n",
      "end iter 17074688\n",
      "start iter 17074688\n",
      "end iter 17075200\n",
      "start iter 17075200\n",
      "end iter 17075712\n",
      "start iter 17075712\n",
      "end iter 17076224\n",
      "start iter 17076224\n",
      "end iter 17076736\n",
      "start iter 17076736\n",
      "end iter 17077248\n",
      "start iter 17077248\n",
      "end iter 17077760\n",
      "start iter 17077760\n",
      "end iter 17078272\n",
      "start iter 17078272\n",
      "end iter 17078784\n",
      "start iter 17078784\n",
      "end iter 17079296\n",
      "start iter 17079296\n",
      "end iter 17079808\n",
      "start iter 17079808\n",
      "end iter 17080320\n",
      "start iter 17080320\n",
      "end iter 17080832\n",
      "start iter 17080832\n",
      "end iter 17081344\n",
      "start iter 17081344\n",
      "end iter 17081856\n",
      "start iter 17081856\n",
      "end iter 17082368\n",
      "start iter 17082368\n",
      "end iter 17082880\n",
      "start iter 17082880\n",
      "end iter 17083392\n",
      "start iter 17083392\n",
      "end iter 17083904\n",
      "start iter 17083904\n",
      "end iter 17084416\n",
      "start iter 17084416\n",
      "end iter 17084928\n",
      "start iter 17084928\n",
      "end iter 17085440\n",
      "start iter 17085440\n",
      "end iter 17085952\n",
      "end loop used 34816\n",
      "tensor([[[[17.4191, 16.3169],\n",
      "          [15.6101, 14.2356],\n",
      "          [16.0185, 13.7510],\n",
      "          [14.7444, 14.7128],\n",
      "          [13.3954, 13.2356],\n",
      "          [16.1997, 18.0319],\n",
      "          [16.9160, 16.2648],\n",
      "          [13.8602, 12.7434],\n",
      "          [15.0947, 15.5169],\n",
      "          [15.4313, 16.1146],\n",
      "          [16.2102, 15.5109],\n",
      "          [15.2971, 16.2284],\n",
      "          [16.4217, 16.0739],\n",
      "          [17.6391, 17.4518],\n",
      "          [16.3083, 15.0231],\n",
      "          [17.0068, 16.4986],\n",
      "          [14.9259, 16.1133],\n",
      "          [15.9568, 16.6757],\n",
      "          [15.0296, 15.6000],\n",
      "          [14.8210, 13.7928],\n",
      "          [17.7585, 17.7076],\n",
      "          [14.1601, 14.1066],\n",
      "          [14.7384, 15.3292],\n",
      "          [15.8839, 15.4306],\n",
      "          [16.6239, 15.9097],\n",
      "          [14.3041, 13.0153],\n",
      "          [15.7981, 15.0480],\n",
      "          [13.9335, 13.6556],\n",
      "          [16.3247, 14.6794],\n",
      "          [14.4621, 14.0951],\n",
      "          [14.9112, 15.9874],\n",
      "          [14.9763, 15.5496],\n",
      "          [15.2100, 14.8648],\n",
      "          [17.0470, 16.2941],\n",
      "          [16.1749, 15.4641],\n",
      "          [15.9328, 14.0403],\n",
      "          [15.7112, 16.3857],\n",
      "          [14.3359, 14.1463],\n",
      "          [15.9552, 15.8491],\n",
      "          [17.3538, 15.7612],\n",
      "          [16.6612, 16.8530],\n",
      "          [16.3937, 17.1890],\n",
      "          [16.2257, 15.8294],\n",
      "          [14.5466, 14.0046],\n",
      "          [15.8192, 15.0362],\n",
      "          [15.7531, 15.1658],\n",
      "          [16.4984, 16.0874],\n",
      "          [14.0336, 15.1942],\n",
      "          [16.1531, 15.2905],\n",
      "          [14.3818, 15.6641],\n",
      "          [16.4701, 15.2789],\n",
      "          [16.9449, 15.1901],\n",
      "          [14.8928, 15.6138],\n",
      "          [15.2616, 16.7531],\n",
      "          [16.0690, 15.2002],\n",
      "          [15.0701, 14.6543],\n",
      "          [15.4905, 14.3244],\n",
      "          [15.1456, 14.7341],\n",
      "          [15.7702, 16.2423],\n",
      "          [17.1478, 17.5375],\n",
      "          [16.7885, 17.3183],\n",
      "          [16.1946, 17.1408],\n",
      "          [16.4945, 15.8734],\n",
      "          [13.2419, 13.8956]],\n",
      "\n",
      "         [[15.7385, 13.1109],\n",
      "          [16.5641, 14.0753],\n",
      "          [14.9806, 14.4624],\n",
      "          [16.4818, 14.9865],\n",
      "          [15.1232, 14.2418],\n",
      "          [14.2834, 12.9092],\n",
      "          [17.3048, 16.2664],\n",
      "          [16.0245, 14.9887],\n",
      "          [15.0506, 12.4652],\n",
      "          [14.9396, 13.7689],\n",
      "          [17.5148, 14.3237],\n",
      "          [13.2835, 13.6052],\n",
      "          [15.1752, 13.9092],\n",
      "          [15.7679, 13.1233],\n",
      "          [16.0703, 14.9850],\n",
      "          [15.2473, 12.9591],\n",
      "          [17.2326, 14.9509],\n",
      "          [15.8083, 14.3894],\n",
      "          [18.2390, 14.6574],\n",
      "          [16.4639, 15.6969],\n",
      "          [15.6592, 15.3655],\n",
      "          [18.0863, 15.5805],\n",
      "          [14.1426, 13.3948],\n",
      "          [14.8479, 13.8322],\n",
      "          [15.5880, 12.5676],\n",
      "          [16.3768, 14.2382],\n",
      "          [16.3410, 14.3575],\n",
      "          [16.4480, 14.8350],\n",
      "          [14.4117, 12.9878],\n",
      "          [14.0140, 12.9982],\n",
      "          [17.3960, 14.9890],\n",
      "          [14.4466, 12.4249],\n",
      "          [16.6377, 13.9025],\n",
      "          [15.5737, 14.3422],\n",
      "          [14.7006, 13.6347],\n",
      "          [12.6270, 12.4664],\n",
      "          [14.1797, 12.0186],\n",
      "          [15.2088, 13.7891],\n",
      "          [14.6567, 14.3617],\n",
      "          [15.1310, 14.1375],\n",
      "          [17.0709, 14.6120],\n",
      "          [16.7250, 14.3536],\n",
      "          [14.2336, 12.5784],\n",
      "          [16.7428, 14.3501],\n",
      "          [13.9295, 13.5346],\n",
      "          [15.9039, 15.3692],\n",
      "          [15.7303, 14.8006],\n",
      "          [17.8150, 14.6586],\n",
      "          [13.3411, 12.5796],\n",
      "          [15.6479, 13.7979],\n",
      "          [14.2321, 12.5538],\n",
      "          [14.4844, 13.5390],\n",
      "          [14.3095, 12.1352],\n",
      "          [15.4100, 13.8844],\n",
      "          [14.9337, 13.6316],\n",
      "          [16.5540, 15.1235],\n",
      "          [15.6225, 13.4869],\n",
      "          [15.0925, 14.3960],\n",
      "          [14.2661, 13.0217],\n",
      "          [14.0711, 14.0308],\n",
      "          [17.4717, 15.2008],\n",
      "          [16.0612, 13.8311],\n",
      "          [14.5224, 13.4933],\n",
      "          [14.6778, 13.0196]]],\n",
      "\n",
      "\n",
      "        [[[18.1202, 16.2478],\n",
      "          [16.9453, 16.0468],\n",
      "          [15.6319, 16.0014],\n",
      "          [15.7520, 15.2579],\n",
      "          [16.7635, 17.2703],\n",
      "          [16.9449, 17.1818],\n",
      "          [16.7911, 15.3966],\n",
      "          [17.5839, 16.6283],\n",
      "          [15.5447, 14.3002],\n",
      "          [16.6311, 15.2006],\n",
      "          [16.3162, 14.5213],\n",
      "          [15.1700, 14.5355],\n",
      "          [15.8747, 15.5312],\n",
      "          [13.8182, 13.7078],\n",
      "          [15.9954, 15.0328],\n",
      "          [14.7567, 15.8301],\n",
      "          [15.9886, 14.5239],\n",
      "          [17.6541, 16.6654],\n",
      "          [16.6619, 15.7748],\n",
      "          [16.4455, 16.7796],\n",
      "          [17.3845, 16.1545],\n",
      "          [18.3250, 18.8106],\n",
      "          [15.3438, 14.9518],\n",
      "          [17.9842, 17.7865],\n",
      "          [13.7892, 13.5716],\n",
      "          [19.3856, 17.7472],\n",
      "          [16.9385, 16.6553],\n",
      "          [15.5130, 13.2654],\n",
      "          [15.3866, 15.9627],\n",
      "          [16.7489, 14.5173],\n",
      "          [17.3557, 17.0930],\n",
      "          [18.2129, 17.2692],\n",
      "          [16.4571, 17.3454],\n",
      "          [19.0063, 17.4249],\n",
      "          [17.3333, 16.2804],\n",
      "          [17.4329, 16.3503],\n",
      "          [16.6614, 14.8055],\n",
      "          [19.0551, 18.1808],\n",
      "          [16.1838, 15.4434],\n",
      "          [15.5209, 14.9813],\n",
      "          [18.4468, 16.9900],\n",
      "          [18.0500, 16.6771],\n",
      "          [15.7767, 15.0528],\n",
      "          [17.3788, 17.6334],\n",
      "          [17.6388, 16.7678],\n",
      "          [17.0314, 16.3864],\n",
      "          [20.3665, 18.4501],\n",
      "          [18.2706, 16.3829],\n",
      "          [16.5181, 17.5824],\n",
      "          [15.3188, 15.8512],\n",
      "          [17.6311, 16.2964],\n",
      "          [13.9420, 13.3786],\n",
      "          [19.1063, 17.2332],\n",
      "          [16.0011, 15.9379],\n",
      "          [14.9111, 14.3514],\n",
      "          [15.2077, 16.1613],\n",
      "          [19.0633, 17.9025],\n",
      "          [16.0221, 16.0091],\n",
      "          [16.9213, 15.5989],\n",
      "          [16.2633, 15.4337],\n",
      "          [16.4540, 15.8133],\n",
      "          [15.9982, 15.5383],\n",
      "          [16.1014, 15.3968],\n",
      "          [17.6171, 16.5278]],\n",
      "\n",
      "         [[16.4799, 15.0383],\n",
      "          [15.7761, 16.1566],\n",
      "          [16.4497, 16.4783],\n",
      "          [18.3208, 17.1108],\n",
      "          [15.5555, 15.8768],\n",
      "          [16.7331, 16.3390],\n",
      "          [15.8624, 14.1462],\n",
      "          [16.9892, 14.5039],\n",
      "          [16.5252, 16.0781],\n",
      "          [15.0871, 13.0328],\n",
      "          [16.7434, 16.4726],\n",
      "          [17.3608, 15.5928],\n",
      "          [13.8807, 13.9621],\n",
      "          [15.1331, 14.2629],\n",
      "          [15.2766, 15.1541],\n",
      "          [14.7497, 14.1273],\n",
      "          [15.8960, 14.7369],\n",
      "          [16.8812, 15.3017],\n",
      "          [15.6619, 15.8918],\n",
      "          [14.2227, 14.2194],\n",
      "          [15.5322, 14.2813],\n",
      "          [14.9333, 13.6002],\n",
      "          [15.2256, 14.8873],\n",
      "          [16.4074, 15.1693],\n",
      "          [15.3135, 13.2356],\n",
      "          [16.5990, 16.3201],\n",
      "          [17.1005, 16.9220],\n",
      "          [16.7203, 15.8228],\n",
      "          [12.5711, 12.6603],\n",
      "          [14.1457, 15.4709],\n",
      "          [17.7267, 16.9351],\n",
      "          [16.4437, 17.1370],\n",
      "          [15.4407, 15.0989],\n",
      "          [16.9946, 15.3702],\n",
      "          [15.9214, 15.0272],\n",
      "          [15.8851, 14.4223],\n",
      "          [16.6708, 15.5961],\n",
      "          [15.5278, 14.2156],\n",
      "          [14.2324, 15.4201],\n",
      "          [16.2425, 16.0438],\n",
      "          [13.8678, 12.6991],\n",
      "          [16.3849, 14.3619],\n",
      "          [15.4847, 14.3021],\n",
      "          [15.5103, 15.0225],\n",
      "          [15.1862, 14.9282],\n",
      "          [15.2113, 14.6868],\n",
      "          [15.6113, 14.2463],\n",
      "          [14.7048, 15.0373],\n",
      "          [17.6401, 16.0127],\n",
      "          [13.8876, 13.8942],\n",
      "          [18.1348, 17.1613],\n",
      "          [15.5502, 14.4693],\n",
      "          [15.3878, 13.8327],\n",
      "          [16.1134, 13.9376],\n",
      "          [14.3596, 12.9579],\n",
      "          [14.2653, 15.6380],\n",
      "          [15.3375, 13.3232],\n",
      "          [16.1991, 14.4440],\n",
      "          [17.1878, 16.5641],\n",
      "          [16.7109, 14.4953],\n",
      "          [14.9764, 13.8139],\n",
      "          [18.3987, 18.4328],\n",
      "          [16.2245, 15.2192],\n",
      "          [15.0133, 13.8013]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "cuda = torch.device('cuda', 0)\n",
    "B = 2\n",
    "NH = 2\n",
    "T_flatten = 64\n",
    "HS = 2\n",
    "Q = torch.rand(B, NH, T_flatten, HS, device = cuda)\n",
    "K = torch.rand(B, NH, T_flatten, HS, device = cuda)\n",
    "V = torch.rand(B, NH, T_flatten, HS, device = cuda)\n",
    "\n",
    "score = torch.rand(B, NH, T_flatten, T_flatten, device = cuda)\n",
    "\n",
    "A = FocusedAttention(score, V)\n",
    "B = FocusedAttention(score, V)\n",
    "\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
